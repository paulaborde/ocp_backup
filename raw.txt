(ROLE)
Openshift Container Platform 4.4 DO380
Red Hat OpenShift Administration III : Scaling
Kubernetes Deployments in the Enterprise
Edition 2
DO380-OCP4.4-en-2-20200803 Copyright ©2020 Red Hat, Inc.

Red Hat OpenShift
Administration III :
Scaling Kubernetes
Deployments in the
Enterprise
DO380-OCP4.4-en-2-20200803 Copyright ©2020 Red Hat, Inc.
Openshift Container Platform 4.4 DO380
Red Hat OpenShift Administration III : Scaling Kubernetes
Deployments in the Enterprise
Edition 2 20200803
Publication date 20200803
Authors: Alex Corcoles, Ivan Chavero, James Mighion, Joel Birchler,
Michael Phillips, Christopher Caillouet, Harpal Singh, Razique Mahroua,
Dan Kolepp
Editor: Nicole Muller
Copyright © 2020 Red Hat, Inc.
The contents of this course and all its modules and related materials, including handouts to audience members, are
Copyright © 2020 Red Hat, Inc.
No part of this publication may be stored in a retrieval system, transmitted or reproduced in any way, including, but
not limited to, photocopy, photograph, magnetic, electronic or other record, without the prior written permission of
Red Hat, Inc.
This instructional program, including all material provided herein, is supplied without any guarantees from Red Hat,
Inc. Red Hat, Inc. assumes no liability for damages or legal action arising from the use or misuse of contents or details
contained herein.
If you believe Red Hat training materials are being used, copied, or otherwise improperly distributed, please send
email to training@redhat.com or phone toll-free (USA) +1 (866) 626-2994 or +1 (919) 754-3700.
Red Hat, Red Hat Enterprise Linux, the Red Hat logo, JBoss, Hibernate, Fedora, the Infinity logo, and RHCE are
trademarks of Red Hat, Inc., registered in the United States and other countries.
Linux® is the registered trademark of Linus Torvalds in the United States and other countries.
Java® is a registered trademark of Oracle and/or its affiliates.
XFS® is a registered trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or
other countries.
The OpenStack® word mark and the Square O Design, together or apart, are trademarks or registered trademarks
of OpenStack Foundation in the United States and other countries, and are used with the OpenStack Foundation's
permission. Red Hat, Inc. is not affiliated with, endorsed by, or sponsored by the OpenStack Foundation or the
OpenStack community.
All other trademarks are the property of their respective owners.
Contributors: Chris Tusa, David Sacco, Eliezer Campos Laux, Fernando Lozano, Jim Rigsbee,
Nicolette Lucas
Document Conventions vii
Introduction ix
Red Hat OpenShift Administration III : Scaling Kubernetes Deployments in the
Enterprise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix
Orientation to the Classroom Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . x
1. Moving From Kubernetes to OpenShift 1
Deploying Kubernetes Applications on OpenShift . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
Guided Exercise: Deploying Kubernetes Applications on OpenShift . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
Optimizing Kubernetes Applications for OpenShift . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
Guided Exercise: Optimizing Kubernetes Applications for OpenShift . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2. Introducing Automation with OpenShift 19
Querying OpenShift Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
Guided Exercise: Querying OpenShift Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
Deploying Scripts on OpenShift . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
Guided Exercise: Deploying Scripts on OpenShift . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
Navigating the OpenShift REST API . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
Guided Exercise: Navigating the OpenShift REST API . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
Writing Ansible Playbooks to Manage OpenShift Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
Guided Exercise: Writing Ansible Playbooks to Manage OpenShift Resources . . . . . . . . . . . . . . . . . 53
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
3. Manage Operators and OpenShift Cluster Operators 59
Describing Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
Quiz: Describing Operators Quiz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
Installing Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
Guided Exercise: Installing Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
Managing Cluster Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
Guided Exercise: Managing Cluster Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
4. Implementing GitOps with Jenkins 81
Introducing Jenkins Declarative Pipelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
Quiz: Introducing Jenkins Declarative Pipelines Quiz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
Deploying Jenkins on OpenShift . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
Guided Exercise: Deploying Jenkins on OpenShift . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
Configuring OpenShift Resources Using a Declarative GitOps Workflow . . . . . . . . . . . . . . . . . . . . . . 109
Guided Exercise: Configuring OpenShift Resources Using a Declarative GitOps
Workflow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
Configuring OpenShift using GitOps and Jenkins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
Guided Exercise: Configuring OpenShift using GitOps and Jenkins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
5. Configuring Enterprise Authentication 141
Configuring the LDAP identity provider . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
Guided Exercise: Configuring the LDAP Identity Provider . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
Synchronizing OpenShift Groups with LDAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
Guided Exercise: Synchronizing OpenShift Groups with LDAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
6. Configuring Trusted TLS Certificates 161
Integrating OpenShift with an Enterprise Certificate Authority . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
Guided Exercise: Integrating OpenShift with an Enterprise Certificate Authority . . . . . . . . . . . . 168
Configuring Applications to Trust the Enterprise Certificate Authority . . . . . . . . . . . . . . . . . . . . . . . . . . 174
DO380-OCP4.4-en-2-20200803 v
Guided Exercise: Configuring Applications to Trust the Enterprise Certificate
Authority . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
Troubleshooting OpenShift Certificates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
Guided Exercise: Troubleshooting OpenShift Certificates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
Lab: Configuring Trusted TLS Certificates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
7. Configuring Dedicated Node Pools 205
Adding Worker Nodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
Guided Exercise: Adding Worker Nodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210
Creating Custom Machine Config Pools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
Guided Exercise: Creating Custom Machine Config Pools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
8. Configuring Persistent Storage 229
Describing the OpenShift Storage Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
Quiz: Describing the OpenShift Storage Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
Provisioning Shared Storage for Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
Guided Exercise: Provisioning Shared Storage for Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244
Provisioning Block Storage for Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
Guided Exercise: Provisioning Block Storage for Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
Provisioning Local Block Storage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
Guided Exercise: Installing the Local Storage Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
Lab: Configuring Persistent Storage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
9. Managing Cluster Monitoring and Metrics 289
Introducing the Cluster Monitoring Stack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
Quiz: Introducing the Cluster Monitoring Stack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
Managing OpenShift Alerts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
Guided Exercise: Managing OpenShift Alerts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
Troubleshooting Using the Cluster Monitoring Stack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
Guided Exercise: Troubleshooting Using the Cluster Monitoring Stack . . . . . . . . . . . . . . . . . . . . . . . . . . 312
Configuring Storage for the Cluster Monitoring Stack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315
Guided Exercise: Configuring Storage for the Cluster Monitoring Stack . . . . . . . . . . . . . . . . . . . . . . . . 318
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
10. Provisioning and Inspecting Cluster Logging 325
Deploying Cluster Logging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
Guided Exercise: Deploying Cluster Logging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
Querying Cluster Logs with Kibana . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340
Guided Exercise: Querying Cluster Logs with Kibana . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343
Visualizing Cluster Logs with Kibana . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
Guided Exercise: Visualizing Cluster Logs with Kibana . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349
Diagnosing Cluster Logging Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352
Guided Exercise: Diagnosing Cluster Logging Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
11. Recovering Failed Worker Nodes 361
Profiling Degraded Worker Nodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
Guided Exercise: Profiling Degraded Worker Nodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
Troubleshooting Worker Node Capacity Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370
Guided Exercise: Troubleshooting Worker Node Capacity Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
A. Creating a Quay Account 377
Creating a Quay Account . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378
Repositories Visibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
vi DO380-OCP4.4-en-2-20200803
Document Conventions
References
"References" describe where to find external documentation relevant to a subject.
Note
"Notes" are tips, shortcuts or alternative approaches to the task at hand. Ignoring a
note should have no negative consequences, but you might miss out on a trick that
makes your life easier.
Important
"Important" boxes detail things that are easily missed: configuration changes that
only apply to the current session, or services that need restarting before an update
will apply. Ignoring a box labeled "Important" will not cause data loss, but may cause
irritation and frustration.
Warning
"Warnings" should not be ignored. Ignoring warnings will most likely cause data loss.
DO380-OCP4.4-en-2-20200803 vii
viii DO380-OCP4.4-en-2-20200803
Introduction
Red Hat OpenShift Administration III : Scaling
Kubernetes Deployments in the Enterprise
This course teaches the skills required to manage OpenShift clusters at scale
to productively support a growing number of stakeholders, applications,
and users. This course enables organizations to innovate faster by adopting
containers as part of Digital Transformation initiatives.
Course
Objectives
• Automating day 2 tasks necessary to establish
production clusters with higher performance
and availability requirements.
• Integrating OpenShift software with enterprise
infrastructure such as authentication and
storage.
• Troubleshooting issues related to cluster
operators, resource availability, and capacity.
Audience • Senior System Administrators focused in
planning, designing, and implementing
production-grade OpenShift clusters.
• Site Reliability Engineers (SREs) focused in
keeping OpenShift clusters and applications
running without disruption.
Prerequisites • Red Hat Certified Specialist in OpenShift
Administration (EX280) certification or
equivalent knowledge for the role of an
OpenShift Cluster Administrator.
• Red Hat Certified Systems Administrator
(EX200) certification or equivalent knowledge
of the Linux shell and managing Linux servers.
• Ability to write and run simple Ansible
Playbooks.
DO380-OCP4.4-en-2-20200803 ix
Introduction
Orientation to the Classroom
Environment
The Workstation Machine
In this course, the main computer system used for hands-on learning activities (exercises) is
workstation.
The workstation machine has a standard user account, student with the password student.
No exercise in this course requires that you log in as root, but if you must, the root password on
the workstation machine is redhat.
It is from the workstation machine that you type oc commands to manage the OpenShift
cluster, which comes preinstalled as part of your classroom environment.
It is also from the workstation machine that you run shell scripts and Ansible Playbooks required
to complete exercises for this course.
If exercises require that you open a web browser to access any application or web site, then you
are required to use the graphical console of the workstation machine and use the Firefox web
browser from there.
Note
The first time you start your classroom environment, OpenShift clusters take a
little longer to become fully available. The lab command at the beginning of each
exercise checks and waits as required.
If you try to access your cluster using either the oc command or the web console
without first running a lab command, then you might find that your cluster is not yet
available. If that happens, then wait a few minutes and try again.
Log in on OpenShift from the Shell
To access your OpenShift cluster from the workstation machine, use https://
api.ocp4.example.com:6443 as the API URL, for example:
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Besides the admin user, who has cluster administrator privileges, your OpenShift cluster also
provides a developer user, with the password developer, who has no special privileges.
Note
If you get the "Use insecure connections" prompt, then answer "yes" because your
cluster uses a TLS certificate that was generated by the installer and is not trusted
by your workstation machine.
x DO380-OCP4.4-en-2-20200803
Introduction
Accessing the OpenShift Web Console
If you prefer to use the OpenShift web console, open a Firefox web browser on your
workstation machine and access the following URL:
https://console-openshift-console.apps.ocp4.example.com
Click htpasswd_provider and provide the log in credentials for either the admin or
developer user.
Note
The first time you try to access the web console, your browser will not trust its TLS
certificate. As with the API TLS certificate, it is generated by the OpenShift installer
and not trusted by the workstation machine. Click Advanced in the Firefox
warning page and follow the steps required to accept the risk and trust the TLS
certificate.
The first time you access the web console you must also trust the TLS certificate of
the OpenShift OAuth authentication server.
The Classroom Environment
Every student gets a complete remote classroom environment. As part of that environment, every
student gets a dedicated OpenShift cluster to perform administration tasks.
The classroom environment runs entirely as virtual machines in a large Red Hat OpenStack
Platform cluster, which is shared among many students.
Red Hat Training maintains many OpenStack clusters, in different data centers across the globe, to
provide lower latency to students from many countries.
DO380-OCP4.4-en-2-20200803 xi
Introduction
All machines on the Student, Classroom, and Cluster Networks run Red Hat Enterprise Linux 8
(RHEL 8), except those machines that are nodes of the OpenShift cluster. These run RHEL
CoreOS.
The systems called bastion, utility, idm, and classroom must always be running. They
provide infrastructure services required by the classroom environment and its OpenShift cluster.
For most exercises, you are not expected to interact with any of these services directly.
Usually, the lab commands from exercises access these machines when there is a requirement to
setup your environment for the exercise, and will require no further action from you.
For the few exercises that require you to access a system other than workstation, primarily the
utility system, you receive explicit instructions and necessary connection information as part of
the exercise.
All systems in the Student Network are in the lab.example.com DNS domain, and all systems in
the Classroom Network are in the example.com DNS domain.
The systems called masterXX and workerXX are nodes of the OpenShift 4 cluster that is part of
your classroom environment.
All systems in the Cluster Network are in the ocp4.example.com DNS domain.
Classroom Machines
Machine name IP addresses ROLE
workstation.lab.example.com 172.25.250.9 Graphical workstation used for
system administration.
classroom.example.com 172.25.254.254 Router linking the Classroom
Network to the Internet.
bastion.lab.example.com 172.25.250.254 Router linking the Student Network
to the Classroom Network.
utility.lab.example.com 172.25.250.253 Router linking the Student Network
to the Cluster Network and also
storage server.
idm.ocp4.example.com 192.168.50.30 RHEL Identity Management (IdM)
server.
master01.ocp4.example.com 192.168.50.10 Control plane node
master02.ocp4.example.com 192.168.50.11 Control plane node
master03.ocp4.example.com 192.168.50.12 Control plane node
worker01.ocp4.example.com 172.25.250.13 Compute node
worker02.ocp4.example.com 172.25.250.14 Compute node
worker03.ocp4.example.com 172.25.250.15 Compute node
worker04.ocp4.example.com 172.25.250.16 Unconfigured compute node
xii DO380-OCP4.4-en-2-20200803
Introduction
Machine name IP addresses ROLE
worker05.ocp4.example.com 172.25.250.17 Unconfigured compute node
worker06.ocp4.example.com 172.25.250.18 Unconfigured compute node
Dependencies on Internet Services
Red Hat OpenShift Container Platform 4 requires access to two container registries to download
container images for operators, S2I builders, and other cluster services. These registries are:
• registry.redhat.io
• quay.io
If either registry is unavailable when starting the classroom environment, then the OpenShift
cluster might not start or could enter a degraded state.
If these container registries experiences an outage while the classroom environment is up and
running, then it might not be possible to complete exercises until the outage is resolved.
A few exercises in DO380 also require that students have personal, free accounts on:
• GitHub (https://github.com)
• Quay.io (https://quay.io)
If either GitHub or Quay.io is not available, the exercises that require them cannot be completed.
Note
Students are expected to sign in to GitHub and Quay.io and create their accounts, if
they do not already have them (see Appendix A), and verify their access by signing
in to these services before starting the class.
The Dedicated OpenShift Cluster
The Red Hat OpenShift Container Platform 4 cluster inside the classroom environment is
preinstalled using the Preexisting Infrastructure installation method; all nodes are treated as bare
metal servers, even though they are actually virtual machines in an OpenStack cluster.
OpenShift cloud-provider integration capabilities are not enabled and a few features that depend
on that integration, such as machine sets and autoscaling of cluster nodes, are not available.
Three of the compute nodes are not initially configured and you will add them to your cluster
during this course. These unconfigured compute nodes also provide an extra local disk device to
be used as local storage.
Your OpenShift cluster is at the state left by running the OpenShift installer with default
configurations, excepting a few day-2 customizations:
• The cluster provides a default storage class backed by a Network File System (NFS) storage
provider. This means that applications requiring persistent storage volumes perform comparably
to running on a cluster installed using the Full-stack Automation installation method.
• There is an HTPasswd Identity Provider (IdP) preconfigured with two users: admin and
developer.
DO380-OCP4.4-en-2-20200803 xiii
Introduction
Two additional machines are in the Cluster Network but are not cluster nodes:
• The idm machine provides a RHEL IdM server that handles LDAP authentication services.
• The utility machine provides NFS and iSCSI storage servers backed by LVM volumes.
It is necessary to access utility for exercises that add nodes to the cluster, because the
utility machine also runs the DHCP server that provides PXE boot to cluster nodes.
The section about Troubleshooting Access to your OpenShift Cluster provides information about
how to access the utility machine.
Restoring Access to your OpenShift Cluster
If you suspect that you cannot log in to your OpenShift cluster as the admin user anymore
because you incorrectly changed your cluster authentication settings, then run the lab finish
command from your current exercise and restart the exercise by running its lab start command.
The lab command for all exercises resets cluster authentication settings and restores the default
passwords of the admin and developer users.
If running a lab command is not sufficient, then you can follow instructions in the next section to
use the utility machine to access your OpenShift cluster.
Troubleshooting Access to your OpenShift Cluster
The utility machine was used to run the OpenShift installer inside your classroom environment,
and it is a useful resource to troubleshoot cluster issues. You can view the installer manifests and
logs in the /home/lab/ocp4 folder of the utility machine.
Logging in to the utility server is rarely required to perform exercises. If it looks like your
OpenShift cluster is taking too long to start, or is in a degraded state, then you can log in on the
utility machine as the lab user to troubleshoot your classroom environment.
The student user on the workstation machine is already configured with SSH keys that enable
logging in to the utility machine without a password.
[student@workstation ~]$ ssh lab@utility
In the utility machine, the lab user is preconfigured with a .kube/config file that grants
access as system:admin without first requiring oc login.
This allows you to run troubleshooting commands, such as oc get node, if they fail from the
workstation machine.
You should not require SSH access to your OpenShift cluster nodes for regular administration
tasks because OpenShift 4 provides the oc debug command; if necessary, the lab user on the
utility server is preconfigured with SSH keys to access all cluster nodes. For example:
[lab@utility ~]$ ssh -i ~/.ssh/lab_rsa core@master01.ocp4.example.com
In the preceding example, replace master01 with the name of the desired cluster node.
xiv DO380-OCP4.4-en-2-20200803
Introduction
Approving Node Certificates on your OpenShift Cluster
Red Hat OpenShift Container Platform clusters are designed to run continuously, 24x7, until they
are decommissioned. Unlike a production cluster, the classroom environment contains a cluster
that was stopped after installation, and will be stopped and restarted a few times before you finish
this course. This presents a scenario that requires special handling that would not be required by a
production cluster.
The control plane and compute nodes in an OpenShift cluster communicate frequently with each
other. All communication between cluster nodes is protected by mutual authentication based on
per-node TLS certificates.
The OpenShift installer handles creating and approving TLS certificate signing requests (CSRs)
for the Full-stack Automation installation method. The system administrator is expected to
manually approve these CSRs for the Preexisting Infrastructure installation method.
All per-node TLS certificates have a short expiration life of 24 hours (the first time) and 30 days
(after renewal). When they are about to expire, the affected cluster nodes create new CSRs
and the control plane automatically approves them. If the control plane is offline when the TLS
certificate of a node expires, then a cluster administrator is required to approve the pending CSR.
The utility machine includes a system service that approves CSRs from the cluster when you
start your classroom, to ensure that your cluster is ready when you begin the exercises. If you
create or start your classroom and begin an exercise too quickly, then you might find that your
cluster is not yet ready. If so, wait a few minutes while the utility machine handles CSRs and
then try again.
Sometimes, the utility machine fails to approve all required CSRs, for example, because the
cluster took too long to generate all required CSRs requests and the system service did not wait
long enough. It’s also possible that some OpenShift cluster nodes did not wait long enough for
their CSRs to be approved, issuing new CSRs that superseded previous ones.
If these issues arise, then you will notice that your cluster is taking too long to come up, and your
oc login or lab commands keep failing. To resolve the problem, you can log in on the the
utility machine, as explained previously, and run the sign.sh script to approve any additional
and pending CSRs.
[lab@utility ~]$ ./sign.sh
The sign.sh script loops a few times just in case your cluster nodes issue new CSRs that
supersede the ones it approved.
After either you approve, or the system service in the utility machine approves all CSRs, then
OpenShift must restart a few cluster operators; it takes a few moments before your OpenShift
cluster is ready to answer requests from clients. To help you handle this scenario, the utility
machine provides the wait.sh script that waits until your OpenShift cluster is ready to accept
authentication and API requests from remote clients.
[lab@utility ~]$ ./wait.sh
Although unlikely, if neither the service on the utility machine nor running the sigh.sh and
wait.sh scripts make your OpenShift cluster available to begin exercises, then open a customer
support ticket.
DO380-OCP4.4-en-2-20200803 xv
Introduction
Note
You can run troubleshooting commands from the utility machine at any time,
even if you have control plane nodes that are not ready. Some useful commands
include:
• oc get node to verify if all of your cluster nodes are ready.
• oc get csr to verify if your cluster still has any pending, unapproved CSRs.
• oc get co to verify if any of your cluster operators are unavailable, in a
degraded state, or progressing through configuration and rolling out pods.
If these fail, you can try destroying and recreating your classroom as a last resort
before creating a customer support ticket.
Controlling Your Systems
Students are assigned remote computers in a Red Hat Online Learning classroom. They are
accessed through a web application hosted at http://rol.redhat.com/. Students should log
in to this site using their Red Hat Customer Portal user credentials.
Controlling the Virtual Machines
The virtual machines in your classroom environment are controlled through a web page. The state
of each virtual machine in the classroom is displayed on the page under the Lab Environment
tab.
Machine States
Virtual Machine
State
Description
active The virtual machine is running and available (or, when booting, soon
will be).
stopped The virtual machine is completely shut down.
building The initial creation of the virtual machine is being performed.
Depending on the state of a machine, a selection of the following actions is available.
Classroom/Machine Actions
Button or Action Description
CREATE Create the ROL classroom. Creates all of the virtual machines needed
for the classroom and starts them. Can take several minutes to
complete.
DELETE Delete the ROL classroom. Destroys all virtual machines in the
classroom. Caution: Any work generated on the disks is lost.
START Start all virtual machines in the classroom.
xvi DO380-OCP4.4-en-2-20200803
Introduction
Button or Action Description
STOP Stop all virtual machines in the classroom.
OPEN CONSOLE Open a new tab in the browser and connect to the console of the
virtual machine. Students can log in directly to the virtual machine
and run commands. In most cases, students should log in to the
workstation virtual machine and use ssh to connect to the other
virtual machines.
ACTION ® Start Start (power on) the virtual machine.
ACTION ®
Shutdown
Gracefully shut down the virtual machine, preserving the contents of
its disk.
ACTION ® Power
Off
Forcefully shut down the virtual machine, preserving the contents of its
disk. This is equivalent to removing the power from a physical machine.
ACTION ® Reset Forcefully shut down the virtual machine and reset the disk to its initial
state. Caution: Any work generated on the disk is lost.
At the start of an exercise, if instructed to reset a single virtual machine node, click ACTION ®
Reset for only the specific virtual machine.
At the start of an exercise, if instructed to reset all virtual machines, click ACTION ® Reset
If you want to return the classroom environment to its original state at the start of the course, you
can click DELETE to remove the entire classroom environment. After the lab has been deleted,
you can click CREATE to provision a new set of classroom systems.
Warning
The DELETE operation cannot be undone. Any work you have completed in the
classroom environment up to that point will be lost.
The Autostop Timer
The Red Hat Online Learning enrollment entitles students to a certain amount of computer time.
To help conserve allotted computer time, the ROL classroom has an associated countdown timer,
which shuts down the classroom environment when the timer expires.
To adjust the timer, click + to add one hour to the timer. Note that there is a maximum time of
twelve hours.
DO380-OCP4.4-en-2-20200803 xvii
xviii DO380-OCP4.4-en-2-20200803
Chapter 1
Moving From Kubernetes to
OpenShift
Goal Demonstrate that OpenShift is Kubernetes by
deploying Kubernetes-native applications on
OpenShift.
Objectives • Deploy applications on OpenShift using
standard Kubernetes resources.
• Enhance Kubernetes applications using
OpenShift extensions from Kubernetes.
Sections • Deploying Kubernetes Applications on
OpenShift (and Guided Exercise)
• Optimizing Kubernetes Applications for
OpenShift (and Guided Exercise)
DO380-OCP4.4-en-2-20200803 1
Chapter 1 | Moving From Kubernetes to OpenShift
Deploying Kubernetes Applications on
OpenShift
Objectives
After completing this section, you should be able to deploy applications on OpenShift using
standard Kubernetes resources.
Describing how OpenShift is Enterprise Kubernetes
Red Hat OpenShift Container Platform is a Kubernetes-based container platform for building
and running applications on-premise or in cloud environments. Kubernetes provides features to
run containers on a cluster of nodes. OpenShift builds on Kubernetes and its extensibility, adding
features such as logging, monitoring, and authentication.
Kubernetes provides kubectl, a command line tool to interact with Kubernetes clusters.
OpenShift provides oc and kubectl. oc adds OpenShift features to kubectl.
Note
kubectl and oc in OpenShift are the same binary. The oc binary embeds
kubectl's code. Execute the binary using the kubectl name to invoke the
kubectl code and features. Execute the binary using the oc name to invoke
OpenShift code and features.
Authenticating to OpenShift
kubectl connects to a Kubernetes cluster using the Kubernetes HTTP API. kubeconfig files
contain the information required to connect to a Kubernetes cluster. Specify the path to the
kubeconfig file with --kubeconfig. The default kubeconfig path is ~/.kube/config.
A kubeconfig file contains clusters, users, and contexts.
A cluster describes a Kubernetes API endpoint, including its URL.
A user describes the credentials to authenticate with a cluster, such as user names and tokens.
A context is a combination of a cluster, a user, and a default namespace.
The OpenShift CLI provides the oc login command to handle the process of logging in to
an OpenShift cluster and managing kubeconfig. Run the oc login command to log in to an
OpenShift cluster.
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
oc login creates or updates ~/.kube/config automatically.
2 DO380-OCP4.4-en-2-20200803
Chapter 1 | Moving From Kubernetes to OpenShift
kubeconfig created by oc login
apiVersion: v1
clusters:
- cluster:
insecure-skip-tls-verify: true
server: https://api.ocp4.example.com:6443
name: api-ocp4-example-com:6443
contexts:
- context:
cluster: api-ocp4-example-com:6443
namespace: default
user: admin/api-ocp4-example-com:6443
name: default/api-ocp4-example-com:6443/admin
current-context: default/api-ocp4-example-com:6443/admin
kind: Config
preferences: {}
users:
- name: admin/api-ocp4-example-com:6443
user:
token: ErPG5n4aT6CuDQ8mJPSwS5WpvYD_N6Fhj8Yd603l7Ns
Run oc login to create a new context using the developer user.
[student@workstation ~]$ oc login -u developer -p developer
Login successful.
The oc login command updates the ~/.kube/config file with the newly created context,
which is comprised of the developer user and the existing cluster.
kubeconfig additions from second oc login
...output omitted...
contexts:
- context:
cluster: api-ocp4-example-com:6443
user: developer/api-ocp4-example-com:6443
name: /api-ocp4-example-com:6443/developer
- context:
cluster: api-ocp4-example-com:6443
namespace: default
user: admin/api-ocp4-example-com:6443
name: default/api-ocp4-example-com:6443/admin
...output omitted...
- name: admin/api-ocp4-example-com:6443
user:
token: ErPG5n4aT6CuDQ8mJPSwS5WpvYD_N6Fhj8Yd603l7Ns
- name: developer/api-ocp4-example-com:6443
user:
token: 6KGBB0WjZpGpE3MJr3KaGsCSY0k4MJQbpb9dWFB1p_M
oc config get-contexts lists the contexts in the kubeconfig file.
DO380-OCP4.4-en-2-20200803 3
Chapter 1 | Moving From Kubernetes to OpenShift
[student@workstation ~]$ oc config get-contexts
CURRENT NAME CLUSTER AUTHINFO NAMESPACE
* /api-ocp4-...:6443/developer api-ocp... developer/...
default/api-ocp4-...:6443/admin api-ocp... admin/... default
oc config use-context changes the current context.
[student@workstation ~]$ oc config use-context \
> default/api-ocp4-example-com:6443/admin
Switched to context "default/api-ocp4-example-com:6443/admin".
oc config set-context updates a context.
[student@workstation ~]$ oc config set-context \
> /api-ocp4-example-com:6443/developer \
> --namespace=namespace
Context "/api-ocp4-example-com:6443/developer" modified.
Deploying Native Kubernetes Applications on
OpenShift
OpenShift Container Platform is a certified Kubernetes distribution. Kubernetes workloads can
be deployed in OpenShift using the same procedures used in any other Kubernetes distribution.
Scripts using kubectl work unmodified on OpenShift.
Use kubectl create -f filename to create a Kubernetes workload made of resources
contained in a file.
Use kubectl get type name to display a resource.
Customizing Resources with Kubernetes Kustomize
kubectl integrates the kustomization tool.
A kustomization is a directory containing a kustomization.yml file.
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
- deployment.yml
The resources field is a list of files containing Kubernetes resources.
A kustomization can contain fields that describe modifications to be made to the kustomization’s
resources.
The images field describes modifications to images in the kustomization.
4 DO380-OCP4.4-en-2-20200803
Chapter 1 | Moving From Kubernetes to OpenShift
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
- deployment.yaml
images:
- name: image
newName: new-image
newTag: new-tag
The image image will be replaced by new-image with tag new-tag.
kustomization.yml can contain a bases field. The bases field is a list of other kustomization
directories.
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
bases:
- path-to-kustomization
A kustomization with a bases field is an overlay. A kustomization without a bases field is a base.
An overlay includes all resources in its bases. kustomization applies modifications described in
an overlay to the resources described in the overlay and its bases.
Use kubectl apply -k directory_name to apply a kustomization.
OpenShift templates are an alternative to kustomizations. OpenShift templates are Kubernetes
parameterized resource definitions. Users provide parameters for templates and deploy the
customized resources using the web console.
References
For more information about oc and kubectl commands, refer to the Usage of oc
and kubectl commands section of the OpenShift CLI (oc) chapter in the Red Hat
OpenShift Container Platform 4.4 CLI Tools documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/cli_tools/index#usage-oc-kubectl
Organizing Cluster Access Using kubeconfig Files
https://kubernetes.io/docs/concepts/configuration/organize-cluster-accesskubeconfig/
Declarative Management of Kubernetes Objects Using Kustomize
https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/
DO380-OCP4.4-en-2-20200803 5
Chapter 1 | Moving From Kubernetes to OpenShift
Guided Exercise
Deploying Kubernetes Applications on
OpenShift
In this exercise you will deploy an application on OpenShift using standard Kubernetes
resources.
Outcomes
You should be able to:
• Interact with OpenShift using the Kubernetes kubectl CLI.
• Deploy Kubernetes resources using either kubectl or oc.
Before You Begin
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise.
[student@workstation ~]$ lab k8s-deploy start
Instructions
1. As the admin user, configure a namespace and rolebinding for the developer user.
1.1. Use the oc login command to log in as the administrator and change to the ~/
DO380/labs/k8s-deploy/ directory.
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
The server uses a certificate signed by an unknown authority.
You can bypass the certificate check, but any data you send to the server could be
intercepted by others.
Use insecure connections? (y/n): y
Login successful.
...output omitted...
[student@workstation ~]$ cd ~/DO380/labs/k8s-deploy/
1.2. Using kubectl and the provided manifest, create a namespace and provide the
developer user with appropriate access.
[student@workstation k8s-deploy]$ kubectl create -f ns-and-rbac.yml
namespace/k8s-deploy created
rolebinding.rbac.authorization.k8s.io/hello-dev created
2. As the developer user, deploy the Kubernetes resources.
6 DO380-OCP4.4-en-2-20200803
Chapter 1 | Moving From Kubernetes to OpenShift
2.1. Use the oc login command to log in as the developer user.
[student@workstation k8s-deploy]$ oc login -u developer -p developer
Login successful.
You have one project on this server: "k8s-deploy"
Using project "k8s-deploy".
2.2. Create a plain Kubernetes deployment, service, and ingress using kubectl and the
provided manifest.
[student@workstation k8s-deploy]$ kubectl apply -f hello.yml
deployment.apps/hello created
service/hello created
ingress.extensions/hello created
2.3. Verify that the deployment is successful.
[student@workstation k8s-deploy]$ kubectl get ingress
NAME HOSTS ADDRESS PORTS AGE
hello hello.apps.ocp4.example.com 80 10s
[student@workstation k8s-deploy]$ curl hello.apps.ocp4.example.com
<html>
<body>
<h1>Hello, world from nginx!</h1>
</body>
</html>
3. Deploy using the Kubernetes-native configuration system: Kustomize.
3.1. As the admin user, create the k8s-deploy-prod namespace.
[student@workstation k8s-deploy]$ oc login -u admin -p redhat
Login successful.
...output omitted...
[student@workstation k8s-deploy]$ kubectl create namespace k8s-deploy-prod
namespace/k8s-deploy-prod created
3.2. Review the provided files in the base and prod overlay Kustomize directories located
at ~/DO380/labs/k8s-deploy/base/ and ~/DO380/labs/k8s-deploy/
overlays/prod/.
3.3. Deploy the base and overlay with Kustomize.
[student@workstation k8s-deploy]$ kubectl apply -k overlays/prod
service/hello created
deployment.apps/hello created
ingress.extensions/hello created
3.4. Verify that the deployment is successful.
DO380-OCP4.4-en-2-20200803 7
Chapter 1 | Moving From Kubernetes to OpenShift
[student@workstation k8s-deploy]$ kubectl get ingress -n k8s-deploy-prod
NAME HOSTS ADDRESS PORTS AGE
hello deploying-practice.apps.ocp4.example.com 80 38s
[student@workstation k8s-deploy]$ curl deploying-practice.apps.ocp4.example.com
Hi!
Note
The container name is required to merge the change to the container image.
3.5. Update the ~/DO380/labs/k8s-deploy/overlays/prod/
kustomization.yml file to change the tag of the image from the base. The file
should look like the following.
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: prod
bases:
- ../../base
images:
- name: quay.io/redhattraining/versioned-hello
newTag: v1.1
3.6. Optionally, compare your changes to the solution file located at ~/DO380/
solutions/k8s-deploy/overlays/prod/kustomization.yml.
3.7. Deploy the patch and verify that the deployment is successful.
[student@workstation k8s-deploy]$ kubectl apply -k overlays/prod
service/hello unchanged
deployment.apps/hello configured
ingress.extensions/hello unchanged
[student@workstation k8s-deploy]$ curl deploying-practice.apps.ocp4.example.com
Hi! v1.1
4. Change to the /home/student/ directory.
[student@workstation k8s-deploy]$ cd
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab k8s-deploy finish
This concludes the section.
8 DO380-OCP4.4-en-2-20200803
Chapter 1 | Moving From Kubernetes to OpenShift
Optimizing Kubernetes Applications for
OpenShift
Objectives
After completing this section, you should be able to enhance Kubernetes applications using
OpenShift extensions from Kubernetes.
Describing Kubernetes Resources on OpenShift
OpenShift adds enterprise features to Kubernetes that support multi-tenancy and improved
developer workflows.
Kubernetes OpenShift
Namespaces Projects
• Provides a scoping mechanism for
resources.
• Supports increased permissions control,
creation requests, and default templates for
improved multi-tenancy workflows.
• Additional display name and description
fields.
Ingresses Routes
• In a plain Kuberenetes cluster, using ingress
resources requires installation of an ingress
controller. Different ingress controllers will
provide different features.
• In OpenShift, creating a Kubernetes
ingress resources will automatically create
an OpenShift route resource with an
ownerReference to the ingress resource.
• Supports TLS termination and
reencryption.
• Supports HAProxy annotation for extended
features such as enforcing HTTPS,
selecting a load-balancing algorithm, and
enabling rate limiting.
Deployments DeploymentConfigs
• Emphasizes availability over consistency.
• Uses ReplicaSets which support set-based
match selectors.
• Supports pausing rollouts.
• Red Hat recommends using
Deployments unless you need a specific
DeploymentConfigs feature.
• Emphasizes consistency over availability.
• Supports automatic rollbacks.
• Changes to the pod template automatically
trigger rollouts.
• Supports custom deployment strategies.
Kustomize Templates
DO380-OCP4.4-en-2-20200803 9
Chapter 1 | Moving From Kubernetes to OpenShift
Kubernetes OpenShift
• Kubernetes kustomize manages
configuration using file overlays without
templates.
• Typically, manifests and kustomize overlays
are stored in version control.
• OpenShift Template resources are
parametized specification resources.
• Create resources from templates using oc
process or the web console.
Discussing Image Streams
• An ImageStream resource is a tagged list of Images.
• Build and deployment resources can automatically trigger updates when new images are added
to the image stream.
• ImageStream images use a unique SHA256 identifier instead of a mutable image tag. This is
more reliable than trusting standard container image tags such as :latest or :v2.1 in which
the image being tagged can change without notice. An imagestream SHA256 tag identifies an
exact container image.
• Check remote repositories for image updates on a periodic basis by importing the image
using the --scheduled flag. For example, oc import-image IMAGE --confirm --
scheduled. By default, --scheduled checks every fifteen minutes.
Annotating Deployments with Image Stream Triggers
Enhance Kubernetes deployments with OpenShift image stream tags by adding the following
metadata annotation.
{
"image.openshift.io/triggers": "[
{
\"from\": {
\"kind\":\"ImageStreamTag\",
\"name\":\"verioned-hello:latest\"
},
\"fieldPath\":\"spec.template.spec.containers[?(@.name==\\\"hello\\
\")].image\"
}
]"
}
Notice that the JSON image.openshift.io/triggers value is a string. Set the trigger
annotation on the deployment using oc set trigger deploy/DEPLOYMENT_NAME --fromimage
IMAGE_STREAM_TAG -c CONTAINER_NAME.
Note
In a gitOps workflow, the state of the resource specifications should be stored
in version control. Annotating deployments with image streams may cause an
unexpected container image version mismatch between the YAML specification
stored in version control and the deployment resource specification in the cluster.
10 DO380-OCP4.4-en-2-20200803
Chapter 1 | Moving From Kubernetes to OpenShift
References
For more information on managing image streams, refer to the Managing
imagestreams chapter in the Red Hat OpenShift Container Platform 4.4 Images
documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/images/index#managing-imagestreams
For more information on managing Deployments and DeploymentConfigs,
refer to the Understanding Deployments and DeploymentConfigs section in the
Deployments chapter in the Red Hat OpenShift Container Platform 4.4 Applications
documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/applications/index#whatdeployments-
are
For more information on templates, refer to the Using Templates chapter in the
Red Hat OpenShift Container Platform 4.4 Images documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/images/index#using-templates
DO380-OCP4.4-en-2-20200803 11
Chapter 1 | Moving From Kubernetes to OpenShift
Guided Exercise
Optimizing Kubernetes Applications for
OpenShift
In this exercise you will enhance a Kubernetes deployment to use OpenShift image streams.
Outcomes
You should be able to annotate a Kubernetes deployment to automatically perform a rollout
when an image is updated.
Before You Begin
To perform this exercise, ensure you have a personal, free Quay.io account. If you need to
register to Quay.io, see the instructions in the Creating a Quay Account section of Appendix
A.
As the student user on the workstation machine, use the lab command to prepare
your system for this exercise. This command provides a basic Kubernetes namespace,
deployment, service, and ingress.
[student@workstation ~]$ lab k8s-optimize start
Instructions
1. Switch to the k8s-optimize project and verify that the resources are deployed.
1.1. Use the oc login command to log in as the administrator.
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
1.2. Switch to the k8s-optimize project.
[student@workstation ~]$ oc project k8s-optimize
Now using project "k8s-optimize" on server "https://api.ocp4.example.com:6443".
1.3. Verify the provided resources are deployed.
[student@workstation ~]$ oc get deployment,pod,service,ingress
NAME READY UP-TO-DATE AVAILABLE AGE
deployment.apps/hello 2/2 2 2 2m56s
NAME READY STATUS RESTARTS AGE
pod/hello-6b67c44987-chxsq 1/1 Running 0 2m56s
pod/hello-6b67c44987-v4cs6 1/1 Running 0 2m56s
12 DO380-OCP4.4-en-2-20200803
Chapter 1 | Moving From Kubernetes to OpenShift
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
service/hello ClusterIP 172.30.40.14 <none> 8080/TCP 2m56s
NAME HOSTS ADDRESS PORTS AGE
ingress.extensions/hello hello.apps.example.com 80 2m56s
2. Identify the container image specified by the deployment.
[student@workstation ~]$ oc get deployment/hello -o json | \
> jq '.spec.template.spec.containers[0].image'
"quay.io/redhattraining/versioned-hello:v1.0"
Note
Alternatively, use a jsonpath expression.
[student@workstation ~]$ oc get deployment/hello \
> -o jsonpath='{.spec.template.spec.containers[].image}{"\n"}'
quay.io/redhattraining/versioned-hello:v1.0
3. Run the provided watch script to observe changes in cluster resources and poll the example
server.
[student@workstation ~]$ ~/DO380/labs/k8s-optimize/watch.sh
Every 2.0s: oc get dep... workstation.lab.example.com: Thu Jul 2 15:42:37 2020
NAME READY UP-TO-DATE AVAILABLE AGE
deployment.apps/hello 2/2 2 2 115s
NAME READY STATUS RESTARTS AGE
pod/hello-7b6d8fbf49-bdqkt 1/1 Running 0 115s
pod/hello-7b6d8fbf49-svj9v 1/1 Running 0 115s
"quay.io/redhattraining/versioned-hello:v1.0"
Hi!
Leave the current terminal running. Open a new terminal to execute the rest of the steps in
the exercise.
4. Using the skopeo command, copy the versioned-hello:v1.0 image from the
RedHatTraining Quay.io repository to an image tagged latest in a public personal
repository.
4.1. Log in to quay.io.
[student@workstation ~]$ podman login quay.io
Username: your_account
Password:
Login Succeeded!
DO380-OCP4.4-en-2-20200803 13
Chapter 1 | Moving From Kubernetes to OpenShift
4.2. Use the skopeo copy SOURCE DESTINATION command to copy the container
image to a repository in your account.
[student@workstation ~]$ skopeo copy \
> docker://quay.io/redhattraining/versioned-hello:v1.0 \
> docker://quay.io/your_account/versioned-hello:latest
Getting image source signatures
...output omitted...
Writing manifest to image destination
Storing signatures
Note
If the skopeo copy commands fails, retry it. There have been cases of intermittent
authentication issues copying to personal repositories.
4.3. Navigate to the new repository page on quay.io. Change the Repository Visibility
setting to public.
Important
Refer to the Repositories Visibility section of Appendix A to read details about how
to change repository visibility.
5. Import the image from the remote repository into your OpenShift cluster and enable
periodically scheduled imports.
[student@workstation ~]$ oc import-image \
> quay.io/your_account/versioned-hello:latest --confirm --scheduled
imagestream.image.openshift.io/versioned-hello imported
...output omitted...
Observe the change in the terminal window running the watch command. The istag lists
the versioned-hello image. Notice that the image stream tag refers to an immutable
image digest sha.
...output omitted...
NAME IMAGE REPOSITORY
TAGS UPDATED
imagestream.image.openshift.io/versioned-hello image-registry.openshift-imageregistry.
svc:5000/k8s-optimize/versioned-hello latest 14 seconds ago
NAME IMAGE REFERENCE
UPDATED
imagestreamtag.image.openshift.io/versioned-hello:latest quay.io/_your_account_/
versioned-hello@sha256:66e0a9c7341e52de04c8cc0c611d6a6073de183f44a7c4af80eacf3ec
b5a105e 14 seconds ago
...output omitted...
6. Annotate the deployment with the image stream trigger.
14 DO380-OCP4.4-en-2-20200803
Chapter 1 | Moving From Kubernetes to OpenShift
[student@workstation ~]$ oc set triggers deployment/hello \
> --from-image versioned-hello:latest -c hello
deployment.apps/hello triggers updated
[student@workstation ~]$ oc get deployment/hello -o json | \
> jq '.metadata.annotations'
{
"deployment.kubernetes.io/revision": "2",
"image.openshift.io/triggers": "[{\"from\":{\"kind\":\"ImageStreamTag\",\"name
\":\"versioned-hello:latest\"},\"fieldPath\":\"spec.template.spec.containers[?
(@.name==\\\"hello\\\")].image\"}]"
}
Observe the change in the terminal window running the watch command. The image used
in the deployment is now the same as the image stream.
...output omitted...
NAME READY STATUS RESTARTS AGE
pod/hello-77c8b88f4f-qczp2 1/1 Running 0 25s
pod/hello-77c8b88f4f-r9zqx 1/1 Running 0 28s
...output omitted...
"quay.io/_your_account_/versionedhello@
sha256:66e0a9c7341e52de04c8cc0c611d6a6073de183
f44a7c4af80eacf3ecb5a105e"
...output omitted...
7. Copy versioned-hello:v1.1 from the RedHatTraining Quay.io repository to the image
tagged latest in your repository.
[student@workstation ~]$ skopeo copy \
> docker://quay.io/redhattraining/versioned-hello:v1.1 \
> docker://quay.io/your_account/versioned-hello:latest
Getting image source signatures
...output omitted...
Writing manifest to image destination
Storing signatures
8. The deployment will update and roll out new pods with the v1.1 container image when the
new image is imported. The default scheduled update imports every fifteen minutes.
8.1. To manually import the image:
[student@workstation ~]$ oc import-image \
> quay.io/your_account/versioned-hello:latest
imagestream.image.openshift.io/versioned-hello imported
...output omitted...
The curl command from the watch terminal displays a message ending with v1.1
when the deployment has successfully completed.
DO380-OCP4.4-en-2-20200803 15
Chapter 1 | Moving From Kubernetes to OpenShift
...output omitted...
NAME READY STATUS RESTARTS AGE
pod/hello-6b68b7675c-nx6vc 1/1 Running 0 46s
pod/hello-6b68b7675c-rs95c 1/1 Running 0 40s
NAME IMAGE REPOSITORY
TAGS UPDATED
imagestream.image.openshift.io/versioned-hello image-registry.openshift-imageregistry.
svc:5000/k8s-optimize/versioned-hello latest 46 seconds ago
NAME IMAGE REFERENCE
UPDATED
imagestreamtag.image.openshift.io/versioned-hello:latest quay.io/_your_account_/
versioned-hello@sha256:834d5eb861cf528ff77388cd99132c426bcaea41d660048191c88e7c0
a08fcb4 46 seconds ago
"quay.io/_your_account_/versioned-hello@sha256:834d5eb861cf528ff77388cd99132c426b
caea41d660048191c88e7c0a08fcb4"
Hi! v1.1
8.2. Confirm that the deployment.kubernetes.io/revision is now three.
[student@workstation ~]$ oc get deployment/hello -o json | \
> jq '.metadata.annotations'
{
"deployment.kubernetes.io/revision": "3",
...output omitted...
}
9. Terminate the watch script in the other terminal with Ctrl+C.
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab k8s-optimize finish
This concludes the section.
16 DO380-OCP4.4-en-2-20200803
Chapter 1 | Moving From Kubernetes to OpenShift
Summary
In this chapter, you learned:
• OpenShift supports native Kubernetes applications.
• How to use Kubernetes Kustomize.
• How to optimize Kubernetes resources with OpenShift features.
DO380-OCP4.4-en-2-20200803 17
18 DO380-OCP4.4-en-2-20200803
Chapter 2
Introducing Automation with
OpenShift
Goal Automate OpenShift using scripts and Ansible
playbooks.
Objectives • Extract resource information from OpenShift
using JSONPath.
• Execute cluster operation scripts from inside
and outside of OpenShift.
• Navigate the OpenShift REST API.
• Automate creation and modification of
OpenShift resources using the Ansible k8s
modules.
Sections • Querying OpenShift Resources (and Guided
Exercise)
• Deploying Scripts on OpenShift (and Guided
Exercise)
• Navigating the OpenShift REST API (and
Guided Exercise)
• Writing Ansible Playbooks to Manage
OpenShift Resources (and Guided Exercise)
DO380-OCP4.4-en-2-20200803 19
Chapter 2 | Introducing Automation with OpenShift
Querying OpenShift Resources
Objectives
After completing this section, you should be able to extract resource information from OpenShift
using JSONPath.
Describing Kubernetes Resources
• A Kubernetes cluster desired persistent state is encoded using resources (pods, services, etc).
• Resources typically have a spec (their desired status) and status (their current status).
• Resources are exposed uniformly by the API
Extracting Information from Resources
List resources of type in namespace with oc get type -n namespace -o json:
[user@demo ~]$ oc get deployment -n openshift-cluster-samples-operator -o json
{
"apiVersion": "v1",
"items": [
{
"apiVersion": "apps/v1",
"kind": "Deployment",
"metadata": {
...output omitted...
},
"spec": {
...output omitted...
},
"status": {
...output omitted...
}
}
],
"kind": "List",
"metadata": {
...output omitted...
}
}
JSON output is a list of items. Each item has metadata, spec and status.
Get information about the "schema" of an object or its properties:
[user@demo ~]$ oc explain deployment.status.replicas
KIND: Deployment
VERSION: apps/v1
20 DO380-OCP4.4-en-2-20200803
Chapter 2 | Introducing Automation with OpenShift
FIELD: replicas <integer>
DESCRIPTION:
Total number of non-terminated pods targeted by this deployment (their
labels match the selector).
Extracting Information from a Single Resource
• Use oc get -n namespace type name to retrieve a single object.
• Use JSONPath templates to extract and format the contents of the queried information.
[user@demo ~]$ oc get deployment -n openshift-cluster-samples-operator \
> cluster-samples-operator -o jsonpath='{.status.availableReplicas}'
...output omitted...
• {} signals JSONPath code to be evaluated
• .status.availableReplicas accesses properties of an object
Iterate over lists in the resource using [*]:
[user@demo ~]$ oc get deployment -n openshift-cluster-samples-operator \
> cluster-samples-operator -o jsonpath='{.status.conditions[*].type}'
Available Progressing
get -o jsonpath prints results in a single line, separated by spaces. To simplify processing by
other tools, put one item per line piping the output to | tr " " "\n" or a similar command.
Get a specific item in a list using [index]:
[user@demo ~]$ oc get deployment -n openshift-cluster-samples-operator \
> cluster-samples-operator -o jsonpath='{.spec.template.spec.containers[0].name}'
cluster-samples-operator
Filter items in a list using [?(condition)]:
[user@demo ~]$ oc get deployment -n openshift-cluster-samples-operator \
> cluster-samples-operator \
> -o jsonpath='{.status.conditions[?(@.type=="Available")].status}'
Extracting a Single Property from Multiple Resources
• oc get returns a JSON object containing an items object, which is a list of all the objects
queried.
To list a single property from many objects, execute:
[user@demo ~]$ oc get route -n openshift-monitoring \
> -o jsonpath='{.items[*].spec.host}'
alertmanager-main-openshift-monitoring.apps.ocp4.example.com
...output omitted...
DO380-OCP4.4-en-2-20200803 21
Chapter 2 | Introducing Automation with OpenShift
Note that get -o=custom-columns also prints properties in a tabular format:
[user@demo ~]$ oc get pod --all-namespaces \
> -o=custom-columns=NAME:.metadata.name,STATUS:.status.phase,NODE:.spec.nodeName
NAME STATUS NODE
nfs-client-provisioner-865c67f466-r7txk Running worker03
openshift-apiserver-operator-7f87667d89-6bcsj Running master01
apiserver-69b8c6f494-9hflm Running master01
apiserver-69b8c6f494-kkfjj Running master02
apiserver-69b8c6f494-xbvmn Running master03
authentication-operator-559d87b6-t5ttm Running master01
oauth-openshift-6786dd496c-ghb6s Running master01
oauth-openshift-6786dd496c-hnbsj Running master02
...output omitted...
Extracting a Single Property with Multiple Nesting from
Resources
• oc get pods returns a list of pods.
• .items[*] iterates over each pod
• Each pod contains a nested spec.containers object which is a list of the containers in the
pod
• Using [*] iterates over all containers in all pods
[user@demo ~]$ oc get pods -A \
> -o jsonpath='{.items[*].spec.containers[*].image}'
quay.io/external_storage/nfs-client-provisioner:latest
...output omitted...
Extracting Multiple Properties at Different Levels of Nesting
from Resources
• Use the {range} statement to perform more complex iteration
• {range x}…template…{end} will execute template for each item in x
Use {range} as an alternative to .items[*].property.
[user@demo ~]$ oc get pods -A \
> -o jsonpath='{range .items[*]}'\
> '{.metadata.namespace} {.metadata.creationTimestamp}{"\n"}'
nfs-client-provisioner 2020-06-22T16:50:34Z
openshift-apiserver-operator 2020-06-22T16:38:09Z
...output omitted...
Use nested ranges to print nested information in lists of items.
Print a line for every pod. Print all pod conditions in each line.
22 DO380-OCP4.4-en-2-20200803
Chapter 2 | Introducing Automation with OpenShift
[user@demo ~]$ oc get -A pods -o jsonpath='{range.items[*]}'\
> '{"\n"}{.metadata.namespace}{"\t"}{.metadata.name}{"\t"}'\
> '{range.status.conditions[*]}{.type}-{.status},{.end}{end}{"\n"}'
nfs-client-provisioner nfs-client-provisioner-865c67f466-spfv6 Initialized-
True,Ready-True,ContainersReady-True,PodScheduled-True,
openshift-apiserver-operator openshift-apiserver-operator-54b67788df-7k9k9
Initialized-True,Ready-True,ContainersReady-True,PodScheduled-True,
openshift-apiserver apiserver-587b85b7db-47nqv Initialized-True,Ready-
True,ContainersReady-True,PodScheduled-True,
...output omitted...
• Use -o jsonpath-file to store JSONPath reports in files for readability
Create not_ready_nodes.jsonpath:
{range .items[*]}
{.metadata.name}
{range .status.conditions[?(@.status=="False")]}
{.type}{"="}{.status} {.message}
{end}
{end}
Execute the report:
[user@demo ~]$ oc get nodes -o jsonpath-file=not_ready_nodes.jsonpath
master01
MemoryPressure=False kubelet has sufficient memory available
DiskPressure=False kubelet has no disk pressure
PIDPressure=False kubelet has sufficient PID available
master02
...output omitted...
Note that whitespace in JSONPath is significant, so the output contains the indentation and line
breaks in the template.
Using Label Filtering
Use --show-labels to view labels with oc get.
[user@demo ~]$ oc get deployment -n openshift-cluster-storage-operator \
> --show-labels
NAME ... LABELS
cluster-storage-operator ... <none>
csi-snapshot-controller ... <none>
csi-snapshot-controller-operator ... app=csi-snapshot-controller-operator
DO380-OCP4.4-en-2-20200803 23
Chapter 2 | Introducing Automation with OpenShift
Use -l label to filter by label:
[user@demo ~]$ oc get deployment -n openshift-cluster-storage-operator \
> -l app=csi-snapshot-controller-operator -o name
deployment.apps/csi-snapshot-controller-operator
Introducing Alternative Methods for Extracting
Resource Information
• oc get -o name can also be useful for many automation cases.
• JSONPath might not cover all operations you might want to perform on resource information.
• As oc get can output JSON, anything that understands JSON can be used for processing
OpenShift resource information.
• jq provides a domain-specific language to process JSON information, which can perform more
powerful transformations.
Describing Automation Challenges with Resource
Updates
• Automation executes a series of steps.
• Steps that change an OpenShift resource specification are executed in the background by
operators following the controller pattern.
• Steps that depend on a previous step that is not ready yet can fail, breaking the automation.
Waiting for the controllers to execute the changes prevents these issues.
Waiting for a predefined amount of time after performing a change can work in some situations,
but it is fragile and inefficient.
Some automation tools provide features to solve this problem in a robust and efficient manner:
• oc rollout status waits until a Deployment has been rolled out.
• Ansible’s k8s module can wait until some resources it manages are in the desired state.
However, sometimes waiting must be implemented ad-hoc.
Waiting on Triggered Deployment Updates
• The authentication Cluster Operator uses a Deployment to run the processes that
implement authentication.
• When using the HTPasswd identity provider, user credentials are stored in a Secret.
• Changing the Secret’s contents will trigger a Deployment update.
• This will happen in the background, so after replacing the secret, changes will not be executed
immediately.
To perform a change and wait until it is executed:
• Retrieve the observedGeneration of the Deployment before applying the change that
triggers the update.
24 DO380-OCP4.4-en-2-20200803
Chapter 2 | Introducing Automation with OpenShift
deployment_generation=$(oc get \
-n openshift-authentication deployment/oauth-openshift \
-o jsonpath='{.status.observedGeneration}')
• Apply the change
• Wait until the observedGeneration increases.
while
new_generation=$(oc get \
-n openshift-authentication deployment/oauth-openshift \
-o jsonpath='{.status.observedGeneration}')
[ $new_generation -eq $deployment_generation ]
do
sleep 3
done
• Execute oc rollout status specifying the new revision. This will block until the changes
have been executed.
oc rollout status -n openshift-authentication deployment/oauth-openshift \
--revision=$new_generation
Executing oc rollout status immediately after triggering the update does not work, as the
update does not start immediately.
• Wait until there are no pods pending deletion.
while [ -n "$(oc get pod -n openshift-authentication -o \
jsonpath='{.items[?(@.metadata.deletionTimestamp != "")].metadata.name}')" ]
do
sleep 3
done
References
Understanding Kubernetes Objects
https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetesobjects/
kubectl CLI
https://kubernetes.io/docs/reference/kubectl/#output-options
JSONPath Support
https://kubernetes.io/docs/reference/kubectl/jsonpath/
DO380-OCP4.4-en-2-20200803 25
Chapter 2 | Introducing Automation with OpenShift
Guided Exercise
Querying OpenShift Resources
In this exercise you will query OpenShift resources using oc get and JSONPath.
Outcomes
You should be able to:
• Extract attributes from OpenShift resources using JSONPath expressions and filters.
• Write shell scripts that extract and modify data from OpenShift resources in a reliable way.
Before You Begin
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise.
[student@workstation ~]$ lab automation-resources start
Instructions
1. Get the expire date for the web console certificate.
1.1. From workstation, log in to OpenShift as the admin user.
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
1.2. Store the host name of the web console in a variable.
[student@workstation ~]$ console=$(oc get route -n openshift-console console \
> -o jsonpath='{.spec.host}')
[student@workstation ~]$ echo $console
console-openshift-console.apps.ocp4.example.com
1.3. Use the curl command to display the expire date of the OpenShift Router TLS
certificate.
[student@workstation ~]$ curl https://$console -k -v 2>&1 | grep 'expire date'
* expire date: Jun 22 16:43:53 2022 GMT
2. Check that all routes are responding.
2.1. Get all routes' host names and store them in a variable.
26 DO380-OCP4.4-en-2-20200803
Chapter 2 | Introducing Automation with OpenShift
[student@workstation ~]$ hosts=$(oc get route -A \
> -o jsonpath='{.items[*].spec.host}')
2.2. Use curl to get the HTTP status for each route.
[student@workstation ~]$ for host in $hosts ; do \
> curl https://$host -k -w "%{url_effective} %{http_code}\n" -o /dev/null -s ; \
> done
https://oauth-openshift.apps.ocp4.example.com/ 403
https://console-openshift-console.apps.ocp4.example.com/ 200
https://downloads-openshift-console.apps.ocp4.example.com/ 200
https://alertmanager-main-openshift-monitoring.apps.ocp4.example.com/ 403
https://grafana-openshift-monitoring.apps.ocp4.example.com/ 403
https://prometheus-k8s-openshift-monitoring.apps.ocp4.example.com/ 403
https://thanos-querier-openshift-monitoring.apps.ocp4.example.com/ 403
3. Create a script to list the users configured in the HTPasswd identity provider.
3.1. Show the OAuth configuration in JSON format to locate the name of the secret that
contains the users.
[student@workstation ~]$ oc get oauth cluster -o json
{
...output omitted...
},
"spec": {
"identityProviders": [
{
"htpasswd": {
"fileData": {
"name": "htpasswd-secret"
}
},
"mappingMethod": "claim",
"name": "htpasswd_provider",
"type": "HTPasswd"
}
]
}
}
3.2. Use JSONPath with a filter to extract the secret name from the identity provider
named htpasswd_provider.
[student@workstation ~]$ filter='?(.name=="htpasswd_provider")'
[student@workstation ~]$ oc get oauth cluster \
> -o jsonpath="{.spec.identityProviders[$filter].htpasswd.fileData.name}{'\n'}"
htpasswd-secret
3.3. Create the get-users.sh script with the contents from the following listing.
DO380-OCP4.4-en-2-20200803 27
Chapter 2 | Introducing Automation with OpenShift
You can compare your script with the solution file in the ~/DO380/solutions/
automation-resources/ folder.
#!/bin/sh
filter='?(.name=="htpasswd_provider")'
secret_name=$(oc get oauth cluster \
-o jsonpath="{.spec.identityProviders[$filter].htpasswd.fileData.name}")
secret_file=$(oc extract secret/$secret_name -n openshift-config --confirm)
cut -d : -f 1 <$secret_file
rm $secret_file
3.4. Execute the script.
[student@workstation ~]$ chmod +x get-users.sh
[student@workstation ~]$ ./get-users.sh
admin
developer
4. Fix a script that adds a user to the HTPasswd identity provider .
4.1. Change to the ~/DO380/labs/automation-resources/ directory.
[student@workstation ~]$ cd ~/DO380/labs/automation-resources/
4.2. Examine the incomplete add-user.sh script. You will make changes at a later step,
after you test the incomplete script.
The script finds the HTPasswd secret and adds a new user to the secret. It tests if the
new user can log in on OpenShift without affecting Kubernetes context of the current
Linux user.
The following partial listing shows the important commands inside the incomplete
add-user.sh script.
...output omitted...
secret=$(oc get oauth/cluster \
-o jsonpath='{.spec.identityProviders[0].htpasswd.fileData.name}')
tmpdir=$(mktemp -d)
oc extract -n openshift-config secret/$secret \
--keys htpasswd --to $tmpdir
htpasswd -b $tmpdir/htpasswd $user $pass
oc set data secret/$secret --from-file htpasswd=$tmpdir/htpasswd \
-n openshift-config
...output omitted...
28 DO380-OCP4.4-en-2-20200803
Chapter 2 | Introducing Automation with OpenShift
oc login -u $user -p $pass --kubeconfig /dev/null \
--insecure-skip-tls-verify \
https://api.ocp4.example.com:6443
4.3. Try to add a user using the script. It is expected to fail.
If by chance it does not fail, then try again, changing the user name, until you see an
error message similar to the following:
[student@workstation automation-resources]$ ./add-user.sh user1 user1
/tmp/tmp.cbAMhwTRob/htpasswd
Adding password for user user1
secret/htpasswd-secret data updated
Login failed (401 Unauthorized)
Verify you have provided correct credentials.
The script might fail because the OpenShift Authentication cluster operator takes
some time to react to the change in the HTPasswd secret and restart the OAuth
pods. If there is a log in attempt before the OAuth pods restart, then the new user is
not recognized.
4.4. Log in as the new user, if login fails then wait a few seconds and try again. Repeat until
you can log in successfully.
If login fails, then wait a few seconds and try again. Repeat until you can log in
successfully.
[student@workstation automation-resources]$ oc login -u user1 -p user1
Login successful.
...output omitted...
4.5. Log in as the admin user.
[student@workstation automation-resources]$ oc login -u admin -p redhat
4.6. Edit the script to wait until:
• The Authentication Operator notices the change in the secret.
• New OAuth pods are deployed.
• Old OAuth pods are gone.
The script has to wait until old OAuth pods are gone because both old and new pods
get requests from the OAuth service. If an authentication request reaches one of the
old pods, then it does not accept the new user.
Replace the FIXME comments with the commands highlighted in the following
partial listing. You can compare your edits with the solution located at the ~/DO380/
solutions/automation-resources/ folder.
...output omitted...
pass=$2
oldpods="$(oc get pod -n openshift-authentication -o name)"
DO380-OCP4.4-en-2-20200803 29
Chapter 2 | Introducing Automation with OpenShift
secret=$(oc get oauth/cluster \
-o jsonpath='{.spec.identityProviders[0].htpasswd.fileData.name}')
...output omitted...
rm -rf $tmpdir
oc wait co/authentication --for condition=Progressing \
--timeout 90s
oc rollout status -n openshift-authentication deployment/oauth-openshift \
--timeout 90s
oc wait $oldpods -n openshift-authentication --for delete \
--timeout 90s
oc login -u $user -p $pass --kubeconfig /dev/null \
--insecure-skip-tls-verify \
https://api.ocp4.example.com:6443
4.7. Add another user, note how log in succeeds.
If you see a "pod not found" error message, similar to the following sample output,
then you can safely ignore it. The error occurs if OpenShift has finished deleting the
pod before the oc wait command.
[student@workstation automation-resources]$ ./add-user.sh user2 user2
...output omitted...
Adding password for user user2
secret/htpasswd-secret data updated
clusteroperator.config.openshift.io/authentication condition met
...output omitted...
deployment "oauth-openshift" successfully rolled out
pod/oauth-openshift-7c9bd95b5d-klq5w condition met
Error from server (NotFound): pods "oauth-openshift-7c9bd95b5d-qvrwg" not found
Login successful.
...output omitted...
4.8. Change to the /home/student/ directory.
[student@workstation automation-resources]$ cd
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab automation-resources finish
This concludes the section.
30 DO380-OCP4.4-en-2-20200803
Chapter 2 | Introducing Automation with OpenShift
Deploying Scripts on OpenShift
Objectives
After completing this section, you should be able to:
• Execute cluster operation scripts from inside and outside of OpenShift.
• Schedule automation scripts using OpenShift CronJobs.
Automating Operations
Replacing repetitive manual work with software automation is a core DevOps practice. By
minimizing hands-on operational tasks, Site Reliability Engineers (SREs) can focus on adding
features and increasing system reliability.
Benefits of automation:
• Automated systems can scale beyond the linear capacity of human operators manually tending
to servers. Without automation, growing the number or complexity of services requires adding
increasingly more administrators to your operations group.
• Version control system, such as git, are a center of collaboration. Scripts and configuration
files are easily shared across teams. Peer reviews and automation of workflows replace manual,
unreviewed system changes.
• Automated operation scripts are both testable and repeatable. Scripts can be unit tested using
popular testing tools.
Discussing Service Accounts
Programs such as continuous integration applications, scripts, and operators provide identification
in the form of a service account. User accounts are for real human users, whereas service accounts
are for programs.
When a user authenticates with the OpenShift OAuth server using the oc client, an access token
is retrieved and saved to a kubeconfig file. The token is used to provide proof of identity to the
OpenShift API server.
Processes running inside an OpenShift cluster interact with the OpenShift API server using an
access token for the service account. By default, OpenShift adds a file containing the service
account access token to a running pod, located at /var/run/secrets/kubernetes.io/
serviceaccount/token. Client binaries, such as oc, and client libraries use an access token to
interact with the OpenShift API.
Creating a Custom Service Account
When running an operational script in OpenShift, create a custom service account to provide its
identity.
A basic service account must only specify the name and namespace. Notice that unlike users,
service accounts belong to a namespace.
DO380-OCP4.4-en-2-20200803 31
Chapter 2 | Introducing Automation with OpenShift
apiVersion: v1
kind: ServiceAccount
metadata:
name: backup_sa
namespace: database
Note
You can also create a service account manually using the oc create
serviceaccount command. However, specifying resources in declarative YAML
or JSON text files encourages the DevOps practices of version control and code
review.
By default, the service account does not have permission to make requests to the OpenShift API
server. For that, you need roles and role bindings.
Defining Roles and Role Bindings
An OpenShift Role or ClusterRole specifies the actions and resources that a user or service
account can perform. Roles are namespaced and only define rules within the scope of the
namespace. However, cluster roles are not namespaced and their rules can be applied to both
namespaced and non-namespaced resources.
OpenShift includes a useful set of predefined cluster roles, such as basic-user, clusterreader,
and view. Use the oc describe clusterrole CLUSTER_ROLE command to list the
rules specified by the cluster role.
[user01@host ~]$ oc describe clusterrole view
Name: view
Labels: kubernetes.io/bootstrapping=rbac-defaults
rbac.authorization.k8s.io/aggregate-to-edit=true
Annotations: rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
Resources ... Verbs
--------- ... -----
namespaces ... [get get list watch]
packagemanifests.pac...coreos.com ... [get list watch get list watch]
appliedclusterresourcequotas ... [get list watch]
...output omitted...
OpenShift associates accounts with roles using role bindings.
A RoleBinding resource is namespaced. It associates accounts with roles within its namespace.
A namespaced role binding can also associate a cluster role with an account. In this case, the
permissions defined by the cluster role only apply to the resources in the namespace.
A ClusterRoleBinding resource is not namespaced and applies to all resources in the cluster.
For example, to grant a service account cluster-reader permissions, create a cluster role
binding as follows:
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
32 DO380-OCP4.4-en-2-20200803
Chapter 2 | Introducing Automation with OpenShift
name: backup
namespace: database
subjects:
- kind: ServiceAccount
name: backup_sa
namespace: database
roleRef:
apiGroup: rbac.authorization.k8s.io
kind: ClusterRole
name: cluster-reader
Subjects are either users, groups, or service accounts.
The roleRef must be either a ClusterRole or Role resource.
Note
Alternately, use the oc policy add-role-to-user command to create or
modify role bindings.
Creating a Custom Role
Assign permissions to accounts with the minimum amount of privileges required to operate.
Limiting access avoids unintended consequences and restricts potential attack vectors.
Roles define a list of policy rules. Each rule specifies the API groups, resources, and permitted
verbs.
In some cases, the predefined cluster roles grant more permissions than are required by a script.
The following example demonstrates a role with a rule permitting the reading and creation of
config maps.
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
name: configurator
namespace: database
rules:
- apiGroups: [""]
resources: ["configmaps"]
verbs: ["get", "list", "create"]
Creating Jobs and CronJobs
A Job creates and executes pods until they successfully complete their task. Jobs can be
configured to run a single pod or many pods concurrently.
Jobs are useful for automating one-off tasks that do not need to be constantly running, such as
report generation or file clean up.
apiVersion: batch/v1
kind: Job
metadata:
name: backup
DO380-OCP4.4-en-2-20200803 33
Chapter 2 | Introducing Automation with OpenShift
namespace: database
spec:
activeDeadlineSeconds: 600
parallelism: 2
template:
metadata:
name: backup
spec:
serviceAccountName: backup_sa
containers:
- name: backup
image: example/backup_maker:v1.2.3
restartPolicy: OnFailure
Optionally, provide a duration limit in seconds. The Job will attempt to terminate if it exceeds
the deadline.
Specify the number of pods to run concurrently.
The spec includes a pod template.
Specify the name of the service account to associate with the pod. If you do not specify a
service account, then the pod will use the default service account in the namespace.
Scheduling OpenShift CronJobs
CronJobs create jobs based on a given time schedule. Use CronJobs for running automation, such
as backups or reports, which should be executed on a regular interval.
The execution schedule is specified in the standard cron format.
• This follows the format "minute hour day-of-the-month month day-of-the-week".
• The * symbol is a wildcard that matches any value. For example, 1 * 1 1 * matches every
hour of the first minute of January 1st.
• A slash / symbol denotes step values. For example, using the pattern */5 * * * * means that
the job should run every five minutes.
apiVersion: batch/v1beta1
kind: CronJob
metadata:
name: backup-cron
namespace: database
spec:
schedule: "1 0 * * *"
jobTemplate:
spec:
activeDeadlineSeconds: 600
parallelism: 2
template:
metadata:
name: backup
spec:
serviceAccountName: backup_sa
containers:
34 DO380-OCP4.4-en-2-20200803
Chapter 2 | Introducing Automation with OpenShift
- name: backup
image: example/backup_maker:v1.2.3
restartPolicy: OnFailure
The example schedule will execute the job at one minute past midnight.
Specify a regular Job in the CronJob’s jobTemplate field.
Comparing Script Deployment Strategies
Strategy Description
Container Command Specify short Bash scripts as arguments to a container in the
spec. This method is easy to deploy, but makes reviewing and
automated tests more difficult.
Volume Mount a ConfigMap or persistent storage as a volume.
Container Image Package the script in a container image with all necessary
dependencies. Use a gitOps pipeline with build, test, and
deployment automation.
References
For more information on roles and role bindings, refer to the Using RBAC to define
and apply permissions chapter in the Red Hat OpenShift Container Platform 4.4
Authentication documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/authentication/index#using-rbac
For more information on Jobs and CronJobs, refer to the Running tasks in pods
using jobs section in the Using Jobs and DaemonSets chapter in the Red Hat
OpenShift Container Platform 4.4 Nodes documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/nodes/index#nodes-nodes-jobs
DO380-OCP4.4-en-2-20200803 35
Chapter 2 | Introducing Automation with OpenShift
Guided Exercise
Deploying Scripts on OpenShift
In this exercise you will execute a cluster operation script from both inside and outside of
OpenShift.
Outcomes
You should be able to:
• Run a cluster operation script from outside of OpenShift.
• Create a service account and custom role for unattended authentication.
• Deploy a cluster operation script to run as a CronJob.
Before You Begin
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise.
This command provides starter files used throughout the exercise and solutions to compare
against.
[student@workstation ~]$ lab automation-scripts start
Instructions
As a cluster administrator, you are tasked with running an audit script, which the security team uses
to list all container images used in the cluster. First, test the script by executing it from outside
of the cluster. Then, create a CronJob so the script automatically runs in the cluster at regular
intervals.
1. Review and execute the shell script.
1.1. Change to the ~/DO380/labs/automation-scripts/ directory.
[student@workstation ~]$ cd ~/DO380/labs/automation-scripts/
1.2. Open audit.sh and review the contents.
The script fetches a list of all of the pods in the cluster and uses the jsonpath
argument to filter the results to only the container images. sed replaces spaces with
newlines. The script then removes duplicates and sorts the entries before printing the
list.
36 DO380-OCP4.4-en-2-20200803
Chapter 2 | Introducing Automation with OpenShift
#!/usr/bin/env bash
oc get pods --all-namespaces \
-o jsonpath="{.items[*].spec.containers[*].image}" \
| sed 's/ /\n/g' \
| sort \
| uniq
1.3. From the workstation machine, log in to OpenShift as the admin user and run the
audit script locally.
[student@workstation automation-scripts]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
You have access to 58 projects, the list has been suppressed. You can list all
projects with 'oc projects'
Using project "default".
[student@workstation automation-scripts]$ ./audit.sh
quay.io/external_storage/nfs-client-provisioner:latest
quay.io/openshift-release-dev/ocp-release@sha256:b51a...c035
quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c945...1c0d
quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ef5c...26af
...output omitted...
2. Create a new project named automation-scripts.
[student@workstation automation-scripts]$ oc new-project automation-scripts
Now using project "automation-scripts" on server "https://
api.ocp4.example.com:6443".
...output omitted...
3. Create a service account with suitable minimal permissions to read pods from all
namespaces.
Ensure that the file named rbac.yml specifies an auditor service account, a cluster role
with read access to pods, and a cluster role binding.
3.1. Define an auditor service account for the job to use for API authentication and bind
the auditor service account to the auditor cluster role with a cluster role binding.
To do that, replace CHANGE_ME with all the name keys with the text auditor.
3.2. Restrict the cluster role to only allow get and list actions performed on pod
resources by replacing CHANGE_ME for the verbs key with the text ["get",
"list"].
3.3. Verify the completed file by comparing it to the solution located at /home/
student/DO380/solutions/automation-scripts/rbac.yml.
3.4. Deploy the OpenShift resources.
DO380-OCP4.4-en-2-20200803 37
Chapter 2 | Introducing Automation with OpenShift
[student@workstation automation-scripts]$ oc create -f rbac.yml
serviceaccount/auditor created
clusterrole.rbac.authorization.k8s.io/auditor created
clusterrolebinding.rbac.authorization.k8s.io/auditor created
4. Deploy an OpenShift job that executes the shell script.
4.1. In the job.yml file, specify auditor as the service account by replacing
CHANGE_ME for the serviceAccountName keys with the text auditor.
4.2. Verify the completed file by comparing it to the solution located at /home/
student/DO380/solutions/automation-scripts/job.yml.
4.3. Deploy the job to OpenShift.
[student@workstation automation-scripts]$ oc create -f job.yml
job.batch/audit-sh created
4.4. Verify that the job completed successfully.
[student@workstation automation-scripts]$ oc get jobs,pods
NAME COMPLETIONS DURATION AGE
job.batch/audit-sh 1/1 9s 16m
NAME READY STATUS RESTARTS AGE
pod/audit-sh-k2c4q 0/1 Completed 0 16m
4.5. Check the logs to review the audit report.
[student@workstation automation-scripts]$ oc logs job.batch/audit-sh
quay.io/external_storage/nfs-client-provisioner:latest
quay.io/openshift-release-dev/ocp-release@sha256:b51a...c035
quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c945...1c0d
quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ef5c...26af
...output omitted...
5. Create a CronJob to automatically run the audit script on a schedule.
5.1. In the cronjob.yml file, replace CHANGE_ME for the apiVersion with the value
batch/v1beta1.
5.2. Schedule the job to run every two minutes by replacing CHANGE_ME for the
schedule with the text "*/2 * * * *".
5.3. Specify auditor as the service account by replacing CHANGE_ME for the
serviceAccountName keys with the text auditor.
5.4. Verify the completed file by comparing it to the solution located at /home/
student/DO380/solutions/automation-scripts/cronjob.yml.
5.5. Create the CronJob.
38 DO380-OCP4.4-en-2-20200803
Chapter 2 | Introducing Automation with OpenShift
[student@workstation automation-scripts]$ oc create -f cronjob.yml
cronjob.batch/audit-cron created
5.6. Watch the CronJob, Job, and Pod resources until the audit-cron script completes.
[student@workstation automation-scripts]$ watch oc get cronjobs,jobs,pods
NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE
cronjob.batch/audit-cron */2 * * * * False 0 45s 2m20
s
NAME COMPLETIONS DURATION AGE
job.batch/audit-cron-1594932720 1/1 13s 40s
job.batch/audit-sh 1/1 25s 3m21s
NAME READY STATUS RESTARTS AGE
pod/audit-cron-1594932720-kp9q7 0/1 Completed 0 40s
pod/audit-sh-9d7cd 0/1 Completed 0 3m21s
5.7. Verify the results of the script by viewing the logs.
[student@workstation automation-scripts]$ oc logs job.batch/audit-cron-1586301240
quay.io/external_storage/nfs-client-provisioner:latest
quay.io/openshift-release-dev/ocp-release@sha256:b51a...c035
quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c945...1c0d
quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ef5c...26af
...output omitted...
6. Change to the /home/student/ directory.
[student@workstation automation-scripts]$ cd
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab automation-scripts finish
This concludes the section.
DO380-OCP4.4-en-2-20200803 39
Chapter 2 | Introducing Automation with OpenShift
Navigating the OpenShift REST API
Objectives
After completing this section, you should be able to navigate the OpenShift REST API.
Discussing the OpenShift REST API
• Clients and automation use the REST API to interact with the cluster.
• The API is served by the OpenShift API Server.
Authenticating with the REST API
• The OpenShift OAuth server handles authentication, validating that you are who you say you
are.
• The OpenShift API server handles authorization, validating that the user has access to make the
API request.
• OpenShift clients retrieve a bearer token from the OpenShift OAuth server, and then include
this token as a header in requests to the API server.
Methods for retrieving the token:
• Request a token from the OAuth server path /oauth/authorize?client_id=openshiftchallenging-
client&response_type=token. The server responds with a 302
Redirect to a location. Find the access_token parameter in the location query string.
• Log in using the oc login command and inspect the kubeconfig yaml. This is normally located
at ~/.kube/config. Find the token listed under your user entry.
• Log in using the oc login command, and then run the oc proxy command to expose an API
server proxy on your local machine. Any requests to the proxy server will identify as the user
logged in with oc.
Warning
Protect access to the localhost proxy port when running oc proxy.
Specify the bearer token as a header in requests to the API server as follows:
[user@demo ~]$ curl -k \
> --header "Authorization: Bearer qfyV5ULvv...i712kJT" \
> -X GET https://api.example.com/api
{
"kind": "APIVersions",
"versions": [
"v1"
],
40 DO380-OCP4.4-en-2-20200803
Chapter 2 | Introducing Automation with OpenShift
"serverAddressByClientCIDRs": [
{
"clientCIDR": "0.0.0.0/0",
"serverAddress": "10.0.136.182:6443"
}
]
}
Finding REST Paths
REST APIs are organized by resource Version and Kind.
• The core v1 API resources, such as pods, are in the /api path. Other resources, such as
deployments and routes, start in the /apis path.
• Cluster-scoped resources: /API_OR_APIS/API_VERSION/RESOURCE_TYPE
• Namespaced resources: /API_OR_APIS/API_VERSION/
namespaces/NAMESPACE/RESOURCE_TYPE
• The API is explorable. Sending a GET request to any part of the path returns a list of possible
paths to request further up the tree.
The REST API can also modify and delete resources in the cluster.
API Verb HTTP Request Command Description
Create POST Create a new resource.
Get/List GET Retrieve a single resource or
a list of resources.
Update PUT Update a resource by
providing a full specification.
Patch PATCH Update a resource by
providing a partial change
described in a patch
operation.
Delete DELETE Delete a resource or a list of
resources.
Create Project Example
[user@demo ~]$ cat project.json
{
"apiVersion": "project.openshift.io/v1",
"kind": "Project",
"metadata": {
"name": "example"
}
}
DO380-OCP4.4-en-2-20200803 41
Chapter 2 | Introducing Automation with OpenShift
[user@demo ~]$ curl -k --header "Authorization: Bearer qfyV5ULvv...i712kJT" \
> --header 'Content-Type: application/json' -d "$(< project.json)" \
> -X POST https://api.example.com/apis/project.openshift.io/v1/projects
{
"kind": "Project",
"apiVersion": "project.openshift.io/v1",
"metadata": {
"name": "example",
...output omitted...
},
"spec": {
"finalizers": [
"openshift.io/origin",
"kubernetes"
]
},
"status": {
"phase": "Active"
}
}
Inspecting API Resources with oc
• The oc explain RESOURCE command displays documentation for a given resource, including
the API version and group necessary for forming a request path.
• For example, the oc explain pvc command outputs PersistentVolumeClaim
as the KIND and v1 as the VERSION. The API path for a PVC is /api/v1/
persistentvolumeclaims.
• The oc api-versions command lists the API group and versions for all available APIs.
• The oc api-resources command retrieves a list of all of the available APIs, including the API
group and whether or not the API resources are namespaced.
Filtering Output
• The API responds in JSON format. Sometimes the responses can be quite long or verbose, and
you only need a field.
• Use the jq application to filter the output.
• Simply piping a curl response to jq will format and color code the output.
Services Names Example
[user@demo ~]$ curl -k --header "Authorization: Bearer qfyV5ULvv...i712kJT" \
> -X GET https://api.example.com/api/v1/services | jq '.items[].metadata.name'
"kubernetes"
"openshift"
"kubelet"
"metrics"
...output omitted...
42 DO380-OCP4.4-en-2-20200803
Chapter 2 | Introducing Automation with OpenShift
References
OpenShift Container Platform 4.4 REST APIs
https://docs.openshift.com/container-platform/4.4/rest_api/objects/index.html
jq
https://stedolan.github.io/jq/
DO380-OCP4.4-en-2-20200803 43
Chapter 2 | Introducing Automation with OpenShift
Guided Exercise
Navigating the OpenShift REST API
In this exercise you will explore the OpenShift REST API using the curl command.
Outcomes
You should be able to:
• Authenticate with the OpenShift OAuth server to retrieve an access token.
• Find and inspect OpenShift resources using the curl command.
Before You Begin
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise.
[student@workstation ~]$ lab automation-rest start
Instructions
1. Attempt a GET request to the OpenShift API server without authentication.
[student@workstation ~]$ curl -k https://api.ocp4.example.com:6443/api
{
"kind": "Status",
"apiVersion": "v1",
"metadata": {
},
"status": "Failure",
"message": "forbidden: User \"system:anonymous\" cannot get path \"/api\"",
"reason": "Forbidden",
"details": {
},
"code": 403
}
2. Authenticate with the OpenShift OAuth server to retrieve an access token.
2.1. Log in on OpenShift as the admin user.
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
44 DO380-OCP4.4-en-2-20200803
Chapter 2 | Introducing Automation with OpenShift
2.2. Find the route to the OpenShift OAuth server using the oc command and assign it to
a variable.
[student@workstation ~]$ oc get route -n openshift-authentication
NAME HOST/PORT PATH SERVICES
PORT TERMINATION WILDCARD
oauth-openshift oauth-openshift.apps.ocp4.example.com oauth-openshift
6443 passthrough/Redirect None
[student@workstation ~]$ OAUTH_HOST=$(oc get route oauth-openshift \
> -n openshift-authentication -o jsonpath='{.spec.host}')
2.3. Authenticate with the OAuth server using the curl command. OpenShift responds
with a 302 Redirect, but there is no need to follow it. The token can be found as the
access_token parameter in the Location URL.
[student@workstation ~]$ curl -u admin -kv "https://$OAUTH_HOST/oauth/\
> authorize?client_id=openshift-challenging-client&response_type=token"
Enter host password for user 'admin': redhat
* Trying 192.168.50.254...
* TCP_NODELAY set
* Connected to oauth-openshift.apps.ocp4.example.com (192.168.50.254) port 443
(#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
* CAfile: /etc/pki/tls/certs/ca-bundle.crt
CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
...output omitted...
* Server auth using Basic with user 'admin'
...output omitted...
< Location: https://oauth-openshift.apps...example.com/oauth/token/
implicit#access_token=xvZ8SsTiA3jRIiEX9QMUOLdaZRUPqubLy2AiQbQGDb0
&expires_in=86400&scope=user%3Afull&token_type=Bearer
...output omitted...
2.4. Save the token to a variable.
[student@workstation ~]$ TOKEN=xvZ8SsTiA3jRIiEX9QMUOLdaZRUPqubLy2AiQbQGDb0
3. Attempt a GET request to the OpenShift API server using the access token.
[student@workstation ~]$ HEADER="Authorization: Bearer $TOKEN"
[student@workstation ~]$ curl -k --header "$HEADER" \
> -X GET https://api.ocp4.example.com:6443/api
{
"kind": "APIVersions",
"versions": [
"v1"
],
"serverAddressByClientCIDRs": [
DO380-OCP4.4-en-2-20200803 45
Chapter 2 | Introducing Automation with OpenShift
{
"clientCIDR": "0.0.0.0/0",
"serverAddress": "192.168.50.10:6443"
}
]
}
4. Traverse the v1 API group using curl requests.
4.1. Fetch the list of API resources available in the v1 API group.
[student@workstation ~]$ curl -k --header "$HEADER" \
> -X GET https://api.ocp4.example.com:6443/api/v1
{
"kind": "APIResourceList",
"groupVersion": "v1",
"resources": [
{
"name": "bindings",
"singularName": "",
"namespaced": true,
"kind": "Binding",
"verbs": [
"create"
]
},
{
"name": "componentstatuses",
"singularName": "",
"namespaced": false,
"kind": "ComponentStatus",
"verbs": [
"get",
"list"
],
"shortNames": [
"cs"
]
},
...output omitted...
4.2. Then fetch a list of one of the resources, such as pods.
[student@workstation ~]$ curl -k --header "$HEADER" \
> -X GET https://api.ocp4.example.com:6443/api/v1/pods
{
"kind": "PodList",
"apiVersion": "v1",
"metadata": {
"selfLink": "/api/v1/pods",
"resourceVersion": "7148472"
46 DO380-OCP4.4-en-2-20200803
Chapter 2 | Introducing Automation with OpenShift
},
"items": [
...output omitted...
4.3. Filter the results to list only the name using jq.
[student@workstation ~]$ curl -k --header "$HEADER" \
> -X GET https://api.ocp4.example.com:6443/api/v1/pods/ | \
> jq ".items[].metadata.name"
"nfs-client-provisioner-865c67f466-k2b98"
"nfs-client-provisioner-865c67f466-spfv6"
"openshift-apiserver-operator-54b67788df-7k9k9"
"apiserver-848cb9fb66-lmjf7"
"apiserver-848cb9fb66-ssl5w"
"apiserver-848cb9fb66-twmcq"
"authentication-operator-5b96cc9776-f8fcv"
"oauth-openshift-6dd57bd6b5-4krw9"
"oauth-openshift-6dd57bd6b5-5zdqk"
"cloud-credential-operator-594567c7b4-4xqwk"
...output omitted...
5. Locate the API endpoint for the Route resources using the oc command, and then fetch a
list of all of the hosts using the curl command.
5.1. Use the oc explain command to discover the resource Kind and API Version.
[student@workstation ~]$ oc explain routes
KIND: Route
VERSION: route.openshift.io/v1
...output omitted...
5.2. List the available APIs for the route.openshift.io/v1 API version group.
[student@workstation ~]$ curl -k --header "$HEADER" \
> -X GET https://api.ocp4.example.com:6443/apis/route.openshift.io/v1
{
"kind": "APIResourceList",
"apiVersion": "v1",
"groupVersion": "route.openshift.io/v1",
"resources": [
{
"name": "routes",
"singularName": "",
"namespaced": true,
"kind": "Route",
...output omitted...
Notice that both routes and routes/status resources are located under the
route.openshift.io/v1 API version group.
5.3. List the hosts for all routes using the curl and jq commands.
DO380-OCP4.4-en-2-20200803 47
Chapter 2 | Introducing Automation with OpenShift
[student@workstation ~]$ curl -k --header "$HEADER" \
> -X GET https://api.ocp4.example.com:6443/apis/route.openshift.io/v1/routes | \
> jq '.items[].spec.host'
"oauth-openshift.apps.ocp4.example.com"
"console-openshift-console.apps.ocp4.example.com"
"downloads-openshift-console.apps.ocp4.example.com"
"alertmanager-main-openshift-monitoring.apps.ocp4.example.com"
"grafana-openshift-monitoring.apps.ocp4.example.com"
"prometheus-k8s-openshift-monitoring.apps.ocp4.example.com"
"thanos-querier-openshift-monitoring.apps.ocp4.example.com"
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab automation-rest finish
This concludes the section.
48 DO380-OCP4.4-en-2-20200803
Chapter 2 | Introducing Automation with OpenShift
Writing Ansible Playbooks to Manage
OpenShift Resources
Objectives
After completing this section, you should be able to automate creation and modification of
OpenShift resources using the Ansible k8s modules.
Explaining the Kubernetes Ansible Modules
There are a handful of modules available to help manage and interact with Kubernetes. Those
same modules work with OpenShift as well, because OpenShift is a distribution of Kubernetes.
The shorthand name for Kubernetes, k8s, is used to refer to all of the Kubernetes modules and is
the group name that prepends of all the Kubernetes modules.
k8s Modules in Ansible 2.9
Module name Description
k8s Manages Kubernetes objects. This module has access to all Kubernetes
APIs, allowing you to create, update, or delete Kubernetes objects.
k8s_auth Authenticates to Kubernetes clusters which require an explicit login step,
like OpenShift. This module returns an api_key that the other modules
can use for authentication.
k8s_info Retrieves information about Kubernetes objects.
k8s_scale Sets a new size for a Deployment, ReplicaSet, Replication Controller, or
Job.
k8s_service Manages Services on Kubernetes.
Most of these modules have common parameters like api_key, context, and namespace.
Shared parameters can be defaulted using the module_defaults attribute.
module_defaults Example
- hosts: localhost
module_defaults:
group/k8s:
api_key: "{{ auth_token }}"
host: https://api.example.com/
ca_cert: ca.pem
Example Playbook
- name: Log in to OpenShift
hosts: localhost
tasks:
DO380-OCP4.4-en-2-20200803 49
Chapter 2 | Introducing Automation with OpenShift
- name: Log in (obtain access token)
k8s_auth:
host: https://api.ocp4.example.com:6443
username: admin
password: redhat
validate_certs: false
register: k8s_auth_results
- name: Demonstrate k8s modules
hosts: localhost
vars:
namespace: dev
module_defaults:
group/k8s:
namespace: "{{ namespace }}"
api_key: "{{ hostvars['localhost']['k8s_auth_results']['k8s_auth']
['api_key'] }}"
host: https://api.ocp4.example.com:6443
validate_certs: false
tasks:
- name: Create objects from the manifest
k8s:
state: present
src: "{{ playbook_dir + '/files/manifest.yml' }}"
- name: Get a info about Pods that are web apps in dev or test
k8s_info:
kind: Pod
label_selectors:
- app = web
- tier in (dev, test)
- name: Scale deployment up
k8s_scale:
kind: Deployment
name: example_deployment
replicas: 3
- name: Expose https port with ClusterIP
k8s_service:
state: present
name: example_service
ports:
- port: 443
protocol: TCP
selector:
key: value
Note
The apiVersion in the manifest might need to be more explicit for the
openshift python package. Instead of putting v1, use the full endpoint like
authorization.openshift.io/v1.
50 DO380-OCP4.4-en-2-20200803
Chapter 2 | Introducing Automation with OpenShift
Important
The current implementation of the k8s modules does not respect role standards.
When using src or resource_definition parameters, you must provide the full
path. The modules will not check the standard files or templates directories in a
role automatically.
Describing the K8s Modules Dependencies
Prerequisite python packages for the k8s, k8s_info, and k8s_scale modules are:
• openshift >= 0.6
• PyYAML >= 3.11
The k8s_service module does not require PyYAML and requires openshift >= 0.6.2.
The k8s_auth module requires:
• urllib3
• requests
• requests-oauthlib
Important
The openshift python package needs to stay at 0.8.x until the k8s_scale module
is updated. https://github.com/ansible/ansible/pull/65312
Explaining the Authentication Options of the Modules
Authenticate with the API with either a kubeconfig file, HTTP Basic Auth, or authentication
tokens.
If no connection options are provided, the openshift client attempts to load the default
configuration file from ~/.kube/config. The kubeconfig module parameter specifies a path
to an existing Kubernetes config file. The context module parameter chooses a context from the
kubeconfig file.
To authenticate via by HTTP Basic Auth, provide the host, username, and password module
parameters. The optional proxy module parameter connects through an HTTP proxy.
The k8s_auth module provides an authentication token that can be used with the api_key
module parameter to authenticate with the API.
Verify the API server’s SSL certificates with the ca_cert, client_cert, and client_key
module parameters. Toggle certificate validation with the validate_certs module parameter.
DO380-OCP4.4-en-2-20200803 51
Chapter 2 | Introducing Automation with OpenShift
References
K8S — Ansible Documentation
https://docs.ansible.com/ansible/latest/modules/
list_of_clustering_modules.html#k8s
52 DO380-OCP4.4-en-2-20200803
Chapter 2 | Introducing Automation with OpenShift
Guided Exercise
Writing Ansible Playbooks to Manage
OpenShift Resources
In this exercise you will create a playbook to automate the creation and modification of
OpenShift resources.
Outcomes
You should be able to:
• Create and execute a playbook that uses the k8s modules.
Before You Begin
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise.
This command provides a working directory with a manifest file and a playbook with some
boilerplate content.
[student@workstation ~]$ lab automation-ansible start
Instructions
1. Change to the ~/DO380/labs/automation-ansible directory.
[student@workstation ~]$ cd ~/DO380/labs/automation-ansible
2. Optionally, review the provided hello.yml manifest. This manifest provides a hello world
application, a service, and a route.
3. Add tasks to the provided k8s.yml playbook. The provided k8s.yml playbook includes
some boilerplate code.
3.1. Edit the k8s.yml playbook with your preferred text editor. Add a task to create
the OpenShift Project. Use the provided namespace variable for the name of the
Project. Set the namespace parameter in the task to "" to allow this task to not use
the module_default value.
...output omitted...
tasks:
- name: Create the project
k8s:
api_version: project.openshift.io/v1
kind: Project
DO380-OCP4.4-en-2-20200803 53
Chapter 2 | Introducing Automation with OpenShift
name: "{{ namespace }}"
state: present
namespace: ""
3.2. Add tasks to create the objects from the hello.yml manifest and view information
about the new pods. Use the src or resource_definition parameter of the
k8s module to read from the manifest file. The completed tasks should match the
following.
...output omitted...
- name: Create objects from the manifest
k8s:
state: present
src: "{{ playbook_dir + '/hello.yml' }}"
- name: Get a info about all of the pods in the namespace
k8s_info:
kind: Pod
Important
The src and resource_definition parameters of the k8s module do not
behave the same as other modules with a src parameter, which automatically read
from the playbook directory or the role’s files directory. The absolute path is
needed.
3.3. Add a task to scale the deployment up.
Scale the number of replicas to 3 using the k8s_scale module.
...output omitted...
- name: Scale deployment up
k8s_scale:
kind: Deployment
name: hello
replicas: 3
3.4. Add tasks to get information about the route, test the route, and display the content
from the application.
...output omitted...
- name: Get hostname of the route
k8s_info:
kind: Route
name: hello
register: route
- name: Test access to the app
uri:
url: "http://{{ route.resources[0].spec.host }}"
return_content: yes
register: response
until: response.status == 200
54 DO380-OCP4.4-en-2-20200803
Chapter 2 | Introducing Automation with OpenShift
retries: 10
delay: 5
- name: Display response of the application
debug:
var: response.content
3.5. Optionally, compare your playbook to the solution file located at ~/DO380/
solutions/automation-ansible/k8s.yml.
4. Log in as the developer user and run the k8s.yml playbook with at least one level of
verbosity to see the output of the tasks.
[student@workstation automation-ansible]$ oc login -u developer -p developer \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
[student@workstation automation-ansible]$ ansible-playbook -v k8s.yml
Using /etc/ansible/ansible.cfg as config file
PLAY [Demonstrate k8s modules] *************************************************
...output omitted...
PLAY RECAP *********************************************************************
localhost : ok=7 changed=3 unreachable=0 failed=0
skipped=0 rescued=0 ignored=0
5. Inspect the results of the playbook.
5.1. Use the oc get all command to inspect the components created by the playbook.
[student@workstation automation-ansible]$ oc project automation-ansible
Now using project "automation-ansible" on server "https://
api.ocp4.example.com:6443".
[student@workstation automation-ansible]$ oc get all
NAME READY STATUS RESTARTS AGE
pod/hello-76db5c69d5-ghrjj 1/1 Running 0 63s
pod/hello-76db5c69d5-wj4l5 1/1 Running 0 66s
pod/hello-76db5c69d5-xhtwm 1/1 Running 0 63s
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
service/hello ClusterIP 172.30.174.154 <none> 80/TCP 66s
NAME READY UP-TO-DATE AVAILABLE AGE
deployment.apps/hello 3/3 3 3 66s
NAME DESIRED CURRENT READY AGE
replicaset.apps/hello-76db5c69d5 3 3 3 66s
DO380-OCP4.4-en-2-20200803 55
Chapter 2 | Introducing Automation with OpenShift
NAME HOST/PORT
PATH SERVICES PORT TERMINATION WILDCARD
route.route.openshift.io/hello hello-automation-ansible.apps.ocp4.example.com
hello 8080 None
5.2. Use the curl command to verify the application is running.
[student@workstation automation-ansible]$ curl \
> hello-automation-ansible.apps.ocp4.example.com
<html>
<body>
<h1>Hello, world from nginx!</h1>
</body>
</html>
6. Change to the /home/student/ directory.
[student@workstation automation-ansible]$ cd
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab automation-ansible finish
This concludes the section.
56 DO380-OCP4.4-en-2-20200803
Chapter 2 | Introducing Automation with OpenShift
Summary
In this chapter, you learned:
• How to use JSONPath to extract information from OpenShift resources.
• About service accounts and how to use them in Roles and RoleBindings.
• How to execute pods as Jobs and CronJobs.
• About the OpenShift REST API and how to navigate it.
• About the Ansible k8s modules and how to use them to automate OpenShift resources.
DO380-OCP4.4-en-2-20200803 57
58 DO380-OCP4.4-en-2-20200803
Chapter 3
Manage Operators and
OpenShift Cluster Operators
Goal Manage Operators and OpenShift Cluster
Operators
Objectives • Describe Kubernetes controllers and the
Operator pattern.
• Install and troubleshoot operators from the
OperatorHub.
• Identify namespaces and resources of a cluster
operator.
Sections • Describing Operators (and Quiz)
• Installing Operators (and Guided Exercise)
• Managing Cluster Operators (and Guided
Exercise)
DO380-OCP4.4-en-2-20200803 59
Chapter 3 | Manage Operators and OpenShift Cluster Operators
Describing Operators
Objectives
After completing this section, you should be able to describe Kubernetes controllers and the
Operator pattern.
Describing Operators
Kubernetes clusters desired persistent state is encoded using resources (pods, services,
etc). Resources are endpoints in the Kubernetes API that store sets of API objects, which
are responsible for representing the state of the cluster. As an analogy with Object Oriented
programming: resource types can be interpreted as classes and API objects can be seen as
instances of these classes.
Controllers are responsible for maintaining the state of the cluster by following a control loop in
which the controller tracks one or more resource types and makes the current state of the objects
closer to the desired state described within them.
The Kubernetes API is extended by adding Custom Resource Definitions (CRDs). CRDs integrate
with the cluster as if they are native resources. In combination with Custom Controllers, CRDs
define a true declarative API.
The Operator pattern is a combination of CRDs and Custom Controllers that can perform
administrative tasks for API objects or the whole cluster.
Describing Operators in Kubernetes
The Kubernetes control plane runs:
• cloud-controller-manager
• kube-controller-manager
cloud-controller-manager controls standard Kubernetes resources that interact with a cloud
provider. cloud-controller-manager includes:
Node controller
Manages Node resources.
Route controller
Manages cloud routes for node communication.
Service controller
Manages cloud load balancers to publish services.
kube-controller-manager controls standard Kubernetes resources that do not interact with a
cloud provider.
60 DO380-OCP4.4-en-2-20200803
Chapter 3 | Manage Operators and OpenShift Cluster Operators
Describing Operators in OpenShift
OpenShift operators implement OpenShift features like self-healing, updates, and other
administrative tasks, either on resources or cluster wide actions. Operators package, deploy and
manage an OpenShift application.
OpenShift uses Operators operators to manage the cluster by controlling tasks such as upgrades
and self-healing among others. For example, the Cluster Version Operator manages Cluster
Operators and their upgrades.
Additional operators can run applications or add features to OpenShift.
There are several operators in an OpenShift cluster such as:
• The OperatorHub, which is a registry for OpenShift operators.
• The Operator Lifecycle Manager, which installs, updates, and manages operators.
Use either the CLI or the web console to install operators located in the Operator Hub.
References
For more information, refer to the Understanding Operators chapter in the Red Hat
OpenShift Container Platform 4.4 Operators documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/operators/index#olm-whatoperators-
are
Controllers
https://kubernetes.io/docs/concepts/architecture/controller/
Operator pattern
https://kubernetes.io/docs/concepts/extend-kubernetes/operator/
DO380-OCP4.4-en-2-20200803 61
Chapter 3 | Manage Operators and OpenShift Cluster Operators
Quiz
Describing Operators Quiz
Choose the correct answers to the following questions:
1. Which of the following methods is the recommended for installing Operators in an
OpenShift cluster?
a. Helm files
b. manual installation
c. OperatorHub
2. Choose the correct statement about OpenShift Operators.
a. The Operator pattern consist of creating custom resources.
b. Custom resources and a controller are used to create Operators.
c. Kubernetes Operators are humans that manage the cluster.
3. Choose the correct statement Kubernetes controllers.
a. Kubernetes controllers job is to maintain the cluster resources in a desired state.
b. Kubernetes controllers are not part of the cluster.
c. A controller never talks to the OpenShift API.
4. Choose the correct statement about OpenShift Operator custom resource definitions.
a. Kubernetes ignores OpenShift Operators custom resource definitions.
b. Custom resource definitions create pods.
c. OpenShift Operators require the creation of custom resources based on the Operator
CRDs.
This concludes the section.
62 DO380-OCP4.4-en-2-20200803
Chapter 3 | Manage Operators and OpenShift Cluster Operators
Solution
Describing Operators Quiz
Choose the correct answers to the following questions:
1. Which of the following methods is the recommended for installing Operators in an
OpenShift cluster?
a. Helm files
b. manual installation
c. OperatorHub
2. Choose the correct statement about OpenShift Operators.
a. The Operator pattern consist of creating custom resources.
b. Custom resources and a controller are used to create Operators.
c. Kubernetes Operators are humans that manage the cluster.
3. Choose the correct statement Kubernetes controllers.
a. Kubernetes controllers job is to maintain the cluster resources in a desired state.
b. Kubernetes controllers are not part of the cluster.
c. A controller never talks to the OpenShift API.
4. Choose the correct statement about OpenShift Operator custom resource definitions.
a. Kubernetes ignores OpenShift Operators custom resource definitions.
b. Custom resource definitions create pods.
c. OpenShift Operators require the creation of custom resources based on the Operator
CRDs.
This concludes the section.
DO380-OCP4.4-en-2-20200803 63
Chapter 3 | Manage Operators and OpenShift Cluster Operators
Installing Operators
Objectives
After completing this section, you should be able to:
• Install and troubleshoot operators from the OperatorHub.
• List workload and custom resources from an operator.
Install Operators from the OpenShift OperatorHub.
There are several ways to install operators in an OpenShift cluster, including: OperatorHub, Helm
charts, custom yaml files among others. The recommended way to install operators is to install
them from the OperatorHub using either the Web Console or the CLI. Operators that come with
OpenShift (also called Cluster Operators) are managed by the Cluster Version Operator, and
Operators that are installed from the OperatorHub are managed by the the Operator Life cycle
Manager (OLM).
Operators can be installed to access all namespaces or just one depending on the application or
task that the Operator will perform.
Operators follow a maturity model that is divided in phases, which range from basic installation to
full automation.
There are three initial settings that must be defined when installing an Operator.
• Installation mode.
• Update Channel.
• Approval strategy.
To Install an Operator from the OLM using the CLI
• Check available operators.
[student@demo ~]$ oc get packagemanifests -n openshift-marketplace
• Inspect an Operator.
[student@demo ~]$ oc describe packagemanifests metering-ocp -n openshiftmarketplace
Name: metering-ocp
Namespace: openshift-marketplace
...output omitted...
Description: A scheduled or on-off Metering Report summarizes data
based on the query specified.
Display Name: Metering Report
64 DO380-OCP4.4-en-2-20200803
Chapter 3 | Manage Operators and OpenShift Cluster Operators
Kind: Report
Name: reports.metering.openshift.io
Version: v1
Description: A SQL query used by Metering to generate reports.
Display Name: Metering Report Query
Kind: ReportQuery
Name: reportqueries.metering.openshift.io
Version: v1
...output omitted...
In the CLI, there is no direct way to get the Operator channel so a jsonpath query is needed:
[student@demo ~]$ oc get packagemanifests metering-ocp -n openshift-marketplace \
> --output='jsonpath={range .status.channels[*]}{.name}{"\n"}{end}'
• Install the metering operator.
• Create namespace for the operator. It has to be a Kubernetes namespace not a project
apiVersion: v1
kind: Namespace
metadata:
labels:
openshift.io/cluster-monitoring: "true"
gls-demos: "true"
name: openshift-metering
[student@demo ~]$ oc apply -f metering-operator-namespace.yaml
namespace/openshift-metering created
[student@demo ~]$ oc project openshift-metering
Now using project "openshift-metering" on server "https://api.ocpichavero3.
do380.dev.nextcle.com:6443".
• Create an operator group object.
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
labels:
gls-demos: "true"
name: openshift-metering
namespace: openshift-metering
spec:
targetNamespaces:
- openshift-metering
[student@demo ~]$ oc apply -f metering-operator-group.yaml
operatorgroup.operators.coreos.com/openshift-metering created
• Create a subscription object.
DO380-OCP4.4-en-2-20200803 65
Chapter 3 | Manage Operators and OpenShift Cluster Operators
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
labels:
gls-demos: "true"
name: test-openshift-metering
namespace: openshift-metering
spec:
channel: "4.4"
name: metering-ocp
source: redhat-operators
sourceNamespace: openshift-marketplace
Name of the operator.
Namespace in which the operator will run.
Name of the operator to subscribe to.
Catalog source that provides the operator.
Namespace of the catalog source.
[student@demo ~]$ oc apply -f metering-operator-subscription.yaml
The OLM will create the operator and start creating and running the proper resources.
Verify an Operator status
• Check the logs of the OLM pod to verify operator installation.
[student@demo ~]$ oc logs pod/olm-operator-c5599dfd7-nknfx\
> -n openshift-operator-lifecycle-manager
• Inspect the subscription object to verify the operator status.
[student@demo ~]$ oc describe sub test-openshift-metering
Name: test-openshift-metering
Namespace: openshift-metering
Labels: gls-demos=true
Annotations: kubectl.kubernetes.io/last-applied-configuration:
{"apiVersion":"operators.coreos.com/
v1alpha1","kind":"Subscription","metadata":{
"annotations":{},"labels":{"gls-demos":"true"},"name":"tes...
API Version: operators.coreos.com/v1alpha1
Kind: Subscription
...output omitted...
Spec:
Channel: 4.4
Name: metering-ocp
Source: redhat-operators
Source Namespace: openshift-marketplace
...output omitted...
• Listing all operators.
66 DO380-OCP4.4-en-2-20200803
Chapter 3 | Manage Operators and OpenShift Cluster Operators
Installed Operators can be listed by checking ClusterServiceVersions. To list all the installed
Operators, get the ClusterServiceVersions in all namespaces.
[student@demo ~]$ oc get csv -A
Operators can be subscribed to one namespace or to all namespaces. To list the operators
managed by the OLM, list the active subscriptions.
[student@demo ~]$ oc get subs -A
• The status and events from CRs related to a given operator can be viewed by describing the
Operator deployment.
[student@demo ~]$ oc describe deployment.apps/metering-operator | grep -i kind
olm.owner.kind=ClusterServiceVersion
"kind": "MeteringConfig",
"kind": "Report",
"kind": "ReportQuery",
"kind": "ReportDataSource",
"kind": "StorageLocation",
"kind": "PrestoTable",
"kind": "HiveTable",
[student@demo ~]$ oc get crd | grep -i hive
hivetables.metering.openshift.io 2020-05-05T07:49:48Z
[student@demo ~]$ oc describe crd hivetables.metering.openshift.io | grep Kind
Kind: CustomResourceDefinition
Kind: HiveTable
List Kind: HiveTableList
Kind: HiveTable
List Kind: HiveTableList
[student@demo ~]$ oc get HiveTable
No resources found in openshift-metering namespace.
• The Operator elements can be checked by getting all objects in the Operator’s namespace. If
the Operator was installed in all the namespaces, the query should be made in the openshiftoperators
namespace and look for the Operator’s name to find out the elements.
[student@demo ~]$ oc get all -n openshift-metering
NAME READY STATUS RESTARTS AGE
pod/metering-operator-857774844f-trnff 2/2 Running 0 24m
NAME TYPE CLUSTER-IP EXTERNALIP
PORT(S)
AGE
service/metering-ansible-operator-metrics ClusterIP 172.30.72.185 <none>
8383/TCP
24m
NAME READY UP-TO-DATE AVAILABLE AGE
DO380-OCP4.4-en-2-20200803 67
Chapter 3 | Manage Operators and OpenShift Cluster Operators
deployment.apps/metering-operator 1/1 1 1 24m
NAME DESIRED CURRENT READY AGE
replicaset.apps/metering-operator-857774844f 1 1 1 24m
• Events and logs of an Operator can be viewed by using the oc logs command.
[student@demo ~]$ oc logs deployment.apps/metering-operator
error: a container name must be specified for pod metering-operator-857774844ftrnff,
choose one of: [ansible operator]
• Troubleshoot updates of operators.
• Check the Operator’s logs
• Check Operator’s components.
• Check the Operator’s prerequisites.
• Updating an operator from the OLM using the CLI.
Modify the Operators YAML file and apply the changes to the subscription object.
[student@demo ~]$ oc apply -f metering-operator-subscription.yaml
or use
[student@demo ~]$ oc edit sub test-openshift-metering
• Deleting operators.
[student@demo ~]$ oc delete sub test-openshift-metering
References
For more information about the OLM, refer to the Understanding Operators chapter
in the Red Hat OpenShift Container Platform 4.4 Operators documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/operators/index#olm-whatoperators-
are
For more information about adding operators to an OpenShift Cluster, refer to
the Adding Operators to a cluster chapter in the Red Hat OpenShift Container
Platform 4.4 Operators documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/operators/index#olm-addingoperators-
to-a-cluster
68 DO380-OCP4.4-en-2-20200803
Chapter 3 | Manage Operators and OpenShift Cluster Operators
Guided Exercise
Installing Operators
In this exercise you will install an Operator from the OpenShift marketplace.
Outcomes
You should be able to install the openshift-metering Operator using the OLM.
Before You Begin
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise.
This command ensures that the cluster API is reachable and that needed resources are setup
for the exercise.
[student@workstation ~]$ lab operators-install start
Instructions
1. Create a namespace for the Operator.
The metering-operator needs a specific namespace.
1.1. Log into your OpenShift cluster as the admin user.
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
1.2. Create a file called metering-operator-namespace.yaml with the following
content:
apiVersion: v1
kind: Namespace
metadata:
labels:
openshift.io/cluster-monitoring: "true"
name: openshift-metering
1.3. Create the namespace
[student@workstation ~]$ oc apply -f metering-operator-namespace.yaml
namespace/openshift-metering created
1.4. Enter the new namespace.
DO380-OCP4.4-en-2-20200803 69
Chapter 3 | Manage Operators and OpenShift Cluster Operators
[student@workstation ~]$ oc project openshift-metering
Now using project "openshift-metering" on server "https://
api.ocp4.example.com:6443".
2. Create a group for the Operator.
Operators that are installed using the OLM require a group that matches the namespace
name.
2.1. Create a file called metering-operator-group.yaml and add this content:
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
name: openshift-metering
namespace: openshift-metering
spec:
targetNamespaces:
- openshift-metering
2.2. Create the Operator group.
[student@workstation ~]$ oc apply -f metering-operator-group.yaml
operatorgroup.operators.coreos.com/openshift-metering created
3. Create the Operator subscription.
The Operator needs to be subscribed to a namespace.
3.1. Create the subscription object YAML file called metering-operatorsubscription.
yaml.
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
name: ge-openshift-metering
namespace: openshift-metering
spec:
channel: "4.4"
name: metering-ocp
source: redhat-operators
sourceNamespace: openshift-marketplace
3.2. Create the subscription object.
[student@workstation ~]$ oc apply -f metering-operator-subscription.yaml
4. Verify that the Operator was installed successfully.
[student@workstation ~]$ oc describe sub ge-openshift-metering
Name: ge-openshift-metering
Namespace: openshift-metering
70 DO380-OCP4.4-en-2-20200803
Chapter 3 | Manage Operators and OpenShift Cluster Operators
Annotations: kubectl.kubernetes.io/last-applied-configuration:
{"apiVersion":"operators.coreos.com/
v1alpha1","kind":"Subscription","metadata":{
"annotations":{},"labels":{},"name":"ge...
API Version: operators.coreos.com/v1alpha1
Kind: Subscription
...output omitted...
Spec:
Channel: 4.4
Name: metering-ocp
Source: redhat-operators
Source Namespace: openshift-marketplace
...output omitted...
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab operators-install finish
This concludes the section.
DO380-OCP4.4-en-2-20200803 71
Chapter 3 | Manage Operators and OpenShift Cluster Operators
Managing Cluster Operators
Objectives
After completing this section, you should be able to:
• Identify namespaces and resources of a cluster operator.
• Troubleshoot cluster operators.
Describing Cluster Operators
OpenShift uses operators to perform internal cluster functions like authentication, DNS, and the
web console. These Operators are called Cluster Operators due to the internal cluster functions
they provide. Cluster Operators are similar to Operators, with the difference that the Cluster
Version Operator (CVO) manages them instead of the OLM. Cluster Operators were previously
referred to as "Platform Operators".
Note
Not all Cluster Operators define CustomResourceDefinitions nor follow the
Controller pattern to the letter.
To list the cluster operators and their status:
[user@demo ~]$ oc get clusteroperator
NAME VERSION AVAILABLE PROGRESSING DEGRADED SINCE
authentication 4.4.6 True False False 4d15h
cloud-credential 4.4.6 True False False 4d15h
cluster-autoscaler 4.4.6 True False False 4d15h
console 4.4.6 True False False 160m
csi-snapshot-controller 4.4.6 True False False 4d15h
...output omitted...
AVAILABLE
The Cluster Operator is working correctly
PROGRESSING
The Cluster Version Operator is making changes to this operator.
DEGRADED
The Cluster Operator has detected a problem and it may not be working correctly.
The console also shows this information in Administration ® Cluster Settings ® Cluster
Operators.
Describing the Cluster Version Operator
The Cluster Version Operator main task is to manage installation and upgrades of Cluster
Operators. During installation and upgrades, the Cluster Version operator scans a release image,
locating Cluster Operators and applying their resources.
72 DO380-OCP4.4-en-2-20200803
Chapter 3 | Manage Operators and OpenShift Cluster Operators
Examine the contents of the release image:
[user@demo ~]$ VER=$(oc get clusterversion -o \
> jsonpath='{.status.desired.image}' version)
[user@demo ~]$ oc adm release extract --from=$VER --to=release-image
[user@demo ~]$ grep -l "kind: ClusterOperator" release-image/*
...output omitted...
release-image/0000_50_cluster-samples-operator_07-clusteroperator.yaml
...output omitted...
[user@demo ~]$ cat release-image/*-samples*clusterop*
apiVersion: config.openshift.io/v1
kind: ClusterOperator
metadata:
name: openshift-samples
spec: {}
status:
versions:
- name: operator
version: "4.4.6"
OpenShift Cluster Operators
This is a list of the cluster operators and where to get information about each of them:
dns
Manages DNS with CoreDNS using the DNS custom resource.
For more information, refer to the DNS Operator in OpenShift Container Platform chapter
in the Red Hat OpenShift Container Platform 4.4 Networking documentation at https://
access.redhat.com/documentation/en-us/openshift_container_platform/4.4/html-single/
networking/index#dns-operator
The cluster-dns-operator project in the GitHub OpenShift organization [https://
github.com/openshift/cluster-dns-operator]
ingress
Manages access to cluster services with HAProxy using the Route and Ingress custom
resources.
For more information, refer to the Ingress Operator in OpenShift Container Platform chapter
in the Red Hat OpenShift Container Platform 4.4 Networking documentation at https://
access.redhat.com/documentation/en-us/openshift_container_platform/4.4/html-single/
networking/index#configuring-ingress
The cluster-ingress-operator project in the GitHub OpenShift organization [https://
github.com/openshift/cluster-ingress-operator]
network
Manages networking using the Network custom resource.
For more information, refer to the Cluster Network Operator in OpenShift Container Platform
chapter in the Red Hat OpenShift Container Platform 4.4 Networking documentation at
https://access.redhat.com/documentation/en-us/openshift_container_platform/4.4/htmlsingle/
networking/index#cluster-network-operator
DO380-OCP4.4-en-2-20200803 73
Chapter 3 | Manage Operators and OpenShift Cluster Operators
The cluster-network-operator project in the GitHub OpenShift organization [https://
github.com/openshift/cluster-network-operator]
node-tuning
Manages the tuned daemon on nodes using the Tuned custom resource.
For more information, refer to the Using the Node Tuning Operator chapter in the Red Hat
OpenShift Container Platform 4.4 Scalability and Performance documentation at https://
access.redhat.com/documentation/en-us/openshift_container_platform/4.4/html-single/
scalability_and_performance/index#using-node-tuning-operator
The cluster-node-tuning-operator project in the GitHub OpenShift organization
[https://github.com/openshift/cluster-node-tuning-operator]
openshift-samples
Manages sample imagestreams and templates.
For more information, refer to the Configuring the Samples Operator chapter in the Red Hat
OpenShift Container Platform 4.4 Images documentation at https://access.redhat.com/
documentation/en-us/openshift_container_platform/4.4/html-single/images/
index#configuring-samples-operator
authentication
Manages authentication using the OAuth custom resource.
The cluster-authentication-operator project in the GitHub OpenShift organization
[https://github.com/openshift/cluster-authentication-operator]
cloud-credential
Requests credentials to cloud providers using the CredentialsRequest custom resource.
The cloud-credential-operator project in the GitHub OpenShift organization [https://
github.com/openshift/cloud-credential-operator]
cluster-autoscaler
Manages scaling of the cluster using the ClusterAutoscaler and MachineAutoscaler
custom resources.
The cluster-autoscaler-operator project in the GitHub OpenShift organization
[https://github.com/openshift/cluster-autoscaler-operator]
console
Manages the web console using the Console custom resource.
The console-operator project in the GitHub OpenShift organization [https://github.com/
openshift/console-operator]
image-registry
Manages the registry.
The cluster-image-registry-operator project in the GitHub OpenShift organization
[https://github.com/openshift/cluster-image-registry-operator]
insights
Reports anonymized system configuration to Red Hat Insights.
The insights-operator project in the GitHub OpenShift organization [https://
github.com/openshift/insights-operator]
74 DO380-OCP4.4-en-2-20200803
Chapter 3 | Manage Operators and OpenShift Cluster Operators
kube-apiserver
Manages the Kubernetes API server using the KubeAPIServerConfig custom resource.
The cluster-kube-apiserver-operator project in the GitHub OpenShift organization
[https://github.com/openshift/cluster-kube-apiserver-operator]
kube-controller-manager
Manages the Kubernetes Controller Manager using the KubeControllerManager custom
resource.
The cluster-kube-controller-manager-operator project in the GitHub OpenShift
organization [https://github.com/openshift/cluster-kube-controller-manager-operator]
kube-scheduler
Manages the Kubernetes Scheduler using the KubeScheduler custom resource.
The cluster-kube-scheduler-operator project in the GitHub OpenShift organization
[https://github.com/openshift/cluster-kube-scheduler-operator]
machine-api
Manages provisioning of machines using the MachineSet, Machine and
MachineHealthCheck custom resources.
The machine-api-operator project in the GitHub OpenShift organization [https://
github.com/openshift/machine-api-operator]
machine-config
Manages configuration of machines using the ControllerConfig, MachineConfigPool,
MachineConfig, KubeletConfig and ContainerRuntimeConfig custom resources.
[The machine-config-operator project in the GitHub OpenShift organization [https://
github.com/openshift/machine-config-operator]
marketplace
Manages the Marketplace.
monitoring
Manages the monitoring stack.
The cluster-monitoring-operator project in the GitHub OpenShift organization
[https://github.com/openshift/cluster-monitoring-operator]
openshift-apiserver
Manages the OpenShift API Server using the OpenShiftAPIServerConfig custom
resource.
The cluster-openshift-apiserver-operator project in the GitHub OpenShift
organization [https://github.com/openshift/cluster-openshift-apiserver-operator]
openshift-controller-manager
Manages the OpenShift Controller using the OpenShiftControllerManager custom
resource.
The cluster-openshift-controller-manager-operator project in the GitHub
OpenShift organization [https://github.com/openshift/cluster-openshift-controller-manageroperator]
operator-lifecycle-manager, operator-lifecycle-manager-catalog, operatorlifecycle-
manager-packageserver
Manage operators.
DO380-OCP4.4-en-2-20200803 75
Chapter 3 | Manage Operators and OpenShift Cluster Operators
service-ca
Manages serving certificates using the ServiceCA custom resource.
The service-ca-operator project in the GitHub OpenShift organization [https://
github.com/openshift/service-ca-operator]
service-catalog-apiserver
Manages the Service Catalog API Server using the ServiceCatalogAPIServer custom
resource.
The cluster-svcat-apiserver-operator project in the GitHub OpenShift organization
[https://github.com/openshift/cluster-svcat-apiserver-operator]
service-catalog-controller-manager
Manages the Service Catalog using the ServiceCatalogControllerManager custom
resource.
The cluster-svcat-controller-manager-operator project in the GitHub OpenShift
organization [https://github.com/openshift/cluster-svcat-controller-manager-operator]
storage
Manages storage defaults.
The cluster-storage-operator project in the GitHub OpenShift organization [https://
github.com/openshift/cluster-storage-operator]
References
For more information on Cluster Operators, refer to the Operators in OpenShift
Container Platform section in the The OpenShift Container Platform control
plane chapter in the Red Hat OpenShift Container Platform 4.4 Architecture
documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/architecture/index#operatorsoverview_
control-plane
76 DO380-OCP4.4-en-2-20200803
Chapter 3 | Manage Operators and OpenShift Cluster Operators
Guided Exercise
Managing Cluster Operators
In this exercise you will examine the Cluster Samples Operator.
Outcomes
You should be able to:
• View the Cluster Samples Operator components and their logs.
Before You Begin
Sample prerequisites
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise.
This command ensures that the cluster API is reachable.
[student@workstation ~]$ lab operators-cluster start
Instructions
1. Examine the Cluster Samples Operator components. To do so, extract the manifests that
define them from the release image.
1.1. Log into your OpenShift cluster as the admin user.
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
1.2. Extract the release.
[student@workstation ~]$ VER=$(oc get clusterversion version \
> -o jsonpath='{.status.desired.image}')
[student@workstation ~]$ oc adm release extract --from=$VER --to=release-image
1.3. View the Cluster Samples Operator manifests.
[student@workstation ~]$ ls release-image/*samples*
1.4. Examine the Cluster Samples Operator deployment.
[student@workstation ~]$ less release-image/*samples*06-operator.yaml
DO380-OCP4.4-en-2-20200803 77
Chapter 3 | Manage Operators and OpenShift Cluster Operators
2. Examine the Cluster Samples Operator components.
2.1. List the Cluster Samples Operator pods.
[student@workstation ~]$ oc get pods -n openshift-cluster-samples-operator
NAME READY STATUS RESTARTS AGE
cluster-samples-operator-5d7d5b94b6-rw284 2/2 Running 0 4d15h
2.2. View Cluster Samples Operator events.
[student@workstation ~]$ oc get -n openshift-cluster-samples-operator events
LAST SEEN TYPE REASON OBJECT MESSAGE
...output omitted...
2.3. View logs for the Cluster Samples Operator pods.
[student@workstation ~]$ oc logs pod/cluster-samples-operator-5d7d5b94b6-rw284 \
> -n openshift-cluster-samples-operator --all-containers
...output omitted...
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab operators-cluster finish
This concludes the section.
78 DO380-OCP4.4-en-2-20200803
Chapter 3 | Manage Operators and OpenShift Cluster Operators
Summary
In this chapter, you learned:
• What operators and controllers are.
• How to install operators from the OpenShift OperatorHub.
• How to examine cluster operators.
DO380-OCP4.4-en-2-20200803 79
80 DO380-OCP4.4-en-2-20200803
Chapter 4
Implementing GitOps with
Jenkins
Goal Implement a GitOps workflow using containerized
Jenkins to administer an OpenShift cluster.
Objectives • Describe Jenkins concepts and Jenkins
Pipeline concepts.
• Deploy Jenkins on OpenShift using standard
templates and image streams.
• Configure cluster operators using resource files
in a declarative fashion.
• Automate configuration of cluster operators
from resources in Git using Jenkins Pipelines.
Sections • Introducing Jenkins Declarative Pipelines (and
Quiz)
• Deploying Jenkins on OpenShift (and Guided
Exercise)
• Configuring OpenShift Resources Using a
Declarative GitOps Workflow (and Guided
Exercise)
• Configuring OpenShift using GitOps and
Jenkins (and Guided Exercise)
DO380-OCP4.4-en-2-20200803 81
Chapter 4 | Implementing GitOps with Jenkins
Introducing Jenkins Declarative Pipelines
Objectives
After completing this section, you should be able to describe Jenkins concepts and Jenkins
Pipeline concepts.
Introducing Continuous Integration (CI) and
Continuous Deployment (CD)
Jenkins is currently the most popular tool for automating software development processes. It
was created as a tool for managing software build processes, which evolved into the Continuous
Integration (CI) process, and also proved to be flexible enough to support Continuous Deployment
(CD) processes and GitOps processes.
The following figure shows a sample CI/CD workflow and illustrates how CI and CD processes are
usually integrated into a larger process that allows a developer to make changes to an application
source code and have quick access to a running instance of that application.
Continuous Integration (CI) Workflows
Continuous Integration (CI) is the process of automatically building a software piece from its
source code and any required dependencies, such as programming libraries, whenever the source
code changes.
To be successful and reliable, a CI workflow requires automated testing such as unit testing. A
more sophisticated CI workflow could include many testing activities, such as security scanning
and code coverage reports.
82 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
The final software artifact generated by a CI process could be anything that makes sense as a
distributable piece of software, for example: a native executable file, an RPM package, a Java
library archive (JAR file), or a container image.
Continuous Delivery (CD) Workflows
Continuous Deployment (CD) is the process of automatically deploying a software piece to a
target environment, such as development or production, whenever the source code changes.
To be successful and reliable, a CI workflow also requires automated testing such as integration
testing. It is also common to add manual approval gates to a CI workflow because other testing
activities, such as functional testing and user acceptance testing may not be fully automated.
CD workflows may depend on accessing a preconfigured target environment, and be limited by the
availability of that environment. More sophisticated CD workflows may be able to create a target
environment in a cloud environment, for example by running Ansible Playbooks.
Container runtimes and Kubernetes clusters are also popular as quick ways to provide temporary
target environments for running tests before deploying in a real production environment.
GitOps Workflows
The following figure shows a sample GitOps workflow that you can compare and contrast with the
previous sample CI/CD workflow.
GitOps is a process that stores configurations of a target environment, such as a server or a
cluster, in files managed by a version control system. It works under the assumption that system
administrators do not change configurations on a live system directly. They are supposed to
change the configurations under version control and optionally have a fellow system administrator
review them before applying to a live system.
Typical GitOps workflows have two pieces: one that takes configurations from version control and
applies those configurations to a live systems, and another that verifies if the configurations from
live systems have changed (drifted) compared to the current configuration under version control.
DO380-OCP4.4-en-2-20200803 83
Chapter 4 | Implementing GitOps with Jenkins
Comparing CI/CD and GitOps
CI/CD workflows are designed from a developer’s point of view and GitOps workflows are
designed from a system administrator’s point of view. Both GitOps and CI/CD assume that
input comes from a version control system, and that something automatically reacts to changes
committed to version control to produce some desired side effect.
The side effect from a CI/CD workflow is a running application, and the side effect of a GitOps
workflow is a live configuration.
It is a common pattern that a CI/CD process generates artifacts that serve as input to a GitOps
process. For example, a CI/CD workflow generates a container image for an application and
tests that application in a stage environment, and a GitOps Workflow sets up the production
environment for that same application.
Most tools that are capable or automating CI/CD processes are also capable of automating
GitOps processes. For some organizations, tools designed for GitOps are replacing a CI/CD tools
to perform the CD part of the workflow.
Introducing Tools for CI/CD
OpenShift features such as Source-to-Image (S2I), build configurations, deployment
configurations, and image streams work together to implement simple CI/CD processes. If you
deploy an application from source code using the oc new-app command, OpenShift setups
these resources in a working CI/CD workflow.
When you require more flexibility and power than provided by S2I, OpenShift integrates well with
external CI/CD tools such as Jenkins. OpenShift includes container images and templates to
deploy containerized Jenkins servers that are discussed elsewhere in this course.
Other popular CI/CD tools include OpenShift Pipelines, based on the Tekton open source project
(Technical Preview as of Red Hat OpenShift Container Platform 4.4), and Jenkins X, an open
source project that is creating a cloud-native variant of Jenkins.
There are also newer tools that specializes in GitOps processes, such as ArgoCD. They are usually
not flexible enough to implement complete CI/CD workflows, but they compensate by making
simpler to implement and manage GitOps workflows.
It is starting to become a recommended best practice withing the Kubernetes community to
combine a Kubernetes-native CI/CD tool, such as OpenShift Pipelines, with a Kubernetes-native
GitOps tools, such as GitOps, in larger integrated processes.
Introducing Jenkins
Jenkins is the leading open source automation server; it supports building, deploying, and
automating any software development task.
Jenkins started as the Hudson build server, with a focus on building Java applications. With time
it evolved into a more general build automation server, and then become capable of automating
more generic CI and other software development workflows.
IT organizations that already have an investment in Jenkins favors using it to implement GitOps
workflows. Some organizations looking into GitOps realize that they also need to implement
CI/CD to realize the benefits of GitOps and opt to automate it all with Jenkins. For other
organizations the fact that Jenkins is not Kubernetes-native is considered a major disadvantage
and they look after other alternatives.
84 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
Jenkins is a Java application that can be deployed to Servlet containers, such as Tomcat and
Jetty, or application servers such as JBoss EAP. Because Jenkins is coded in Java, Jenkins servers
ran can on any operating system and that contributed to its popularity.
Despite being coded in Java, Jenkins is known for its wide ecosystem of plug-ins that target most
other popular programming language runtimes.
Describing Essential Jenkins Concepts
Jenkins is a very sophisticated automation server. The following describe essential concepts and
terms for using and managing Jenkins.
Project (or Job)
A script that describe a workflow that Jenkins should perform, such as building an application.
Pipeline
A kind of Job that follows the pipeline concept and syntax and describes a workflow as a
sequence of steps that are grouped in stages.
Build
A single execution of a project, including its runtime logs and output artifacts. The term comes
from Jenkins' origins as a software build server.
Node
A server or container that runs builds.
Worker
A thread in a master node that either runs a build or orchestrates available agents.
Workspace
A file system folder, dedicated to a project and sometimes also to a node, where builds store
data that is either temporary, or reused between multiple builds of the same project.
Credential
A Jenkins construct that provides projects and builds with access credentials to external
resources. There are different kinds of credentials to store user name and password pairs, SSH
keys, and other credential types.
Plug-in
Almost all of Jenkins functionalities are extensible by plug-ins written in Java. There are many
community plug-ins that support different kinds of nodes, credentials, and programming
languages.
Types of Jenkins Nodes
There are two kinds of Jenkins nodes:
Master
Stores definitions of projects and their builds.
Agent
Run builds (or parts of a build) under the control of a master node.
A Jenkins instance contains one (and only one) master node and zero or more agent nodes. A
master node is also an agent node, so a simple Jenkins deployment can have a single node.
The following figure shows how the previous concepts of Jenkins projects and builds relate to the
types of Jenkins nodes.
DO380-OCP4.4-en-2-20200803 85
Chapter 4 | Implementing GitOps with Jenkins
Figure 4.3: Jenkins Architecture
A master node provides a Web UI and REST APIs to manage projects and builds.
Agents can be bare-metal servers, local containers, or pods in a Kubernetes cluster. Agent nodes
also spread the load of running jobs, freeing up resources and capacity of a master node.
You can tune agent nodes according to specific requirements of an application, such as the version
of an operating system, independent of the masters. For example: a Jenkins master running Linux
could use a Jenkins agent running Windows to for a multi-platform application.
Managing Jenkins
The Jenkins Web UI, also known as the Jenkins Dashboard because of its welcome page, is a web
application that enables managing all aspects of a Jenkins server. The following list illustrates the
main things you can manage through the Jenkins web UI:
• Projects and builds for each project.
• Folders that group projects related to a business unit.
• Security settings that allow a Jenkins user to manage the server or only those projects the user
owns.
• Installed and available plug-ins.
• Online and offline agent nodes.
Jenkins also provides both a Java-based CLI and a REST API that allow limited management of a
Jenkins server, including creating projects and starting builds.
86 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
A standard Jenkins setup provides few automation capabilities and is very GUI-oriented. A
number of plug-ins, for example the Configuration as Code (CasC) plug-in, add more extensive
automation capabilities to Jenkins. These plug-ins are beyond the scope for this course.
Introducing Jenkins Declarative Pipelines
You can define a pipeline from the Jenkins Web UI, using an interactive graphical builder and store
it in a Jenkins instance. Nowadays the preferred approach is writing a Jenkinsfile using a text
editor and storing it on a version control system such as Git.
A Jenkinsfile is a text file using Groovy syntax that is very similar to Java and JavaScript. There are
two possible styles for writing a Jenkinsfile:
Scripted pipelines
Pipelines that start with a node directive and define imperative scripts using the full Groovy
programming language.
Declarative pipelines
Pipelines that start with a pipeline directive and define declarative scripts using a specialpurpose
domain-specific language (DSL) that is a subset of Groovy.
Many Jenkins plug-ins provide DSL statements and directives to perform tasks such as: running
Java Maven builds, running NUnit integration tests, resolving NPM dependencies, and building
container images. Note that these plug-ins may depend on the availability of software tools such
as the Java Development Kit (JDK) and the Node Package Manager (NPM) on agent nodes.
The declarative style is preferred because it is simpler. If you find a plug-in whose DSL statements
require the scripted style, you can add script blocks to a declarative pipeline and use these
statements.
Describing the Structure of a Jenkinsfile
The following illustrates a minimal Jenkinsfile, using the declarative style, that includes comments
and variable substitution.
pipeline {
agent any
stages {
stage('Example') {
// use interactive input judiciously
input {
message 'Should we continue?'
ok 'Yes, we should.'
parameters {
string(name: 'PERSON', defaultValue: 'Mr Jenkins',
description: 'Who should I say hello to?')
}
}
steps {
echo "Hello, ${PERSON}, nice to meet you."
}
}
}
}
Most directives require brackets.
DO380-OCP4.4-en-2-20200803 87
Chapter 4 | Implementing GitOps with Jenkins
This entire pipeline runs in any agent available, including the master node.
The Jenkins Web UI displays the name of each stage and its overall status for each build.
Example of a single-line comment.
Sample input directive that displays a text entry box and assigns the value to the PERSON
variable.
Line breaks and indentation are allowed inside a statement or directive.
Pipelines (and Groovy) support string quoting, escaping, and variable substitution rules
similar to Bash commands.
Most directives inside a declarative pipeline are optional.
• The pipeline directive requires one agent and one stages directive.
• The stages directive requires one or more stage directives.
• A stage directive requires one or more steps directives.
The following directives are allowed inside pipelines directive:
• triggers: defines conditions that fire automatic execution of builds from that pipeline.
• options: defines general configuration settings for the pipeline, overriding most properties
from the Web UI, for example: timeouts for running builds and retention of build logs.
• parameters: defines parameters that a user, or an upstream pipeline, can provide for running a
build.
• environment: defines environment variables available inside a pipeline or a stage.
• agent: defines which agent nodes should execute either all stages or a single stage of the
pipeline.
Some directives, for example agent, may occur at different levels inside a pipeline, for example at
either pipeline or stage. Most directives can be defined in any order inside a pipeline or stage.
The stage directive defines a logical piece of a workflow, for example, building, testing, or
deploying an application. Besides its mandatory steps directive and the optional directives that
were already introduced, a stage can also include:
• when: defines conditions under which the stage is executed.
• input: allows a stage to display interactive prompts and wait for an answer.
Inside a steps directive you include statements such as echo, input, and sh that are provided by
Jenkins and its plug-ins.
An optional post directive may exist inside either the pipeline or stages directives and defines
steps that execute after a stage (or all stages) are completed, to allow tasks such as clean up and
recovery an error.
88 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
References
CI/CD from the Wikipedia at
https://en.wikipedia.org/wiki/CI/CD
Jenkins Glossary at
https://www.jenkins.io/doc/book/glossary/
Jenkins Pipeline Syntax at
https://www.jenkins.io/doc/book/pipeline/syntax/
DO380-OCP4.4-en-2-20200803 89
Chapter 4 | Implementing GitOps with Jenkins
Quiz
Introducing Jenkins Declarative Pipelines
Quiz
Choose the correct answers to the following questions:
1. Which two of the following statements about Jenkins, CI/CD, and GitOps are correct?
(Choose two.)
a. Jenkins is a general-purpose process automation server that is popular among software
developers.
b. Jenkins is a container-native automation service that orchestrates containers as part of a
larger process.
c. Jenkins is an automation server created to manage CI processes.
d. Jenkins is an automation server created to manage GitOps processes.
2. Choose the correct statement about Jenkins nodes:
a. Master nodes can only manage projects and cannot run builds.
b. Both master and agent nodes can manage projects and run builds.
c. Agent nodes cannot manage projects nor run builds.
d. Both master and agent nodes can run builds.
3. Choose the correct statement about managing Jenkins:
a. A standard Jenkins deployment requires using a web UI to perform most management
tasks.
b. The Jenkins REST API provides easy access to all settings of a Jenkins instance and its
agent nodes.
c. The Jenkins CLI can automatically generate pipelines for common scenarios.
d. The properties directive of a declarative pipeline offers a limited subset of the project
settings available through the Jenkins web UI.
4. Choose the correct statement about Jenkins pipeline projects:
a. Scripted pipelines are designed for CI/CD processes and declarative pipelines are
designed for GitOps projects.
b. Writing scripted pipelines requires basic knowledge of Groovy programming.
c. Only declarative pipelines support running builds on containerized agents.
d. Only scripted pipelines can make use of most Jenkins plug-ins.
90 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
5. Given the structure of Jenkinsfiles and declarative pipelines, which two statements are
correct? (Choose two.)
a. The agent directive allows selecting an agent that is preconfigured with requirements of
the pipeline.
b. The stage directive provides shortcuts for performing common development tasks
without any coding.
c. The steps directive contains statements that perform that the actual tasks of building,
testing, and deploying pieces of software.
d. All directives except steps are optional in a declarative pipeline.
e. All statements inside a steps directive come from specialized plug-ins and there is no
way of running standard shell commands.
f. The post directive includes statements that are executed only after all stages are
successful.
This concludes the section.
DO380-OCP4.4-en-2-20200803 91
Chapter 4 | Implementing GitOps with Jenkins
Solution
Introducing Jenkins Declarative Pipelines
Quiz
Choose the correct answers to the following questions:
1. Which two of the following statements about Jenkins, CI/CD, and GitOps are correct?
(Choose two.)
a. Jenkins is a general-purpose process automation server that is popular among software
developers.
b. Jenkins is a container-native automation service that orchestrates containers as part of a
larger process.
c. Jenkins is an automation server created to manage CI processes.
d. Jenkins is an automation server created to manage GitOps processes.
2. Choose the correct statement about Jenkins nodes:
a. Master nodes can only manage projects and cannot run builds.
b. Both master and agent nodes can manage projects and run builds.
c. Agent nodes cannot manage projects nor run builds.
d. Both master and agent nodes can run builds.
3. Choose the correct statement about managing Jenkins:
a. A standard Jenkins deployment requires using a web UI to perform most management
tasks.
b. The Jenkins REST API provides easy access to all settings of a Jenkins instance and its
agent nodes.
c. The Jenkins CLI can automatically generate pipelines for common scenarios.
d. The properties directive of a declarative pipeline offers a limited subset of the project
settings available through the Jenkins web UI.
4. Choose the correct statement about Jenkins pipeline projects:
a. Scripted pipelines are designed for CI/CD processes and declarative pipelines are
designed for GitOps projects.
b. Writing scripted pipelines requires basic knowledge of Groovy programming.
c. Only declarative pipelines support running builds on containerized agents.
d. Only scripted pipelines can make use of most Jenkins plug-ins.
92 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
5. Given the structure of Jenkinsfiles and declarative pipelines, which two statements are
correct? (Choose two.)
a. The agent directive allows selecting an agent that is preconfigured with requirements of
the pipeline.
b. The stage directive provides shortcuts for performing common development tasks
without any coding.
c. The steps directive contains statements that perform that the actual tasks of building,
testing, and deploying pieces of software.
d. All directives except steps are optional in a declarative pipeline.
e. All statements inside a steps directive come from specialized plug-ins and there is no
way of running standard shell commands.
f. The post directive includes statements that are executed only after all stages are
successful.
This concludes the section.
DO380-OCP4.4-en-2-20200803 93
Chapter 4 | Implementing GitOps with Jenkins
Deploying Jenkins on OpenShift
Objectives
After completing this section, you should be able to:
• Deploy Jenkins on OpenShift using standard templates and image streams.
• Deploy a CI/CD development pipeline on Jenkins.
• Manage Jenkins projects and builds using the Jenkins web UI.
Introducing Red Hat Jenkins Images and Templates for
OpenShift
Red Hat provides a few supported Jenkins images as part of Red Hat OpenShift Container
Platform, including:
registry.redhat.io/openshift4/ose-jenkins
A Jenkins master image that is preconfigured with a number of Jenkins plug-ins that integrate
with OpenShift.
registry.redhat.io/openshift4/ose-jenkins-agent-maven
A Jenkins agent image that provides Java development tools.
registry.redhat.io/openshift4/ose-jenkins-agent-nodejs
A Jenkins agent image that provides Node.js development tools.
registry.redhat.io/openshift4/ose-jenkins-agent-base
A base image for building custom agent images. You use that base image if to create a new
Jenkins agent image if the previous agent images do not provide tools that you need.
All of Red Hat’s Jenkins agent images also include the OpenShift CLI because it is a requirement
of the OpenShift Pipeline DSL plug-in.
Issues related to these images, such as security vulnerabilities, fall under the Service-Level
Agreement (SLA) of the Red Hat OpenShift Container Platform if you deploy these images on
OpenShift clusters. Other ways of deploying Jenkins, including using these same images under a
local container runtime, are supported in a best-effort basis.
OpenShift also provides predefined image streams for the master, Java agent, and Node.js agent
in the openshift namespace.
The recommended way to deploy Jenkins on OpenShift is using the predefined templates in the
openshift namespace, among them:
• jenkins-ephemeral deploys a Jenkins server using ephemeral storage for quick testing.
• jenkins-persistent deploys a Jenkins server using persistent storage for development and
production environments.
There are also versions of these templates that deploy Jenkins integrated with OpenShift
cluster Alerting and Metering. As of Red Hat OpenShift Container Platform 4.4 these monitoring
templates had known issues and the fixes are expected for version 4.5.
94 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
Describing Jenkins Plug-ins For OpenShift
The Jenkins master image from Red Hat includes a number of plug-ins that allow Jenkins to work
integrated with an OpenShift cluster:
Kubernetes plug-in
Creates agents on-demand as pods.
Kubernetes Credentials plug-in
Authenticates to OpenShift using credentials from a service account.
OpenShift Sync plug-in
Creates Jenkins credentials, accounts, and agents from OpenShift image streams,
configuration maps, service accounts, and secrets.
OpenShift Jenkins Pipeline DSL
Also known as OpenShift Client plug-in, provides imperative DSL statements that deploy
applications and manage OpenShift resources.
These plug-ins are Open Source software so nothing prevents you from adding these plug-ins to
an external Jenkins instance, running outside of OpenShift. That Jenkins instance could manage
both projects that target OpenShift and also projects that target other deployment platforms.
If you decide to do so, remember that the Red Hat OpenShift Platform Service-Level Agreement
(SLA) only covers scenarios where Jenkins is deployed on OpenShift and using Red Hat’s
container images for Jenkins. Support for Jenkins servers deployed outside of OpenShift and for
using the OpenShift Jenkins plug-ins outside of the Red Hat’s Jenkins images, is best-effort only.
If you require additional Jenkins plug-ins that do not come preinstalled with Red Hat’s Jenkins
container images, the product documentation describes how to use Source-to-Image (S2I) to
build child Jenkins images and these images are fully supported, except for issues related to the
plug-ins you add, which are supported as best-effort.
Deploying Jenkins on OpenShift Using Standard
Templates
All Jenkins templates provided with OpenShift create a deployment configuration to manage an
Jenkins master pod. These templates do not deploy any agent pod. These are created on demand,
as required by Jenkins builds, and the Jenkins master takes the role of a controller for its agent
pods.
If you choose a persistent template to deploy Jenkins, it also creates a persistent volume claim for
the Jenkins configuration folder. You can use that same volume to store build artifacts in the lack
of an external artifact server, such as a Nexus server or a container image registry.
If you run builds on the master node, these builds also reuse the volume to store workspace
folders. If you run builds on agent pods, these builds use ephemeral storage for workspace folders
so every build starts from a clean state. If you need to save artifacts from builds running on agent
pods, your pipeline has to send artifacts for storage in an external server or in the master node.
Do not scale the Jenkins deployment configuration to multiple replica pods because Jenkins does
not support multiple instances writing to the same configuration folder.
That is, High Availability (HA) for a Jenkins instance on OpenShift comes from Kubernetes ability
to restart a pod managed in a deployment configuration, and from the storage provider’s ability to
retain data in persistent volumes.
DO380-OCP4.4-en-2-20200803 95
Chapter 4 | Implementing GitOps with Jenkins
The standard templates define parameters and default values for persistent volume size and
memory requirements, among others. Small Jenkins instances require no changes to these default
parameter values and could be deployed by just creating a project and then running:
[user@demo ~]$ oc new-app --template jenkins-persistent
The standard templates also create a service account named jenkins for all interactions with the
OpenShift cluster. The Jenkins service account is assigned the edit role on its OpenShift project,
so it is ready to deploy applications to the same project that runs the Jenkins instance.
If your Jenkins instance requires additional rights, you must add more project or cluster roles to
that service account, for example: to allow Jenkins to create resources in other projects.
Deployment Topologies for Jenkins on OpenShift
In theory a single, large Jenkins instance could manage all projects in all OpenShift clusters
for an organization. Some organizations prefer this kind of central CI/CD server that is strictly
managed by a corporate IT operations team. That large Jenkins instance could hold credentials to
authenticate to multiple OpenShift clusters.
Because such a large Jenkins instance may be hard to manage, many organizations run multiple
Jenkins instances in a single cluster or across multiple OpenShift clusters.
It is also easier to manage permissions for these Jenkins instances if they are in different projects.
A common pattern is: each development group inside an organization manages its own Jenkins
instance, and that instance does not manage projects from other groups.
That way the IT operations team is free of the burden of managing Jenkins instances for
development teams. OpenShift role-based access permissions, combined with OpenShift
resource limits and resource quotas, are sufficient to ensure that these development teams cannot
break their clusters with their pipelines. And developers are able to grant access to Jenkins to
projects that they own.
Organizations that use Jenkins to implement GitOps workflows can create one or more dedicated
Jenkins instances with either cluster administrator privileges or custom roles that allow limited
access to a few cluster operators and not share these instances with application developers.
Mapping OpenShift Roles to Jenkins User Permissions
Jenkins provides authentication and authorization mechanisms that can be mostly ignored when
using the images from Red Hat. These images automatically translate standard OpenShift project
roles into Jenkins permissions as follows:
• admin: manages the Jenkins instance.
• edit: create and edit projects in the Jenkins instance.
• view: view status, logs, and artifacts of builds from the Jenkins instance.
When an OpenShift user logs in Jenkins, the OpenShift Sync plug-in automatically defines a
Jenkins user by concatenating the OpenShift user name and its roles. The same Jenkins plug-in
manages translation of OpenShift role bindings to Jenkins permissions.
Creating a Pipeline Project Using the Jenkins Web UI
Older Red Hat OpenShift Container Platform releases supported embedding Jenkins pipelines
inside a Build Configuration resource, using the pipeline strategy. That feature and the integration
96 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
of these pipeline build configurations with the OpenShift web console allowed OpenShift users to
avoid the Jenkins web UI for basic tasks such as creating a pipeline project and running builds.
Red Hat OpenShift Container Platform 4.3 deprecated the pipeline build strategy because
build configurations are to be replaced by a new build system and operator, integrated with the
OpenShift Pipelines feature. Note that the Jenkins container images, templates, and plug-ins
are not deprecated. Only the integration with Jenkins and OpenShift build configurations is
deprecated.
Jenkins users are advised to either use the Jenkins web UI to manage pipeline projects and builds,
or to add plug-ins such as the Configuration as Code (CasC) plug-in.
To access the Jenkins web interface, just take the host name of the single route created by the
Jenkins template. Open that host name with a web browser and you get an OpenShift log in page.
After a successful log in, you get the Jenkins Dashboard page.
The New Item link in the Jenkins Dashboard page allow you to create many project types and
other Jenkins configuration items, such as: folders to group projects and projects of different
types.
The two most common project types are:
Pipeline
Runs a pipeline taking as input a single branch from a version control system repository.
Multibranch pipeline
Automatically creates new projects when new branches are detected on a version control
system repository. All these projects share the same pipeline definition, that should be flexible
enough to not create conflicts between builds of different branches.
Managing Pipeline Builds Using the Jenkins Web UI
The Jenkins web UI allows creating, changing, and deleting projects. It also allows starting,
stopping, deleting, and inspecting builds.
A Jenkins instance also provides an alternative Ajax-based web UI called Blue Ocean for
monitoring pipelines and managing builds. The Blue Ocean UI is considered to be more developerfriendly
and does not offer capabilities to manage Jenkins servers. The standard web UI is a
traditional web application and is required to administer a Jenkins server.
The main elements of the Jenkins web UI are:
• The initial page which is called the Jenkins Dashboard. From there you can navigate to items
(projects and folders).
• A bread crumb that shows your context withing the UI. The Jenkins link always allows you to
return to the Jenkins Dashboard.
• A side bar menu with links. These links change according to the context withing the UI.
DO380-OCP4.4-en-2-20200803 97
Chapter 4 | Implementing GitOps with Jenkins
Figure 4.4: Jenkins web UI bread crumb and site bar menu
The body of the Jenkins Dashboard page shows current folders and projects. If you click a folder,
you see its nested projects and folders. If you click a project, you see a list of builds for that
project.
If you click a build of a pipeline project, you see the Stage View of that build and each stage is
colored as green or red depending on its successful or failed status. You can click each stage to
see its logs or click Console output to see the complete logs of the entire build.
The side bar menu changes according to the context: Dashboard, folder, project, or build.
Multibranch pipeline projects are actually folders with autogenerated pipeline projects for each
branch but project settings are available only from the Multibranch pipeline folder.
98 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
References
For more information about the Jenkins master container image, refer to the
Configuring Jenkins images section in the Using images chapter in the Red Hat
OpenShift Container Platform 4.4 Images documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/images/index#images-other-jenkins
For more information about the Jenkins agent container images, refer to the
Jenkins agent section in the Using images chapter in the Red Hat OpenShift
Container Platform 4.4 Images documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/images/index#images-otherjenkins-
agent
Kubernetes plug-in for Jenkins
https://github.com/jenkinsci/kubernetes-plugin
Kubernetes Credentials plug-in for Jenkins
https://github.com/jenkinsci/kubernetes-credentials-plugin
OpenShift Sync plug-in for Jenkins
https://github.com/jenkinsci/openshift-sync-plugin
OpenShift Jenkins Pipeline (DSL) plug-in
https://github.com/jenkinsci/openshift-client-plugin
DO380-OCP4.4-en-2-20200803 99
Chapter 4 | Implementing GitOps with Jenkins
Guided Exercise
Deploying Jenkins on OpenShift
In this exercise you will deploy and validate a simple Continuous Integration/Continuous
Deployment (CI/CD) pipeline for a sample application.
Outcomes
You should be able to:
• Deploy a Jenkins master on OpenShift.
• Create a CI/CD pipeline project using a Jenkinsfile in GitHub from the Jenkins Web UI.
• Start and monitor a pipeline build from the Jenkins Web UI and the OpenShift CLI.
Before You Begin
To perform this exercise, ensure you have access to:
• A running OpenShift cluster.
• The Jenkins master and Node.js agent container images from Red Hat.
• A personal free account at GitHub.
• The source code for the "hello, world" sample application.
• The Jenkinsfile to build and deploy the sample application.
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise.
This command ensures that the cluster API is reachable and downloads sources of the
sample application.
[student@workstation ~]$ lab gitops-deploy start
Instructions
1. Create a GitHub project to host the sample application and also the Jenkinsfile.
1.1. Open a web browser and access https://github.com.
If you do not have a GitHub account, click Sign Up and follow the instructions to
create a personal account.
If you do have a GitHub account, click Sign In and follow the instructions to log in
using your personal account.
1.2. Create a new empty repository named gitops-deploy.
In the Repositories page, click New to enter the Create a new repository page.
Type gitops-deploy in the Repository name field. Leave all other fields in their
default values and click Create repository.
100 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
In the Quick setup page, click the clipboard icon to copy the HTTPS URL of your
repository.
Do not close your web browser, you will come back to GitHub a few times during this
exercise.
1.3. Open a terminal on the workstation machine, and then clone the repository into
your home folder.
[student@workstation ~]$ git clone \
> https://github.com/youraccount/gitops-deploy.git
Cloning into 'gitops-deploy'...
warning: You appear to have cloned an empty repository.
1.4. Copy the sample application sources and the Jenkinsfile from the ~/DO380/labs/
gitops-deploy folder into your local clone of the git repository.
[student@workstation ~]$ cp ~/DO380/labs/gitops-deploy/* ~/gitops-deploy
[student@workstation ~]$ cd ~/gitops-deploy
[student@workstation gitops-deploy]$ ls
app.js Jenkinsfile package.json
2. Review the Jenkinsfile for a sample CI/CD pipeline.
2.1. Inspect the Jenkinsfile and review its stages.
You do not need to understand all steps inside the Jenkinsfile. These stages work asis
and are ready to build and deploy the sample application.
The pipeline uses the OpenShift Client plug-in DSL to create a new OpenShift
project, which is named from the name of the branch and the number of the build,
then build and deploy the sample application from its source code, emulating the
steps a developer would perform using the oc new-app command.
Observe that, inside each step of each stage, there are multiple echo statements.
These statements are intended to help you relate each step with its console output
from the Jenkins Master.
[student@workstation gitops-deploy]$ less Jenkinsfile
pipeline {
environment {
...output omitted...
stages {
stage('create') {
...output omitted...
stage('build') {
...output omitted...
stage('deploy') {
...output omitted...
stage('test') {
...output omitted...
}
post {
DO380-OCP4.4-en-2-20200803 101
Chapter 4 | Implementing GitOps with Jenkins
...output omitted...
}
}
2.2. Review more closely the test stage.
It waits for interactive prompt before proceeding, to give you time to inspect
OpenShift resources in the middle of a Jenkins build, and then tests the application
using the curl command.
[student@workstation gitops-deploy]$ less Jenkinsfile
...output omitted...
stage('test') {
input {
message 'About to test the application'
ok 'Ok'
}
steps {
echo "Check that '${env.APP}.${env.DOMAIN}' returns HTTP 200"
sh "curl -s --fail ${env.APP}.${env.DOMAIN}"
}
}
...output omitted...
2.3. Commit the sources and the Jenkinsfile, and then push to GitHub.
[student@workstation gitops-deploy]$ git add *
[student@workstation gitops-deploy]$ git commit -m 'sample app and pipeline'
...output omitted...
create mode 100644 Jenkinsfile
...output omitted...
[student@workstation gitops-deploy]$ git push
...output omitted...
* [new branch] master -> master
2.4. Switch to your web browser and refresh the GitHub page that shows your repository.
Verify that you see your Jenkinsfile and sample application files there.
3. As a developer, create a project in OpenShift and deploy Jenkins using the persistent
template.
3.1. Switch to your terminal and log in on OpenShift as the developer user.
[student@workstation gitops-deploy]$ oc login -u developer -p developer \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
3.2. Create the gitops-deploy project.
[student@workstation gitops-deploy]$ oc new-project gitops-deploy
Now using project "gitops-deploy" on server "https://api.ocp4.example.com:6443".
...output omitted...
102 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
3.3. List all standard Jenkins templates provided with OpenShift.
[student@workstation gitops-deploy]$ oc get template -n openshift | grep jenkins
jenkins-ephemeral Jenkins service, without persistent storage....
jenkins-ephemeral-monitored Jenkins service, without persistent storage....
jenkins-persistent Jenkins service, with persistent storage....
jenkins-persistent-monitored Jenkins service, with persistent storage....
3.4. List the parameters of the jenkins-persistent standard template. Note that
it provides parameters that allow you to customize things such as storage size and
memory requests for larger environments. Note also that all parameters have default
values.
[student@workstation gitops-deploy]$ oc process jenkins-persistent --parameters \
> -n openshift
NAME DESCRIPTION ... VALUE
...output omitted...
MEMORY_LIMIT Maximum amount of memory the container can use. ... 1Gi
VOLUME_CAPACITY Volume space available for data, e.g. 512Mi, 2Gi. ... 1Gi
...output omitted...
3.5. Deploy Jenkins using the jenkins-persistent template. Accept the default
values for all of the template’s parameters.
[student@workstation gitops-deploy]$ oc new-app --template jenkins-persistent
...output omitted...
--> Creating resources ...
route.route.openshift.io "jenkins" created
persistentvolumeclaim "jenkins" created
deploymentconfig.apps.openshift.io "jenkins" created
serviceaccount "jenkins" created
rolebinding.authorization.openshift.io "jenkins_edit" created
service "jenkins-jnlp" created
service "jenkins" created
--> Success
...output omitted...
4. As a cluster administrator, grant to the Jenkins service account permission to create
OpenShift projects.
4.1. Log in on OpenShift as the admin user.
[student@workstation gitops-deploy]$ oc login -u admin -p redhat
Login successful.
...output omitted...
4.2. Grant the Jenkins service account permission to create new projects.
[student@workstation gitops-deploy]$ oc adm policy add-cluster-role-to-user \
> self-provisioner -z jenkins -n gitops-deploy
clusterrole.rbac.authorization.k8s.io/self-provisioner added: "jenkins"
DO380-OCP4.4-en-2-20200803 103
Chapter 4 | Implementing GitOps with Jenkins
Note
The oc adm policy command creates a cluster role binding resource, which is
not namespaced, and references a service account, which is namespaced, and thus
requires the -n option to specify the project of the service account.
5. As a developer, use the Jenkins web UI to create a Jenkins project that takes its Jenkinsfile
from a Git repository.
5.1. Log in on OpenShift as the developer user and enter the gitops-deploy project
in OpenShift.
[student@workstation gitops-deploy]$ oc login -u developer -p developer
Login successful.
...output omitted...
[student@workstation gitops-deploy]$ oc project gitops-deploy
Already on project "gitops-deploy" on server "https://api.ocp4.example.com:6443".
...output omitted...
5.2. Wait until the Jenkins master pod is ready and running. It takes time for the pod to
have its single container ready because Jenkins needs some time to initialize.
To confirm that Jenkins is fully initialized, look for the message "Jenkins is fully up and
running" in the container logs.
[student@workstation gitops-deploy]$ oc get pod
NAME READY STATUS RESTARTS AGE
jenkins-1-deploy 0/1 Completed 0 2m57s
jenkins-1-xvwwr 1/1 Running 0 2m54s
[student@workstation gitops-deploy]$ oc logs jenkins-1-xvwwr | grep \
> 'up and running'
2020-06-12 14:00:31.798+0000 [id=19] INFO hudson.WebAppMain$3#run: Jenkins is
fully up and running
5.3. Get the host name of the Jenkins master. Do not close your terminal, you will come
back to it a few times during this exercise.
[student@workstation gitops-gitops]$ oc get route
NAME HOST/PORT ...
jenkins jenkins-gitops-deploy.apps.ocp4.example.com ...
5.4. Open a new web browser tab in the workstation machine and enter the Jenkins
web UI using the URL from the previous step.
Accept the TLS certificate of the Jenkins web UI, if your browser is not trusting it yet,
and click Log in with OpenShift.
Also accept the TLS certificate of the OpenShift OAuth server, if your browser is
not trusting it yet, and click htpasswd_provider to enter the OpenShift log in
credentials page.
Log in as developer with the developer password and click Allow selected
permissions. OpenShift redirects your browser to the Jenkins Dashboard page.
5.5. Create a new multibranch pipeline project for the sample application.
104 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
Click New Item in the Jenkins Dashboard page and type hello as the item name.
Select Multibranch Pipeline and click Ok.
Under the Branch Sources heading, select Add Source ® Git and type the HTTPS
URL of your GitHub repository, that is https://github.com/youraccount/
gitops-deploy.git in the Project Repository field. Because your Git repository
is public you do not provide any credential.
Do not set any other field and click Save.
5.6. Verify that Jenkins finds the master branch.
Jenkins automatically starts a Scan Multibranch Pipeline job and displays its logs.
Wait until it finishes and verifies that the logs of the job indicate that Jenkins found a
master branch. If not, click Configure and fix your Jenkins project settings then click
Scan Multibranch Pipeline Now.
5.7. Verify that there is a build running for the master branch.
Click either Up or Jenkins to go to the Jenkins Dashboard page. Click hello to
enter the new project and click master to enter the project’s Stage View page.
If you do not see any build, click Build Now to start a build. If you see a built that is inprogress,
wait until it reaches the test stage.
Once the build reaches the test stage, it becomes paused, waiting for user
interaction. Leave the build paused at the test stage for now.
Do not close your web browser. You will come back to the Jenkins web UI a few times
during this exercise.
Note
If you see errors during the build, hover the mouse over each stage of the build and
click Logs to find clues about what caused these errors and review your previous
steps.
If you think you have to change your Jenkins project settings, click hello and click
Configure.
If you think you have to change your Git repository, commit and push your changes.
Then click Build Now to start a new build.
6. As a developer, monitor a build from the Jenkins web UI and also from the OpenShift CLI.
DO380-OCP4.4-en-2-20200803 105
Chapter 4 | Implementing GitOps with Jenkins
6.1. Click Jenkins to enter the Jenkins Dashboard page and see that, under the heading
Build executor status, there is a representation of an agent running a build.
You should be able to identify the name of the agent, the name of the project, the
name of the branch, and the number of the build.
6.2. Switch to your terminal and verify that there is a pod with the same name of the
agent you saw in the previous step.
[student@workstation gitops-gitops]$ oc get pod
NAME READY STATUS RESTARTS AGE
jenkins-1-deploy 0/1 Completed 0 18m
jenkins-1-xvwwr 1/1 Running 0 18m
nodejs-56b7s 1/1 Running 0 14m
6.3. Also verify that there is no controller resource for the Jenkins agent pod. It runs as an
unmanaged, stand-alone pod.
[student@workstation gitops-gitops]$ oc status
In project gitops-deploy on server https://api.ocp4.example.com:6443
svc/jenkins-jnlp - 172.30.249.140:50000
https://jenkins-gitops-deploy.apps.ocp4.example.com (redirects) (svc/jenkins)
dc/jenkins deploys openshift/jenkins:2
deployment #1 deployed 12 minutes ago - 1 pod
pod/nodejs-56b7s runs .../jenkins-agent-nodejs:latest
...output omitted...
6.4. Verify that there is a project named hello-branch-build and the sample
application was build and deployed there:
[student@workstation gitops-gitops]$ oc status -n hello-master-1
In project hello-master-1 on server https://api.ocp4.example.com:6443
http://hello-master-1.apps.ocp4.example.com to pod port 8080-tcp (svc/nodeapp)
dc/nodeapp deploys istag/nodeapp:latest <-
bc/nodeapp source builds https://github.com/youraccount/gitops-deploy.git on
openshift/nodejs:12
106 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
deployment #1 deployed 2 minutes ago - 1 pod
...output omitted...
6.5. Switch to your web browser on the Jenkins web UI to inspect the build logs.
From the Jenkins Dashboard page, click hello and click master to return to your
project’s Stage View page.
Click the number of the build, which should be #1 to enter the Build page and click
Console Output to view its build logs. Search for the messages from the echo steps
from your pipeline.
At the end of the page, just before a spinning disk, there is the message from the
input directive of the test stage. Do not click any of the Ok nor Abort links.
6.6. Return to the Stage View page and allow the build to continue.
Click either Back to project or master to return to the Stage View page and
hover the mouse over the test stage box until you see the popup pane of the input
directive and click Ok.
The build finishes successfully after a few moments.
6.7. Switch to your terminal and verify that both the agent pod and the hello-main-1
project are gone. The project might be available for a few moments before OpenShift
deletes it.
[student@workstation gitops-gitops]$ oc get pod
NAME READY STATUS RESTARTS AGE
jenkins-1-deploy 0/1 Completed 0 26m
jenkins-1-xvwwr 1/1 Running 0 26m
[student@workstation gitops-gitops]$ oc get project
NAME DISPLAY NAME STATUS
gitops-deploy Active
7. As a developer, delete the Jenkins project and your GitHub repository.
DO380-OCP4.4-en-2-20200803 107
Chapter 4 | Implementing GitOps with Jenkins
Do not delete the OpenShift project. You will reuse this project and the Jenkins server you
just deployed in a subsequent exercise.
7.1. As a developer, delete the Jenkins project.
Switch to your web browser and, in the Jenkins web UI, click Jenkins to return to the
Jenkins Dashboard page and click the down arrow close to hello, then click Delete
Multibranch pipeline.
Click Yes to confirm the operation. It may be necessary to scroll down the page to
find the Yes button.
7.2. Switch to your web browser and, in the GitHub repository page, click Settings and
scroll down until you find the Danger Zone header. Click Delete this repository
and follow the instructions to confirm deleting the repository.
7.3. Change to your home folder and remove your local clone of the sample application
repository
[student@workstation gitops-gitops]$ cd ~
[student@workstation ~]$ rm -rf gitops-deploy
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab gitops-deploy finish
This concludes the section.
108 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
Configuring OpenShift Resources Using a
Declarative GitOps Workflow
Objectives
After completing this section, you should be able to:
• Configure cluster operators using resource files in a declarative fashion
• Compare live OpenShift resources with resource files
• Describe GitOps concepts
Reviewing GitOps Concepts
GitOps processes follow infrastructure-as-code (IaC) practices: all configurations should come
from files that are versioned, tested, and reviewed like source code in a Continuous Integration and
Continuous Deployment (CI/CD) process.
Though early descriptions of GitOps workflows were based in GitHub proprietary pull requests,
GitOps does not require a Git server nor a GitHub account. Any kind of version control system and
branch-based workflow could fit a GitOps process.
The main challenge of implementing GitOps, compared to CI/CD, is that "configurations" do not
produce stand alone artifacts, such as binary executables, that you can store, copy, compare, and
replace. Configurations change the state of a live system.
Viable GitOps processes usually require declarative configuration files, similar to Ansible
Playbooks: imperative configurations (using the command module) usually cause maintenance
problems, whereas declarative configurations (using idempotent modules) are more reliable and
also easier to apply and maintain.
The realization of GitOps processes with OpenShift is made easier if configuration files for cluster
operators, custom resources, and also standard resources such as configuration maps and cron
jobs are stored as either YAML or JSON files in a version control system. That may be a challenge
for system administrators used to imperative OpenShift CLI commands such as oc scale.
Comparing Imperative and Declarative Resource
Management with OpenShift
The OpenShift CLI provides a rich set of imperative commands to manage application resources,
such as the oc new-app and oc set commands. Using imperative commands provide the
following benefits:
• Ease of learning and usage, lowering the barrier to entry for developers and system
administrators.
• Require less typing and provide clearer error messages
• Do not require deep knowledge of OpenShift resource types and their attributes.
Imperative oc commands work well for a CI/CD process but unfortunately not for a GitOps
process.
DO380-OCP4.4-en-2-20200803 109
Chapter 4 | Implementing GitOps with Jenkins
The OpenShift CLI also provides a small set of declarative commands to manage application
resources from either YAML or JSON files, such as the oc apply command. Using declarative
commands provide the following advantages:
• A single syntax to creating, deleting, and updating multiple resource types because all resourcespecific
details are inside resource files.
• Ease of reusing the same resource files for multiple automation tools, such as Ansible Playbooks
and GitOps tools.
• Ease of comparing multiple releases of source resource files and interpreting outputs of version
control logs compared to scripts using imperative commands.
To use declarative commands, you have to write resource definition files using YAML or JSON
syntaxes. And to to create these YAML files, you require deeper knowledge of OpenShift resource
types.
Using declarative commands also has some disadvantages because often resource files look right
but do not produce the expected results. Some call these "silent errors". Examples of silent errors:
• Kubernetes happily and silently accepts resource files that define unknown attributes, for
example because of a typing mistake in the name of the attribute.
• You can very easy, with YAML files, type an incorrect number of indentation spaces and adding
an attribute to an incorrect nested structure.
Those disadvantages encourage beginner OpenShift administrators and developers to prefer
imperative commands. Fortunately the benefits from automation surpass the disadvantages of
declarative commands.
Some OpenShift CLI command such as oc apply mitigate these disadvantages by supporting
the --validate option. That option validates resource definitions against the same resource
type metadata that provides attribute descriptions to the oc explain command.
Creating Resource Files from Scratch
To create a resource file from scratch, you have to find samples from Red Hat OpenShift Container
Platform product documentation, upstream Kubernetes documentation, or from operators that
define new custom resources types.
OpenShift is able to provide online help about all its predefined resource types, including
OpenShift API extensions and custom resources from operators, using the following commands:
• oc api-resources lists all resource types currently enabled in the cluster, including custom
resources.
• oc api-versions commands show available API versions, including APIs defined by operators
and Kubernetes extensions. There can be multiple API versions of the same resource type for
backwards compatibility with previous versions of OpenShift and Kubernetes.
• oc explain lists attributes of a resource type and their descriptions, including custom
resources defined by operators that comply to the Operator Life-cycle Manager
(OLM). Use dot notation for looking at structured resources, for example: oc explain
route.spec.host
Red Hat generally recommends that you use these commands and documentation sources to
write your resource files from scratch.
110 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
It seems to be a good idea to create and configure a resource using imperative commands, and
them export the resource to a file as an starting point. Unfortunately that approach may be harder
than you imagine because the exported file contains:
• Attributes with values that are generated at runtime such as createTimeStamp and status.
• Attributes with default values that you really do not care about and sometimes change between
Kubernetes and OpenShift releases for good reason.
• Labels and annotations added at runtime by resource controllers, admission plug-ins and
operators.
You are advised to remove all the above to get a minimum resource file that defines only the set of
attributes that you actually require because the longer the resource file, the harder to write, review,
and maintain it.
Another reason to write resource files from scratch instead of exporting live resources include the
fact that, when you export a resource, attributes are listed in alphabetical order. You may prefer a
more logical ordering of attributes, for example listing metadata.name at the beginning of your
resource file, close to kind.
Sometimes OpenShift standard templates are a nice starting point to find sample resource
definitions, but some of them could use some cleaning and reordering of attributes for easier
reading.
Finding Changes in OpenShift Resources
The fact that live OpenShift resources have a number of attributes, labels, and annotations that
only matter at runtime complicates comparing resource files on disk or under version control with
live resources in a cluster. Exporting an OpenShift resource and comparing it with a text file using
standard diff tools requires significant pre or postprocessing to remove all the noise.
The complexity of pre and postprocessing varies with each resource type, and operators add new
custom resource types that your pre and postprocessing scripts do not know about.
Fortunately OpenShift provides a safe and reliable way to compare of resource files with live
resources: the oc diff command:
• The oc diff commands sends resource definitions to the OpenShift API and lets all running
controllers make changes, but does not applies them, as if using the --dry-run option.
• Then OpenShift API returns the change set that would have been applied by oc apply if it
were not a --dry-run.
That way runtime attributes, default values, and ordering of attributes are ignored because they
would not create actual changes with oc apply.
Another benefit of using the oc diff command is that it ignores YAML file comments.
Preprocessing Resource Files with Kustomize
More sophisticated GitOps processes usually require some form of text preprocessing of resource
files, to handle requirements such as:
• Attribute values that change between development and production environments.
• Inserting text files inside configuration maps.
• Encoding binary files inside secrets.
DO380-OCP4.4-en-2-20200803 111
Chapter 4 | Implementing GitOps with Jenkins
The OpenShift CLI embeds the Kustomize utility to perform these and other preprocessing
tasks. Many OpenShift commands, such as oc apply and oc diff, accept the -k
kustomize_folder option to pre-process resource files before submitting them to the
OpenShift API.
Kustomize requires that the kustomize_folder folder contains a kustomize.yaml file, as in
the following example that generates two resources, a deployment and a TLS secret:
resources:
- deployment.yaml
secretGenerator:
- name: mycert
files:
- tls.crt=priv-cert.crt
- tls.key=priv-cert.key
type: kubernetes.io/tls
generatorOptions:
disableNameSuffixHash: true
Kustomize takes the deployment.yaml file as-is. It must exist inside the Kustomize folder.
Kustomize generates one secret named mycert in the current project.
Kustomize encodes priv-cert.crt and priv-cert.keys file contents to base 64 and
places the results in their respective keys inside the generated secret.
Kustomize copies any other attributes verbatim to the generated secret.
Kustomize generators accept a few options. This one disables adding a suffix to the names of
the generated secrets.
To test that your Kustomize configuration file contains no syntax errors, use the oc kustomize
kustomize_folder command. That command outputs the results of Kustomize pre-processing
without making any changes to files inside kustomize_folder folder nor to live resources in
your cluster.
References
GitOps: A Path to More Self-service IT at
https://queue.acm.org/detail.cfm?id=3237207/
Declarative Management of Kubernetes Objects Using Configuration Files at
https://kubernetes.io/docs/tasks/manage-kubernetes-objects/declarative-config/
Declarative Management of Kubernetes Objects Using Kustomize at
https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/
112 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
Guided Exercise
Configuring OpenShift Resources Using a
Declarative GitOps Workflow
In this exercise you will configure HTPasswd authentication from resource files in a
declarative way.
Outcomes
You should be able to:
• Configure an HTPasswd Identity Provider (IdP) from YAML files.
• Configure an HTPasswd secret using Kustomize files.
• Compare HTPasswd resources from an OpenShift cluster with local YAML files ignoring
status and default attributes.
Before You Begin
To perform this exercise, ensure you have access to:
• A running OpenShift cluster.
• YAML files with incomplete HTPasswd Identity Provider (IdP) resources.
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise.
This command ensures that the cluster API is reachable and downloads all sample files
required by this exercise.
[student@workstation ~]$ lab gitops-resources start
If you find yourself locked out of the cluster in the middle of this exercise, because you made
mistakes changing the authentication settings, running the lab gitops-resources
start command again restores the initial authentication settings of your classroom.
Instructions
1. Complete the Kustomize folder for configuring an HTPasswd IdP.
1.1. Enter the ~/DO380/labs/gitops-resources folder and verify that it contains
two folders.
Make all your changes to the config folder. The orig folder provides a pristine copy
of the initial classroom settings for your reference.
[student@workstation ~]$ cd ~/DO380/labs/gitops-resources
[student@workstation gitop-resources]$ ls
config orig
DO380-OCP4.4-en-2-20200803 113
Chapter 4 | Implementing GitOps with Jenkins
1.2. Inspect the resource files inside the ~/DO380/labs/gitops-resources/config
folder. The following files are ready to use and require no changes:
• oauth.yaml: configures the OpenShift Authentication Operator with an
HTPasswd IdP.
• htpasswd-secret-data: defines HTPasswd users and passwords. Besides the
initial admin and developer users, this file adds the kustom user with password
redhat123.
1.3. Create a Kustomize configuration file named kustomization.yaml inside the ~/
DO380/labs/gitops-resources/config folder.
Note
If you are unsure about the indentation spaces for each line when you make edits to
YAML files, refer to the solution files in the ~/DO380/labs/gitops-resources/
config folder.
List the oauth.yaml resource file and add a generator for an HTPasswd secret
named htpasswd-secret, as referenced by the oauth.yaml file. Add the
namespace attribute so the secret lands in the openshift-config namespace as
required by the HTPasswd IdP. Set the disableNameSuffixHash generator option
to true.
resources:
- oauth.yaml
secretGenerator:
- name: htpasswd-secret
namespace: openshift-config
files:
- htpasswd=htpasswd-secret-data
generatorOptions:
disableNameSuffixHash: true
1.4. Validate that your Kustomize configuration inside the config folder generates an
OAuth resource and also a secret resource.
[student@workstation gitops-resources]$ oc kustomize config
apiVersion: v1
...output omitted...
kind: Secret
metadata:
name: htpasswd-secret
namespace: openshift-config
type: Opaque
---
apiVersion: config.openshift.io/v1
kind: OAuth
...output omitted...
spec:
identityProviders:
- htpasswd:
114 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
fileData:
name: htpasswd-secret
...output omitted...
2. As a cluster administrator, apply your configuration files to the cluster and validate that you
can log in as the kustom user.
2.1. Log in on OpenShift as the admin user.
[student@workstation gitops-resources]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
2.2. Apply all resources from the ~/DO380/labs/gitops-resources/config folder
using Kustomize. If you get warnings similar to the ones in the following sample
outputs, you can safely ignore them.
[student@workstation gitops-resources]$ oc apply -k config
Warning: oc apply should be used on resource created by either oc create --saveconfig
or oc apply
secret/htpasswd-secret configured
Warning: oc apply should be used on resource created by either oc create --saveconfig
or oc apply
oauth.config.openshift.io/cluster configured
2.3. Wait until a new set of OAuth pods are ready and running. Also wait until all
terminating pods, from the previous deployment, are gone.
[student@workstation gitops-resources]$ oc get pod -n openshift-authentication
NAME READY STATUS RESTARTS AGE
oauth-openshift-55fb849579-9mz99 1/1 Running 0 4m54s
oauth-openshift-55fb849579-r2mhn 1/1 Running 0 4m46s
2.4. Log in on OpenShift as the kustom user to confirm that your new authentication
settings are applied.
[student@workstation gitops-resources]$ oc login -u kustom -p redhat123
Login successful.
...output omitted...
3. As a cluster administrator, delete the kustom user without changing your Kustomize folder.
3.1. Log in to OpenShift as the admin user.
[student@workstation gitops-resources]$ oc login -u admin -p redhat
Login successful.
...output omitted...
3.2. Extract the HTPasswd secret data to a temporary file.
DO380-OCP4.4-en-2-20200803 115
Chapter 4 | Implementing GitOps with Jenkins
[student@workstation gitops-resources]$ oc extract secret/htpasswd-secret \
> -n openshift-config --confirm --to /tmp/
/tmp/htpasswd
3.3. Remove the kustom user from the temporary file.
[student@workstation gitops-resources]$ htpasswd -D /tmp/htpasswd kustom
Deleting password for user kustom
3.4. Replace the HTPasswd secret data with the contents of the temporary file.
[student@workstation gitops-resources]$ oc set data secret/htpasswd-secret \
> -n openshift-config --from-file htpasswd=/tmp/htpasswd
secret/htpasswd-secret data updated
3.5. Wait until a new set of OAuth pods are ready and running. Also wait until all
terminating pods, from the previous deployment, are gone.
[student@workstation gitops-resources]$ oc get pod -n openshift-authentication
NAME READY STATUS RESTARTS AGE
oauth-openshift-856784b8c8-2566b 1/1 Running 0 82s
oauth-openshift-856784b8c8-6bzsb 1/1 Running 0 90s
3.6. Try to log in on OpenShift as kustom to confirm that the HTPasswd secret was
updated.
[student@workstation gitops-resources]$ oc login -u kustom -p redhat123
Login failed (401 Unauthorized)
4. Compare HTPasswd IdP and secret resources with YAML files on disk using the diff and
the oc diff commands.
4.1. Export the OAuth configuration resource and the HTPasswd secret to temporary
files.
[student@workstation gitops-resources]$ oc extract secret/htpasswd-secret \
> -n openshift-config --confirm --to /tmp/
/tmp/htpasswd
[student@workstation gitops-resources]$ oc get oauth cluster \
> -o yaml > /tmp/oauth-config.yaml
4.2. Compare the temporary files with the files on your ~/DO380/labs/gitopsresources/
config folder using the diff command.
Verify that, besides changes to the HTPasswd data, it also reports a number of
differences even though you made no changes to the OAuth configuration.
116 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
[student@workstation gitops-resources]$ diff /tmp/htpasswd \
> config/htpasswd-secret-data
2a3
...output omitted...
[student@workstation gitops-resources]$ diff /tmp/oauth-config.yaml \
> config/oauth.yaml
5,6d4
...output omitted...
4.3. Compare the live OAuth configuration resource and the HTPasswd secret with the
files on your Kustomize folder oc diff command.
Verify only differences related to the HTPasswd secret data are displayed.
[student@workstation gitops-resources]$ oc diff -k config
...output omitted...
@@ -1,6 +1,6 @@
apiVersion: v1
data:
- htpasswd: YWRt..Lwo=
+ htpasswd: YWRt..MAo=
kind: Secret
metadata:
annotations:
exit status 1
5. Restore the default authorization settings for your classroom using the files from the ~/
DO380/labs/gitops-resources/orig folder, and then change to your home folder.
[student@workstation gitops-resources]$ oc apply -k orig
secret/htpasswd-secret configured
oauth.config.openshift.io/cluster unchanged
[student@workstation gitops-resources]$ cd ~
[student@workstation ~]$
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab gitops-resources finish
This concludes the section.
DO380-OCP4.4-en-2-20200803 117
Chapter 4 | Implementing GitOps with Jenkins
Configuring OpenShift using GitOps and
Jenkins
Objectives
After completing this section, you should be able to:
• Automate the configuration of cluster operators from resources in Git using Jenkins Pipelines.
• Describe pipeline settings using Jenkinsfile directives instead of the Jenkins web UI.
Designing Jenkins Pipelines for GitOps
The following figure illustrate a sample GitOps workflow without detailing tools and other
implementation details:
One possible design that translates that workflow into Jenkins uses two pipelines:
• An Apply pipeline that applies configuration changes from version control system (VCS) to a live
cluster.
• A Drift detection pipeline that detects when live cluster configurations changed outside of the
expected GitOps process.
These pipelines primarily use the oc apply and oc diff commands, introduced elsewhere.
The following sections present one variation of that design and walks through a sample
implementation. A real-world implementation would be more sophisticated.
First, you learn about some common pipeline settings and their usage. Then, you explore using
these settings to implement Apply and Drift Pipelines to realize a GitOps workflow.
118 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
Configuring a Pipeline from a Jenkinsfile
Traditional Jenkins usage is very GUI-oriented. To follow configuration-as-code principles and
be consistent wit GitOps, avoid setting pipeline and multibranch pipeline project types from the
Jenkins web UI.
Fortunately most settings from the web UI are available from the triggers, options, and agent
directives of a pipeline.
Triggering Pipeline Builds
The trigger directive defines when Jenkins starts a build of a pipeline. Pipelines without a
trigger directive have to be started manually through the Jenkins web UI.
The main DSL statements allowed inside a trigger directive are:
• poolSCM takes a crontab expression to check a VCS repository periodically for changes. When
it finds changes (new commits or merges) it starts a new build.
• cron starts new builds according to a crontab expression. It starts a new build periodically and
unconditionally.
• upstream starts a pipeline build after another pipeline completes a build. It is a way of
concatenating multiple pipelines into a larger workflow.
Both the poolSCM and cron directives accept H/number as an alternative to */number for fixed
time intervals. Using H takes a random number inside the internal and * starts from zero and leads
to a more even distribution of builds over time.
A triggers directive can take multiple statements, for example:
triggers {
pollSCM ('H/20 * * * *')
upstream (upstreamProjects: 'library/main', threshold:
hudson.model.Result.SUCCESS)
}
Jenkins supports another way of starting a build from an automated process: a web hook. Web
hooks are Jenkins URLs that embed a random secret for authentication and the name of a VCS
repository. When Jenkins receives a web hook, it starts builds for all pipelines that would pool that
VCS repository.
Using web hooks require that your VCS be able to connect to your Jenkins instance, which may
be prevented by firewall rules, especially when your VCS is an externally hosted service such as
GitHub.
Selecting an Agent to Run a Build
The agent directive defines the agent for a pipeline or an stage. Its node directive selects an
agent by its label.
Non-containerized Jenkins instances which use preconfigured servers as agent nodes require that
you configure your agent servers with labels so they can be selected by a node directive.
The OpenShift Sync plug-in automatically labels the Red Hat Jenkins Agent container images with
the name of their image streams: nodejs and maven. The OpenShift Sync plug-in also creates
multiple instances (pods) of each agent if there are concurrent builds.
DO380-OCP4.4-en-2-20200803 119
Chapter 4 | Implementing GitOps with Jenkins
It is recommended that all pipelines on OpenShift run as disposable agent pods instead of running
on the Jenkins master. That makes your Jenkins instance more reliable, requires less memory
from the Jenkins master, and distributes the load of multiple builds among your OpenShift cluster
nodes.
The following example configures a pipeline to run using the standard Node.js container image
from Red Hat:
agent {
node {
label 'maven'
}
}
Configuring Options of a Pipeline
The options directive supports miscellaneous settings for a pipeline or an stage, for example:
• disableConcurrentBuilds prevents multiple builds of the same pipeline from building in
parallel. For example, it avoids concurrent and conflicting updates to the same cluster operator
in a GitOps pipeline. With a CI/CD workflow, it could prevent concurrent updates to the same
test database.
• buildDiscarder removes old builds, their logs, and their artifacts from the Jenkins instance. If
you don’t discard these your master node could fill up all its configuration folder volume.
• timeout puts a time limit for the duration of a build. If a build takes longer, it is aborted and
reported as failed.
Among the arguments for a buildDiscarder, the logRotator directive supports keeping a
number of the most recent builds, as in the following example:
options {
buildDiscarder (logRotator (numToKeepStr: '30', artifactNumToKeepStr: '30'))
disableConcurrentBuilds ()
timeout (time: 1, unit: 'HOURS')
}
Designing Stages of a Pipeline
When you design and implement a pipeline it is important to consider which kind of pipeline
project to use. Do you require support for running builds from multiple branches?
• Pipeline projects ignore all branches except the one specified at project creation time which is
usually your master or main branch. You have to merge any other branch to the main branch for
the pipeline to act on your changes.
• Multibranch pipeline projects could run parallel builds for different branches, and require that
you write your pipeline to work reliably under these conditions.
If you use a branch-based workflow for approving changes, you probably require a multibranch
pipeline. One way of writing a pipeline for a Multibranch pipeline project is to either perform or skip
stages based on the branch running the build.
120 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
Skipping Stages in a Pipeline
The when directive allows a stage to be executed or skipped under a number of conditions, for
example:
• branch specifies the name of a branch that runs the stage in a multibranch pipeline.
• changeset specifies that the stage runs for changes affecting a set of files from VCS. File
changes are specified using Maven file set syntax.
• expression allows using a Groovy expression to refer to values of parameters and
environment variables.
Using multiple conditions inside the same when directive requires use of the allOf and anyOf
directives, for example:
when {
allOf {
branch 'master'
changeset '**/*.js'
}
}
Using the DSL of the OpenShift Client Plug-in or Raw
OpenShift CLI Commands
The OpenShift Client plug-in offers a Domain-Specific Language (DSL) designed for imperative
CI/CD processes. Sophisticated CI/CD pipelines that perform complex manipulations of
OpenShift resources usually benefit from using the DSL
As powerful as the OpenShift Client DSL is, it may not be the best choice for a GitOps workflow.
Especially to a system administrator with no Java or JavaScript programming knowledge that we
could relate to the Groovy syntax of a Jenkins DSL.
Among the disadvantages of using the OpenShift Client DSL to implement a GitOps workflow:
• The DSL always assumes a current project reads back all resources it creates or changes. It
forces you to break commands that manipulate resources from different namespaces into
multiple steps.
That behavior is fine for CI/CD pipelines because they usually build and deploy a single
application, that lives inside a single OpenShift project.
GitOps pipelines frequently mix namespaced and non-namespaced resources, or resources
from multiple namespaces (of multiple operators) that a single oc apply command would
happily manage.
• The DSL increases verbosity for simple declarative GitOps workflows because it is designed for
scripted pipelines and requires adding script directives to declarative pipelines.
• Even if your pipeline never changes OpenShift credentials (runs as the Jenkins service account)
and also never changes project, the DSL it requires withCluster and withProject Groovy
blocks, further increasing verbosity.
The following example shows a pipeline stage that applies OpenShift resource files OpenShift
using the DSL:
DO380-OCP4.4-en-2-20200803 121
Chapter 4 | Implementing GitOps with Jenkins
stage ('Apply resources') {
steps {
script {
openshift.withCluster () {
openshift.withProject ('openshift-config') {
openshift.apply ('-k config')
}
}
}
}
}
Follows the same example stage, but coded using the sh step to invoke the OpenShift CLI:
stage ('Apply resources') {
steps {
sh 'oc apply -k config'
}
}
Given the simplicity of most GitOps declarative pipelines, using the OpenShift Client DSL is
usually overkill. Just invoking oc commands is easier and fits a large number of scenarios.
Designing and Implementing The Apply Pipelines
It is time to join the concepts of OpenShift declarative resource management, Jenkins declarative
pipelines, and Jenkinsfile configuration directives to implement a GitOps workflow. You start with
the Apply pipeline, later you review an implementation of the Drift pipeline.
The Apply Pipeline supports multiple branches as required by a typical GitOps process because
you want to make changes to a branch and allow someone can review your changes before
applying them to a live cluster. At the same time, you want that your branch gets at least minimal
validation of your resource file to prevent incorrect resource files from being merged to your main
branch.
A sample Apply Pipeline has the following stages:
• Validate: Checks the syntax of the input configuration files to catch syntax errors early on. That
stage runs for all your branches.
• Apply: Apply the configuration changes to a cluster. That stage only runs for your main branch.
• Test: Test that the configurations are live on a cluster by validating their expected side-effects.
That stage also runs only for your main branch.
The following sections walk through the main pieces of a Jenkinsfile for the Apply Pipeline.
Configuring the Apply Pipeline
The Apply Pipeline polls the VCS periodically for changes and disallow parallel builds because
overlapping builds could fail intermittently as they apply and test changes to the same live cluster
and the same cluster operators.
Any standard agent would work, but the Node.js agent is the lightest one.
122 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
Validating Configuration Files
The Validate stage is the only one that runs for all branches.
Be warned that it is not possible to completely validate OpenShift resource files before applying
them to a live cluster. You do a best effort using the --dry-run and --validate options. To be
clear: sometimes a resource works for oc apply command with the --dry-run option and fails
without that option.
You cannot perform reliable validation of your resource files because your cluster could have
multiple resource controllers and admission plug-ins which depends on each other and with the --
dry-run options they do not see each other changes. But for most common scenarios the best
effort will be sufficient.
It is conceivable to write a pipeline that provisions a temporary OpenShift cluster on-demand in
a cloud provider, it may not be a quick nor cheap operation to perform for every configuration
change. This example assumes that you have a single live cluster.
The following is a complete implementation of a Validate stage of a GitOps Apply pipeline:
stage ('Validate resources') {
steps {
sh 'oc apply --dry-run -k config'
}
}
Applying Changes to Configuration Files
The Apply stage only applies changes to the cluster when running builds from the master branch
because these are approved changes.
The Apply stage must wait until each of the affected cluster operators act on the configuration
changes. If the Apply state does not wait, the following Test stage could fail intermittently.
The details vary for each cluster operator and a common scenario is:
• First, wait for each operator to start progressing.
• Then, wait for all pods managed by the operator to be redeployed.
Some cluster operators might also require that you make sure that there are not old pods from a
previous deployment.
The following listing shows a generic example of waiting for a cluster operator and its
redeployment:
stage ('Apply resources') {
when {
branch 'master'
}
steps {
sh 'oc apply -k config'
// May require multiple of the following:
sh 'oc wait co/operatorname --for condition=Progressing \
--timeout 15s || true'
sh 'oc rollout status deployment/operatormanagedapp \
DO380-OCP4.4-en-2-20200803 123
Chapter 4 | Implementing GitOps with Jenkins
-n operatornamespace -w --timeout 360s'
}
}
Testing Configuration Changes
The Test stage only tests side-effects of configuration changes when running builds from the
master branch, similar to the Apply stage.
The logic on the test stage could vary from a very simple and incomplete smoke test, just to
verify that your cluster is still up after your configuration changes, to a very specific check of the
intended side effects of your configuration files.
Sometimes your test steps also have side effects that may require a post directive to undo and
clean up.
The following is an skeleton of a Test stage of a GitOps Apply pipeline.
stage ('Test configuration changes') {
when {
branch 'master'
}
steps {
// oc commands to test side-effects of previous changes
...output omitted...
}
post {
always {
// oc commands to clean up side-effects of the tests
...output omitted...
}
}
}
Designing and Implementing the Drift Pipeline
The Drift pipeline is simpler than the Apply pipeline because it has no side effects to be tested and
undone. It is also simpler because it is not part of the process of reviewing and approving changes
to configuration files and does not have to be a multibranch pipeline.
A sample Drift pipeline has the following stages:
• Check: verifies if the configuration of the cluster matches the configuration files on version
control. It if does not match, there is a configuration drift.
• Remediate: if a configuration drift was found, take actions to fix the issue and restore the cluster
to the latest approved configurations.
An alternative action for the Remediate stage would be to just notify IT Operations about the
issue, for example by sending an email message or opening a support ticket.
The example implementation makes the Remediate state a post directive instead of an actual
stage because it executes only when the Check stage fails because it finds a drift.
The following sections describe the main pieces of a Jenkinsfile for the Drift pipeline.
124 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
Configuring the Drift Pipeline
Run the drift pipeline periodically because unexpected configuration changes could happen at any
time. Any of the standard agents would work, and the Node.js agent is the lightest one.
Because the Drift pipeline has no side-effects of its own, parallel builds are not an issue.
Checking for Configuration Drift
The drift pipeline saves configuration drift a report for auditing purposes. The report is just the
output of oc diff.
The Check stage fails the build if a drift is found so these builds stand out as red in the Jenkins
web UI. When there is no drift, everything is fine and thus the build succeeds.
The following is a way to implement the Check stage of a GitOps Drift pipeline:
stage ('Check resource drift') {
steps {
sh 'oc diff -k config | tee drift-report.txt'
sh '! test -s drift-report.txt'
}
}
Firing the Apply Pipeline
If we wanna that the Apply pipeline remediates cluster configurations, it does not have to duplicate
the logic required to validate, apply, and test configuration changes. That logic is already part of
the Apply pipeline.
Jenkins supports running a pipeline as a step of another pipeline using the build statement.
The post directive also saves either the drift report generated by the Check stage or crates a
no-drift report for auditing purposes in the Jenkins instance using the archiveArtifacts
statement. Both kinds of reports are just text files.
A more production-like Drift pipeline could use an HTML beautifier of diff command outputs
and the Jenkins HTML Publisher plug-in to generate better-looking drift and no-drift reports.
The following is an example of the post directive that saves these reports and fires the Apply
pipeline to restore the cluster configurations to the state in version control.
post {
failure {
archiveArtifacts artifacts: '*.txt'
build job: 'apply/master'
}
success {
sh 'rm drift-report.txt'
sh 'echo \'There is no configuration drift\' > no-drift.txt'
archiveArtifacts artifacts: '*.txt'
}
DO380-OCP4.4-en-2-20200803 125
Chapter 4 | Implementing GitOps with Jenkins
Improving the Design of Your GitOps Pipelines
It is a recommended practice that you create a shell script with a test suite that is invoked from a
Jenkinsfile instead of coding tests directly in a Jenkinsfile.
A related best practice for implementing complex logic outside of a pipeline is creating Groovy
libraries. That may not be an exciting prospect to a non-developer system administrator but it is an
attractive alternative to developers implementing CI/CD workflows.
As your Test and Apply stages become more complex, you could create multiple pairs of Apply and
Drift pipelines, for example one for the settings of each cluster operator. Each of these pipeline
pairs would take input from a different VCS repository. That approach makes your Test stages
simpler, and also allow you to control, using VCS security rules, who can change configurations for
each cluster operators.
If your OpenShift cluster allows inbound access from your VCS, you could use a web hook instead
of periodic VCS checks. That would save a few cluster compute resources wasted pooling your
VCS.
Sooner or later you will have to consider: how to roll back changes from a failed build of the
Apply pipeline? Minimally, add some notification capabilities so the IT team takes action, such as
manually reverting to the latest known good configuration in version control.
A GitOps tool native to Kubernetes, such as ArgoCD, could observe resources and perform drift
checking only when resources change instead of performing drift checking periodically. That would
save a few cluster compute resources wasted comparing live resources with version control when
there was no change to the live resources.
Consider using different Git repositories to store your pipelines and your configuration files. That
makes the whole process more reliable because it decreases the changes of human error.
References
Jenkins Pipeline Syntax at
https://www.jenkins.io/doc/book/pipeline/syntax/
126 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
Guided Exercise
Configuring OpenShift using GitOps and
Jenkins
In this exercise you will automate the configuration of HTPasswd authentication and group
membership from resource files in Git using Jenkins.
Outcomes
You should be able to:
• Create a Jenkins pipeline that applies HTPasswd Identity Provider (IdP) settings and
RBAC permissions from YAML files in a Git repository.
• Create another Jenkins pipeline that compares configuration in an OpenShift cluster with
YAML files and, if there are differences, remediates the configuration drift.
Before You Begin
To perform this exercise, ensure you have access to:
• A running OpenShift cluster.
• The Jenkins master and Node.js agent container images from Red Hat.
• A running Jenkins master in the gitops-deploy OpenShift project.
• A personal free account at GitHub.
• Resource files for configuring an HTPasswd IdP and RBAC permissions.
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise.
This command ensures that the cluster API is reachable and downloads sample resource
files. It also verifies that Jenkins is running in the gitops-deploy project and deploys
Jenkins if not.
[student@workstation ~]$ lab gitops-gitops start
Instructions
1. Create a GitHub project to host your sample configuration files.
1.1. Open a web browser and access https://github.com.
If you do not have a GitHub account, then click Sign Up and follow the instructions to
create a personal account.
If you have a GitHub account, then click Sign In and follow the instructions to log in
using your personal account.
1.2. Create a new empty repository named gitops-gitops.
DO380-OCP4.4-en-2-20200803 127
Chapter 4 | Implementing GitOps with Jenkins
On the Repositories page, click New to enter the Create a new repository page.
Type gitops-gitops in the Repository name field. Leave all other fields at their
default values and click Create repository.
On the Quick setup page, click on the clipboard icon to copy the HTTPS URL of
your repository.
1.3. Open a terminal in the workstation machine and clone the new, empty repository
into your home folder. Paste the URL obtained in the previous step into your terminal.
[student@workstation ~]$ git clone \
> https://github.com/youraccount/gitops-gitops.git
Cloning into 'gitops-gitops'...
warning: You appear to have cloned an empty repository.
1.4. Copy the contents of the ~/DO380/labs/gitops-gitops/ folder into your local
clone of the git repository, and then enter your local clone.
[student@workstation ~]$ cp -r ~/DO380/labs/gitops-gitops/* ~/gitops-gitops
[student@workstation ~]$ cd ~/gitops-gitops
[student@workstation gitops-gitops]$ ls
config Jenkinsfile-apply Jenkinsfile-drift orig wait_oauth.sh
1.5. Inspect the sample configuration files in the ~/gitops-gitops/config folder.
These files are ready for this exercise and you should not change any of them until
specifically instructed to do so.
• oauth.yaml: configures the OpenShift Authentication Operator with an
HTPasswd IdP.
• htpasswd-secret-data: defines HTPasswd users and passwords. Besides the
initial admin and developer users, it defines the testuser user for the Test
stage of the Apply Pipeline.
• project-leaders.yaml: defines the project-leaders group and its
members.
• self-provisioners.yaml: defines the groups allowed to create OpenShift
projects.
• kustomization.yaml: Kustomize configuration that generates the HTPasswd
secret.
1.6. Commit the configuration files and push to GitHub.
[student@workstation gitops-gitops]$ git add config
[student@workstation gitops-gitops]$ git commit -m 'initial auth + rbac settings'
...output omitted...
create mode 100644 config/self-provisioners.yaml
...output omitted...
[student@workstation gitops-gitops]$ git push
...output omitted...
* [new branch] master -> master
128 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
2. Verify that your Jenkins master is ready and running and assign cluster administration rights
to the Jenkins service account.
2.1. Log in to OpenShift as admin and enter the gitops-deploy project in OpenShift.
[student@workstation gitops-gitops]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
[student@workstation gitops-gitops]$ oc project gitops-deploy
Now using project "gitops-deploy" on server "https://api.ocp4.example.com:6443".
2.2. Verify that the Jenkins pod is ready and running, and also verify that it is fully
initialized. Look for the message "Jenkins is fully up and running". You might get that
message more than once.
[student@workstation gitops-gitops]$ oc get pod
NAME READY STATUS RESTARTS AGE
jenkins-1-bft7r 1/1 Running 0 2m54s
jenkins-1-deploy 0/1 Completed 0 2m57s
[student@workstation gitops-gitops]$ oc logs jenkins-1-bft7r | grep 'up and
running'
2020-06-10 10:00:46.784+0000 [id=19] INFO hudson.WebAppMain$3#run: Jenkins is
fully up and running
2.3. Remove the developer user from the gitops-deploy project. You do not want to
allow developers to create and build pipelines that configure the cluster.
Also, remove the ability of the Jenkins service account to create new projects. That
permission becomes unnecessary because in the next step.
You can safely ignore the warnings from the oc adm policy command.
[student@workstation gitops-gitops]$ oc policy remove-role-from-user admin \
> developer
clusterrole.rbac.authorization.k8s.io/admin removed: "developer"
[student@workstation gitops-gitops]$ oc adm policy remove-cluster-role-from-user \
> self-provisioner -z jenkins
Warning: Your changes may get lost whenever a master is restarted,
unless you prevent reconciliation of this rolebinding using the
following command: oc annotate clusterrolebinding.rbac selfprovisioners
'rbac.authorization.kubernetes.io/autoupdate=false' --
overwriteclusterrole.rbac.authorization.k8s.io/self-provisioner removed: "jenkins"
2.4. Assign cluster administrator privileges to the Jenkins service account.
Remember that cluster roles are not namespaced and the service account is in the
current project, so you do not require the '-n' option.
[student@workstation gitops-gitops]$ oc adm policy add-cluster-role-to-user \
> cluster-admin -z jenkins
clusterrole.rbac.authorization.k8s.io/cluster-admin added: "jenkins"
3. Complete the Jenkins file for your Apply Pipeline.
DO380-OCP4.4-en-2-20200803 129
Chapter 4 | Implementing GitOps with Jenkins
Configure your pipeline to pool its Git repository for changes every 3 minutes, retain only
the latest 5 builds, and use a Node.js agent. Also, add steps to the Validate, Apply, and Test
stages to your pipeline.
3.1. Open with a text editor the file named Jenkinsfile-apply in the ~/gitopsgitops
folder. To save you time from typing, that file provides a skeleton for the
Apply pipeline that you will complete with configuration settings and steps for each
stage as you perform the following steps.
pipeline {
triggers {
}
options {
}
agent {
}
stages {
stage ('Validate configuration resources') {
steps {
}
}
stage ('Apply resources') {
steps {
}
}
stage ('Verify test user') {
steps {
}
}
}
}
3.2. Complete the triggers and options directives.
Pool your Git repository at three-minute intervals and disable concurrent builds.
pipeline {
triggers {
pollSCM ('H/3 * * * *')
}
options {
disableConcurrentBuilds()
}
stages {
...output omitted...
3.3. Complete the agent directive.
Add a node directive that runs all your stages using the agent image with label
nodejs.
pipeline {
...output omitted...
agent {
node {
130 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
label 'nodejs'
}
}
stages {
...output omitted...
3.4. Complete the Validate stage.
Add steps that validate configuration files by performing a dry run with resource
validation of the oc apply command with Kustomize.
pipeline {
...output omitted...
stages {
stage ('Validate configuration resources') {
steps {
sh 'oc apply --dry-run --validate -k config'
}
}
...output omitted...
3.5. Complete the Apply stage.
Add steps using the oc apply command with Kustomize, and a condition that
performs the stage only for the master branch. Also, call the wait_oauth.sh
script that waits until the OAuth pods redeploy to ensure that the new authentication
settings are in effect.
pipeline {
...output omitted...
stages {
...output omitted...
stage ('Apply resources') {
when {
branch 'master'
}
steps {
sh 'oc apply -k config'
sh './wait_oauth.sh'
}
}
...output omitted...
3.6. Complete the Test stage.
Add steps that log in to OpenShift as testuser and attempt to create a project,
expecting that project creation fails. This test is intended to prove that regular users
(not members of the project-leaders group) cannot create projects.
Also, add a condition that performs the stage only for the master branch.
pipeline {
...output omitted...
stage ('Verify test user') {
when {
branch 'master'
DO380-OCP4.4-en-2-20200803 131
Chapter 4 | Implementing GitOps with Jenkins
}
steps {
sh 'oc login -u testuser -p redhat123 --insecure-skip-tls-verify \
https://kubernetes.default.svc:443'
sh 'oc new-project test-testuser || true'
}
}
}
}
3.7. Save your changes, commit, and then push to GitHub. Also, commit and push the
wait_oauth.sh script.
Before you commit and push, you can compare your new Jenkinsfile-apply file
with the contents of Jenkinsfile-apply in the ~/DO380/solutions/gitopsgitops
folder.
[student@workstation gitops-gitops]$ diff Jenkinsfile-apply \
> ~/DO380/solutions/gitops-gitops/Jenkinsfile-apply
[student@workstation gitops-gitops]$ git add Jenkinsfile-apply wait_oauth.sh
[student@workstation gitops-gitops]$ git commit -m 'Apply Pipeline definition'
...output omitted...
[student@workstation gitops-gitops]$ git push
...output omitted...
3.8. Inspect the wait_oauth.sh script that you invoke from the Apply stage. It is ready
to use and contains commands that wait until:
• The Authentication Cluster Operator recognizes the changes.
• A new set of OAuth pods is available.
• The previous set of OAuth pods has been removed.
#!/bin/bash
oc wait co/authentication --for condition=Progressing \
--timeout 90s
oc rollout status -n openshift-authentication deployment/oauth-openshift \
--timeout 90s
while [ -n "$(oc get pod -n openshift-authentication -o \
jsonpath='{.items[?(@.metadata.deletionTimestamp != "")].metadata.name}')" ]
do
sleep 3
done
4. Create and test the Apply Pipeline in Jenkins.
4.1. Get the host name of the Jenkins master. Do not close your terminal, you will come
back to it a few times during this exercise.
[student@workstation gitops-gitops]$ oc get route
NAME HOST/PORT ...
jenkins jenkins-gitops-deploy.apps.ocp4.example.com ...
132 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
4.2. Open the Jenkins web UI using a web browser and the URL from the previous step.
If necessary, accept the TLS certificate when prompted, and then click Log in with
OpenShift.
Also, accept the TLS certificate of the OpenShift OAuth server, if necessary, and then
click htpasswd_provider to enter the OpenShift log in page.
Log in as the admin user with the redhat password and click Allow selected
permissions. OpenShift redirects your browser to the Jenkins Dashboard page.
4.3. Create a new multibranch pipeline project for the Apply Pipeline.
Click New Item on the Jenkins Dashboard page and type apply as the item name.
Select Multibranch Pipeline and click Ok.
Under the Branch Sources heading, select Add Source ® Git and type the HTTPS
URL of your GitHub repository, that is https://github.com/youraccount/
gitops-gitops.git, on the Project Repository field. Because your Git
repository is public you do not provide any credentials.
Under the Build Configuration heading, type Jenkinsfile-apply on the Script
Path field.
Do not set any other field and click Save.
4.4. Verify that Jenkins finds the master branch.
Jenkins automatically starts a Scan Multibranch Pipeline job and displays its logs.
Wait until it finishes and verifies that the logs of the job indicate that Jenkins found a
master branch. If not, click Configure, fix your Jenkins project settings, and then click
Scan Multibranch Pipeline Now.
4.5. Verify that the master branch has a successful build.
Click either Up or Jenkins to go to the Jenkins Dashboard page. Click apply to
enter the Apply project and click master to list builds of the master branch of your
Git repository.
If you do not see any build, then click Build Now to start a build. If you see a build
that is in progress, then wait until it finishes with a successful status and all stages
green.
If you see errors during the build, hover the mouse over each stage of the build and
click Logs to find clues about what caused these errors. Fix your Jenkinsfile, commit,
and push your changes. Then, click Build Now to start a new build.
Do not close your web browser. You will come back to the Jenkins web UI a few times
during this exercise.
5. Add a new HTPasswd user to the HTPasswd data file on Git, and then verify that the Apply
Pipeline builds and adds the new user to the cluster.
5.1. Add the newdev user with password redhat123 user to the htpasswd-secretdata
file in the ~/gitops-gitops/config folder.
[student@worksation gitops-gitops]$ htpasswd -b config/htpasswd-secret-data \
> newdev redhat123
Adding password for user newdev
5.2. Add the newdev user as a member of the project-leaders group.
Use a text editor to open the project-leaders.yaml file in the ~/gitopsgitops/
config folder, and then add a newdev entry to the users array.
DO380-OCP4.4-en-2-20200803 133
Chapter 4 | Implementing GitOps with Jenkins
apiVersion: user.openshift.io/v1
kind: Group
metadata:
name: project-leaders
users:
- developer
- newdev
5.3. Save your changes, commit, and push to GitHub.
[student@workstation gitops-gitops]$ git add config/htpasswd-secret-data
[student@workstation gitops-gitops]$ git add config/project-leaders.yaml
[student@workstation gitops-gitops]$ git commit -m 'Add new project leader'
...output omitted...
[student@workstation gitops-gitops]$ git push
...output omitted...
5.4. Switch to the Jenkins web UI and enter the master branch of the Apply project, if you
are not already there. Wait until a new build starts and completes successfully.
Verify that close to the date and time of the build there is a box with a reference to
the latest commit made to the branch. Hover the mouse over the Apply resources
and Verify test user stages and review their logs to verify that the oc apply
command changed the HTPasswd secret, the oauth-openshift deployment
successfully rolled out, and the test-user user can log in to OpenShift but cannot
create a new project.
5.5. Switch to a terminal, log in to OpenShift as the newdev user, and verify you can
create new projects. This proves again that the pipeline applied to the cluster all
settings that you made to the configuration files in your Git repository.
[student@workstation gitops-gitops]$ oc login -u newdev -p redhat123
Login successful.
...output omitted...
[student@workstation gitops-gitops]$ oc new-project test-newdev
Now using project "test-newdev" on server "https://api.ocp4.example.com:6443".
...output omitted...
5.6. Log in to OpenShift as a cluster administrator, verify that the newdev user is a
member of the the project-leaders group, and delete the test-newdev project
before proceeding to the next step.
[student@workstation gitops-gitops]$ oc login -u admin -p redhat
Login successful.
...output omitted...
[student@workstation gitops-gitops]$ oc get group
NAME USERS
project-leaders developer, newdev
[student@workstation gitops-gitops]$ oc delete project test-newdev
Now using project "test-newdev" on server "https://api.ocp4.example.com:6443".
project.project.openshift.io "test-newdev" deleted
134 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
6. Complete a Jenkinsfile for the Drift Pipeline.
Configure the pipeline to build every 5 minutes using a Node.js agent. Also, add steps to the
Check stage and to the post actions that generate either a no-drift.txt or a driftreport.
txt file and fire the Apply pipeline depending on the success status of the Check
stage.
6.1. Use a text editor to open the file named Jenkinsfile-drift in the ~/gitopsgitops
folder and complete the triggers directive.
pipeline {
triggers {
cron ('H/5 * * * *')
}
agent {
node {
label 'nodejs'
}
}
stages {
...output omitted...
6.2. Complete the Check stage using the oc diff command with Kustomize and saving
the output to a text file. Make sure that the stage fails if the text file is not empty.
pipeline {
...output omitted...
stages {
stage ('Check resource drift') {
steps {
sh 'oc diff -k config | tee drift-report.txt'
sh '! test -s drift-report.txt'
}
}
}
...output omitted...
6.3. Complete the post directive to either save the drift report text file and fire the Apply
Pipeline or create and save a no-drift report text file.
pipeline {
...output omitted...
post {
failure {
archiveArtifacts artifacts: '*.txt'
build job: 'apply/master'
}
success {
sh 'rm drift-report.txt'
sh 'echo \'There is no configuration drift\' > no-drift.txt'
archiveArtifacts artifacts: '*.txt'
}
}
}
DO380-OCP4.4-en-2-20200803 135
Chapter 4 | Implementing GitOps with Jenkins
6.4. Save your changes, commit, and push to GitHub.
Before you commit and push, you can compare your new Jenkinsfile-drift file
with the contents of Jenkinsfile-drift in the ~/DO380/solutions/gitopsgitops
folder.
[student@workstation gitops-gitops]$ diff Jenkinsfile-drift \
> ~/DO380/solutions/gitops-gitops/Jenkinsfile-drift
[student@workstation gitops-gitops]$ git add Jenkinsfile-drift
[student@workstation gitops-gitops]$ git commit -m 'Drift Pipeline definition'
...output omitted...
[student@workstation gitops-gitops]$ git push
...output omitted...
7. Create and test a project in Jenkins for the Drift Pipeline.
7.1. Switch to the Jenkins web UI and create a new pipeline project for the Drift Pipeline.
Click Jenkins to enter the Jenkins Dashboard page. Click New Item and type
drift as the item name. Select Pipeline, and then click Ok.
Under the Pipeline heading, select Pipeline script from SCM. Select Git on the
SCM field and type the HTTPS URL of your GitHub repository, that is https://
github.com/youraccount/gitops-gitops.git, on the Repository URL field.
Then type Jenkinsfile-drift on the Script Path field.
Clear the Lightweight checkout field. If you forget to do that, Jenkins will not be
able to fetch your Jenkinsfile from GitHub.
Do not set any other field and click Save.
7.2. Verify that Jenkins can successfully build the Drift Pipeline.
Jenkins should be on the Builds page for the Drift Pipeline. Click Build Now to start a
new build and wait until it finishes successfully.
If you see errors during the build, then hover the mouse over each stage of the build,
and click Logs to find clues about what caused these errors. Fix your Jenkinsfile,
commit, and push your changes.
After you have a successful build, hover the mouse over the down arrow icon of the
build to verify it produced an artifact named no-drift.txt
8. Remove the newdev user from the project-leaders group in your cluster and verify
that the Drift Pipeline restores group membership.
8.1. Change the live group in your cluster, using the oc adm groups command, to
remove the newdev member.
[student@workstation gitops-gitops]$ oc adm groups remove-users \
> project-leaders newdev
group.user.openshift.io./project-leaders removed: "newdev"
8.2. Verify that the project-leaders group does not contain the newdev user.
The Drift Pipeline might fire and restore group membership between this and the
previous step. If that happens, then ignore the difference in outputs and proceed to
the next step.
136 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
[student@workstation gitops-gitops]$ oc get group project-leaders
NAME USERS
project-leaders developer
8.3. Wait until Jenkins starts a new build of the Drift Pipeline, and the Drift Pipeline starts
a new build of the Apply Pipeline to restore cluster settings.
Switch to the Jenkins web UI and enter the drift project if you are not there.
Wait until a new build starts and terminates with a failure. When you switch to your
web browser, Jenkins might already be finished with the build.
Verify that the down arrow icon of the build displays a drift-report.txt file.
Verify also that the Check resource drift stage failed, its logs show the same
information as the drift-report.txt file, and the newdev user was removed from
the project-leaders group.
In the logs of the Declarative: post actions stage, verify that it scheduled a build of
the apply/master project. Make a note of the build number so you can verify that
build of the Apply Pipeline.
By the time you finish verifying stages and logs, you will probably see a new build of
the Drift Pipeline that finds no drift.
8.4. Verify that a build of the Apply Pipeline was started by the Drift Pipeline.
Click Jenkins, then apply, then master, then click the number of the build you got
from the previous step.
You should see a page with the message "Started by upstream project drift" and the
build number of that project.
8.5. Switch to the terminal and verify that the newdev user is again a member of the
project-leaders group
[student@workstation gitops-gitops]$ oc get group project-leaders
NAME USERS
project-leaders developer, newdev
9. Clean up your Jenkins instance and remove it from your OpenShift cluster. Also remove
your repository from GitHub.
9.1. Switch to the Jenkins web UI and remove the Drift Pipeline first, followed by the
Apply Pipeline.
Click Jenkins, click the down arrow close to drift, click Delete Pipeline, and then
click Yes to confirm the operation.
Click Jenkins, click the down arrow close to apply, click Delete Multibranch
Pipeline and then click Yes to confirm the operation.
9.2. Switch to the GitHub repository page, and click Settings and scroll down until
you find the Danger Zone header. Click Delete this repository and follow the
instructions to confirm deleting the repository.
9.3. Switch to the terminal, enter your home folder, and revoke cluster administration
privileges from the Jenkins service account.
DO380-OCP4.4-en-2-20200803 137
Chapter 4 | Implementing GitOps with Jenkins
The role binding remains even after the project and the service account are removed,
and any developer could create new projects reusing these names to get cluster
administrator privileges.
[student@workstation gitops-gitops]$ cd ~
[student@workstation ~]$ oc adm policy remove-cluster-role-from-user \
> cluster-admin -z jenkins
clusterrole.rbac.authorization.k8s.io/cluster-admin removed: "jenkins"
9.4. Delete the gitops-deploy project.
[student@workstation ~]$ oc delete project gitops-deploy
project.project.openshift.io "gitops-deploy" deleted
9.5. Restore your cluster initial authentication settings:
[student@workstation ~]$ oc apply -k ~/DO380/labs/gitops-gitops/orig
clusterrolebinding.rbac.authorization.k8s.io/self-provisioners configured
secret/htpasswd-secret configured
oauth.config.openshift.io/cluster unchanged
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab gitops-gitops finish
This concludes the section.
138 DO380-OCP4.4-en-2-20200803
Chapter 4 | Implementing GitOps with Jenkins
Summary
In this chapter, you learned:
• Jenkins is a popular software development automation server.
• Red Hat provides a set of Jenkins container images, which includes plug-ins for interacting with
OpenShift and Kubernetes.
• OpenShift comes with a few standard templates that makes it easy to deploy Jenkins using
images from Red Hat.
• CI/CD workflows are usually developer-centric, follow an imperative style, and make heavy use
of the DSL of the OpenShift Client plug-in.
• GitOps workflows are usually system administrator-centric, follow an declarative style, and make
use of YAML files and Kustomize.
• A GitOps workflow does two things:
– Apply configuration changes from files under version control.
– Detect when configuration drifts from the state under version control, then reconcile or notify.
• Declarative GitOps workflows for Kubernetes are based on the oc apply and oc diff
commands.
• CI/CD and GitOps workflows are implemented with Jenkins as pipelines
• Jenkins pipelines are composed of stages and each stage performs a number of steps.
• A step can perform any task required by its workflow, for example: building a container image,
setting RBAC permissions, and applying cluster operator configurations.
DO380-OCP4.4-en-2-20200803 139
140 DO380-OCP4.4-en-2-20200803
Chapter 5
Configuring Enterprise
Authentication
Goal Configure OpenShift integration with enterprise
identity providers
Objectives • Configure OpenShift to authenticate using an
IdM user and credentials.
• Automate group synchronization with an LDAP
directory.
Sections • Configuring the LDAP Identity Provider (and
Guided Exercise)
• Synchronizing OpenShift Groups with LDAP
(and Guided Exercise)
DO380-OCP4.4-en-2-20200803 141
Chapter 5 | Configuring Enterprise Authentication
Configuring the LDAP identity provider
Objectives
After completing this section, you should be able to:
• Configure OpenShift to authenticate using an IdM user and credentials.
• Troubleshoot issues with IdM user login integration.
Enterprise Authentication
OpenShift clusters have two categories of users: normal users and service accounts. Normal users
represent human operators and are assumed to be managed by an independent system such
as HTPasswd, LDAP or OpenStack Keystone. Service Accounts are managed by OpenShift and
are usually associated to projects to perform specific tasks with different levels of authorization.
When the OpenShift API receives a request with no authentication or an invalid authentication,
this request is assumed to be made by the anonymous user.
Users can be associated to groups, which can also be managed by an external service.
Authentication Methods
OpenShift API requests use one of the following methods to authenticate:
OAuth Access Tokens: the API reads a token from the HTTP headers to verify that the request
is authenticated. * X.509 Client Certificates: the API checks the certificate validity to verify the
authenticity of the request.
OpenShift OAuth Server
The OpenShift OAuth server uses an identity provider to validate the identity of users and issue
OAuth tokens so users can interact with the API.
Identity Providers
OpenShift supports the following Identity Providers:
• HTPasswd
• Keystone
• LDAP
• Basic authentication
• Request header
• GitHub or GitHub Enterprise
• GitLab
• Google
• OpenID Connect
142 DO380-OCP4.4-en-2-20200803
Chapter 5 | Configuring Enterprise Authentication
Configuring the LDAP Identity Provider
To configure an LDAP server as an identity provider:
Create a secret with the IdM admin user password. This secret will be used for the LDAP identity
provider:
[student@demo ~]$ oc create secret generic ldap-secret \
> --from-literal=bindPassword=${LDAP_ADMIN_PASSWORD} \
> -n openshift-config
TLS communication needs certificate authority validation, in this case, the CA is obtained from the
IdM, which is configured with the classroom.
Create a configmap containing the IdM’s Certificate Authority’s root certificate to make OpenShift
trust the IdM’s certificates:
[student@demo ~]$ oc create configmap ca-config-map \
> --from-file=\
> ca.crt=<(curl http://idm.ocp-${GUID}.example.com/ipa/config/ca.crt) \
> -n openshift-config
Create a file to modify OpenShift’s OAuth configuration by adding the LDAP identity provider and
the other elements for its proper configuration.
apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
name: cluster
spec:
identityProviders:
- name: ldapidp
mappingMethod: claim
type: LDAP
ldap:
attributes:
id:
- dn
email:
- mail
name:
- cn
preferredUsername:
- uid
bindDN: "uid=admin,cn=users,cn=accounts,dc=ocp4,dc=example,dc=com"
bindPassword:
name: ldap-secret
ca:
name: ca-config-map
insecure: false
url: "ldaps://idm.ocp4.example.com/
cn=users,cn=accounts,dc=ocp4,dc=example,dc=com?uid"
DO380-OCP4.4-en-2-20200803 143
Chapter 5 | Configuring Enterprise Authentication
Apply the custom resource and wait until the pods in the openshift-authentication
namespace are restarted so our identity provider is active:
[student@demo ~]$ oc apply -f tmp/ldap-cr.yml
Verify that you are be able to log in with the IdM’s users. If further users are added to the IdM, then
they will be immediately available and the OpenShift OAuth pods will not be restarted unlike in the
HTPasswd use case.
[student@demo ~]$ oc login -u admin -p ${LDAP_ADMIN_PASSWORD}
[student@demo ~]$ oc whoami
To Verify if there are any issues with the IdM integration. Check the pod status and logs of the
deployment.apps/oauth-openshift deployment in the openshift-authentication
namespace.
[student@demo ~]$ oc get pods -n openshift-authentication
[student@demo ~]$ oc logs deployment.apps/oauth-openshift
When users log in using the LDAP server as identity provider, OpenShift will create new user and
identity entries.
[student@demo ~]$ oc get user
[student@demo ~]$ oc get identity
Note
To perform a clean up, remove both: user and identity. If the user is removed but not
the identity, then you won’t be able to log with that user and will get 500 errors.
LDAP Users and Role Based Access Control
Role Based Access Control (RBAC) is a technique to assign privileges to users and groups in a
project or cluster wide. OpenShift RBAC implementation uses roles as descriptors of permissions
and rolebindings as the relations of the roles with users and groups. Cluster administrators can use
RBAC to delegate tasks or to give developers permissions to perform administrative operations in
their projects.
Add the cluster-admin role to users of the IdM that need cluster administration privileges.
[student@demo ~]$ oc adm policy add-cluster-role-to-user cluster-admin admin
Troubleshooting
Authentication troubleshooting can be a complex task the main things to check are:
• Authentication Operator Logs.
• Oauth Pods status.
• LDAP server logs.
144 DO380-OCP4.4-en-2-20200803
Chapter 5 | Configuring Enterprise Authentication
The identity manager logs, status and network communication with the OpenShift Cluster should
also be checked while troubleshooting.
References
Note
For more information, refer to the Configuring an LDAP identity provider section
in the Configuring identity providers chapter, in the Red Hat OpenShift Container
Platform 4.4 Authentication documentation at https://access.redhat.com/
documentation/en-us/openshift_container_platform/4.4/html-single/
authentication/index#configuring-ldap-identity-provider
DO380-OCP4.4-en-2-20200803 145
Chapter 5 | Configuring Enterprise Authentication
Guided Exercise
Configuring the LDAP Identity Provider
In this exercise you will configure an LDAP Identity provider for OpenShift.
Outcomes
You should be able to:
• Add an LDAP Identity Provider (IdP) to the OAuth operator.
• Show logs of failed and successful log in attempts.
Before You Begin
To perform this exercise, ensure you have:
• A running OpenShift cluster.
• Administrative access to the OpenShift cluster.
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise.
This command ensures that the environment for this lab is properly setup.
[student@workstation ~]$ lab auth-ldap start
Instructions
Allow users stored in an Identity Management (IdM) system to log in to the OpenShift Cluster.
1. Create the support resources for the identity provider configuration.
1.1. Log into your OpenShift cluster as the admin user.
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
1.2. Create a Secret with the LDAP bind password, which is required for the identity
provider configuration.
[student@workstation ~]$ oc create secret generic ldap-secret \
> --from-literal=bindPassword='Redhat123@!' \
> -n openshift-config
secret/ldap-secret created
146 DO380-OCP4.4-en-2-20200803
Chapter 5 | Configuring Enterprise Authentication
1.3. Create a ConfigMap containing the IdM certificate to trust.
[student@workstation ~]$ curl -O http://idm.ocp4.example.com/ipa/config/ca.crt
...output omitted...
[student@workstation ~]$ oc create configmap -n openshift-config \
> ca-config-map --from-file=ca.crt
configmap/ca-config-map created
2. Modify the OpenShift OAuth configuration.
2.1. Change to the ~/DO380/labs/auth-ldap/ directory.
[student@workstation ~]$ cd ~/DO380/labs/auth-ldap/
2.2. Edit the ldap-cr.yml file and change the dn, mail, cn, uid, ldap-secret and
ca-config-map values as shown.
apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
name: cluster
spec:
identityProviders:
- htpasswd:
fileData:
name: htpasswd-secret
mappingMethod: claim
name: htpasswd_provider
type: HTPasswd
- name: ldapidp
mappingMethod: claim
type: LDAP
ldap:
attributes:
id:
- dn
email:
- mail
name:
- cn
preferredUsername:
- uid
bindDN: "uid=admin,cn=users,cn=accounts,dc=ocp4,dc=example,dc=com"
bindPassword:
name: ldap-secret
ca:
name: ca-config-map
insecure: false
url: "ldaps://idm.ocp4.example.com/
cn=users,cn=accounts,dc=ocp4,dc=example,dc=com?uid"
DO380-OCP4.4-en-2-20200803 147
Chapter 5 | Configuring Enterprise Authentication
Note
• The url: "ldaps://…?uid" fragment must be typed as a single line.
• The htpasswd_provider identity provider defines the admin user that you use
in exercises and labs. The ldap-cr.yml file includes the htpasswd_provider
identity provider so that the admin user is always available.
2.3. Apply the custom resource. You can safely ignore the warning.
[student@workstation auth-ldap]$ oc apply -f ldap-cr.yml
Warning: oc apply should be used on resource created by either oc create --saveconfig
or oc apply
oauth.config.openshift.io/cluster configured
2.4. Wait until the oauth-openshift pods in the openshift-authentication
namespace finish redeploying. Press Ctrl+C to exit the watch command.
[student@workstation auth-ldap]$ watch oc get pods -n openshift-authentication
NAME READY STATUS RESTARTS AGE
oauth-openshift-6d57946ff7-757k8 1/1 Running 0 45s
oauth-openshift-6d57946ff7-fxcbg 1/1 Running 0 39s
3. Change to the /home/student/ directory.
[student@workstation auth-ldap]$ cd
4. Log in as the openshift-user user with password openshift-user from LDAP to
verify the configuration.
[student@workstation ~]$ oc login -u openshift-user -p openshift-user
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab auth-ldap finish
This concludes the section.
148 DO380-OCP4.4-en-2-20200803
Chapter 5 | Configuring Enterprise Authentication
Synchronizing OpenShift Groups with
LDAP
Objectives
After completing this section, you should be able to:
• Automate group synchronization with an LDAP directory.
• Troubleshoot group synchronization issues.
LDAP Group Synchronization
Groups are used to manage user permissions in bulk.
As with normal users, groups are usually stored in the external identity management system and
synchronized with the OpenShift Cluster when a user logs in.
OpenShift can synchronize LDAP groups by querying the LDAP server and creating them
internally. This process should be run periodically to ensure groups are kept up to date. OpenShift
administrators can grant permissions based on LDAP groups, this permissions have to be assigned
using the OpenShift RBAC system and are not inherited from the IdM.
The LDAPSyncConfig resource that contains the settings that the OpenShift cluster needs for
group synchronization, this can be performed manually or using a CronJob.
Configuring LDAP Group Synchronization
There are several steps to configure the LDAP group synchronization:
Create a file containing an LDAPSyncConfig. The CronJob will use a secret and a ConfigMap
instead of embedding the password and a local path.
kind: LDAPSyncConfig
apiVersion: v1
url: ldaps://idm.ocp4.example.com
bindDN: uid=ldap_user_for_sync,cn=users,cn=accounts,dc=example,dc=com
bindPassword: ldap_user_for_sync_password
insecure: false
ca: /path/to/ca.crt
rfc2307:
groupsQuery:
baseDN: "cn=groups,cn=accounts,dc=example,dc=com"
scope: sub
derefAliases: never
pageSize: 0
filter: (objectClass=posixgroup)
groupUIDAttribute: dn
groupNameAttributes: [ cn ]
groupMembershipAttributes: [ member ]
usersQuery:
baseDN: "cn=accounts,dc=example,dc=com"
DO380-OCP4.4-en-2-20200803 149
Chapter 5 | Configuring Enterprise Authentication
scope: sub
derefAliases: never
pageSize: 0
userUIDAttribute: dn
userNameAttributes: [ cn ]
tolerateMemberNotFoundErrors: false
tolerateMemberOutOfScopeErrors: true
Execute a dry run to verify the synchronization process.
[user@demo ~]$ oc adm groups sync --sync-config tmp/ldap-sync.yml
apiVersion: v1
items:
- metadata:
annotations:
openshift.io/ldap.sync-time: "2020-04-14T09:26:29Z"
openshift.io/ldap.uid: cn=admins,cn=groups,cn=accounts,dc=example,dc=com
openshift.io/ldap.url: idm.ocp4.example.com:636
creationTimestamp: "2020-03-30T14:15:49Z"
labels:
openshift.io/ldap.host: idm.ocp4.example.com
name: admins
resourceVersion: "2752973"
selfLink: /apis/user.openshift.io/v1/groups/admins
uid: f4bd4e3a-7290-11ea-b166-0a580a80001b
users:
- Administrator
...
In case of failure the sync process will display errors:
[user@demo ~]$ oc adm groups sync --sync-config tmp/ldap-sync.yml
...
error: could not bind to the LDAP server: LDAP Result Code 49 "Invalid
Credentials":
Scheduling LDAP Group Synchronization
The following steps detail how to create a CronJob to perform a periodic LDAP group
synchronization:
• Store the LDAP bind password in an OpenShift Secret so the CronJob can access the
password in a secure way.
• Store the LDAPSyncConfig and the IdM certificate in a ConfigMap so the CronJob can use
them.
Create a new LDAPSyncConfig:
kind: LDAPSyncConfig
apiVersion: v1
url: ldaps://idm.ocp4.example.com
bindDN: uid=admin,cn=users,cn=accounts,dc=example,dc=com
bindPassword:
150 DO380-OCP4.4-en-2-20200803
Chapter 5 | Configuring Enterprise Authentication
file: /etc/secrets/bindPassword
insecure: false
ca: /etc/config/ca.crt
rfc2307:
groupsQuery:
baseDN: "cn=groups,cn=accounts,dc=example,dc=com"
scope: sub
derefAliases: never
pageSize: 0
filter: (objectClass=ipausergroup)
groupUIDAttribute: dn
groupNameAttributes: [ cn ]
groupMembershipAttributes: [ member ]
usersQuery:
baseDN: "cn=users,cn=accounts,dc=example,dc=com"
scope: sub
derefAliases: never
pageSize: 0
userUIDAttribute: dn
userNameAttributes: [ uid ]
Create a Secret:
[user@demo ~]$ oc create secret generic ldap-secret \
> --from-literal bindPassword=r3dh4t
Create a ConfigMap:
[user@demo ~]$ oc create configmap ldap-config --from-file \
> ldap-group-sync.yaml=tmp/ldap-sync-config-cronjob.yml,ca.crt=tmp/ca.crt
Create a CronJob:
apiVersion: batch/v1beta1
kind: CronJob
metadata:
name: group-sync
spec:
schedule: "*/1 * * * *"
jobTemplate:
spec:
template:
spec:
restartPolicy: Never
containers:
- name: group-sync
image: quay.io/openshift/origin-cli
command:
- /bin/sh
- -c
- oc adm groups sync --sync-config /etc/config/ldap-group-sync.yaml --
confirm
volumeMounts:
DO380-OCP4.4-en-2-20200803 151
Chapter 5 | Configuring Enterprise Authentication
- mountPath: "/etc/config"
name: "ldap-sync-volume"
- mountPath: "/etc/secrets"
name: "ldap-bind-password"
volumes:
- name: "ldap-sync-volume"
configMap:
name: ldap-config
- name: "ldap-bind-password"
secret:
secretName: ldap-secret
serviceAccountName: ldap-group-syncer-sa
serviceAccount: ldap-group-syncer-sa
Schedule the job to run every minute to have shorter test cycles.
Provide an OCI image to run the CronJob. This image must contain the oc command.
The command to execute in the CronJob is the sync command with --confirm.
Provide the Secret and ConfigMap to the CronJob.
Provide a ServiceAccount, which can execute get, list, create, update verbs on the
groups resource.
Verify LDAP Group Synchronization
Use the watch command to inspect the CronJob execution:
[user@demo ~]$ watch oc get cronjobs,jobs,pods
NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE
cronjob.batch/group-sync */1 * * * * False 1 41s 83s
NAME COMPLETIONS DURATION AGE
job.batch/group-sync-1586857680 0/1 38s 38s
NAME READY STATUS RESTARTS AGE
pod/group-sync-1586857680-6969r 0/1 Error 0 38s
pod/group-sync-1586857680-dz4tp 0/1 Error 0 21s
pod/group-sync-1586857680-n7c6l 0/1 ContainerCreating 0 1s
Check the pod logs:
[user@demo ~]$ oc logs pod/group-sync-1586857680-6969r
group/admins
...
Review LDAP Users Permissions and Group Membership
Verify a group definition and membership:
[user@demo ~]$ oc get group admins -o yaml
apiVersion: user.openshift.io/v1
kind: Group
metadata:
annotations:
openshift.io/ldap.sync-time: "2020-04-14T09:50:45Z"
152 DO380-OCP4.4-en-2-20200803
Chapter 5 | Configuring Enterprise Authentication
openshift.io/ldap.uid: cn=admins,cn=groups,cn=accounts,dc=ocpexample,
dc=do380,dc=dev,dc=nextcle,dc=com
openshift.io/ldap.url: idm.ocp4.example.com:636
creationTimestamp: "2020-03-30T14:15:49Z"
labels:
openshift.io/ldap.host: idm.ocp4.example.com
name: admins
resourceVersion: "2802446"
selfLink: /apis/user.openshift.io/v1/groups/admins
uid: f4bd4e3a-7290-11ea-b166-0a580a80001b
users:
- admin
Granting Permissions to IdM User Groups
OpenShift does not inherit permissions from the LDAP server, they have to be added to each
group.
Roles can be added to LDAP groups in the same way as regular OpenShift groups after the group
synchronization is performed.
[user@demo ~]$ oc adm policy add-cluster-role-to-group cluster-admin admins
References
• For more information, refer to the Syncing LDAP groups chapter in the Red Hat
OpenShift Container Platform 4.4 Authentication documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/authentication/index#ldapsyncing
• For information about Active Directory nested groups, refer to the LDAP nested
membership sync example section in the Syncing LDAP groups chapter, which is in
the Red Hat OpenShift Container Platform 4.4 Authentication documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/authentication/index#ldapsyncing-
augmented-activedir_ldap-syncing-groups
DO380-OCP4.4-en-2-20200803 153
Chapter 5 | Configuring Enterprise Authentication
Guided Exercise
Synchronizing OpenShift Groups with
LDAP
In this exercise you will configure OpenShift for LDAP group synchronization.
Outcomes
You should be able to:
• Run group synchronization one-off.
• Create a CronJob to do the synchronization.
• Use impersonation (--as and --as-group) to verify that users only have the
permissions they are supposed to, according to the scenario.
• Use oc policy who-can to list users and groups who can perform specific actions on
specific resources according to a scenario.
Before You Begin
The classroom lab environment is in active development.
For this exercise, administrative access to an OpenShift cluster such as the one provided with
DO280 is required.
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise.
This command ensures that the environment for this lab is properly setup.
[student@workstation ~]$ lab auth-ldapsync start
Instructions
1. Perform a dry run of the group synchronization process.
1.1. Log into your OpenShift cluster as the admin user.
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
1.2. Change to the ~/DO380/labs/auth-ldapsync/ directory.
[student@workstation ~]$ cd ~/DO380/labs/auth-ldapsync/
154 DO380-OCP4.4-en-2-20200803
Chapter 5 | Configuring Enterprise Authentication
1.3. Fetch the IdM certificate to trust in the synchronization process.
[student@workstation auth-ldapsync]$ wget \
> http://idm.ocp4.example.com/ipa/config/ca.crt
1.4. Edit the ldap-sync.yml file and change the bindPassword and ca values as
shown.
kind: LDAPSyncConfig
apiVersion: v1
url: ldaps://idm.ocp4.example.com
bindDN: uid=admin,cn=users,cn=accounts,dc=ocp4,dc=example,dc=com
bindPassword: Redhat123@!
insecure: false
ca: ca.crt
rfc2307:
groupsQuery:
baseDN: "cn=groups,cn=accounts,dc=ocp4,dc=example,dc=com"
scope: sub
derefAliases: never
pageSize: 0
filter: (objectClass=ipausergroup)
groupUIDAttribute: dn
groupNameAttributes: [ cn ]
groupMembershipAttributes: [ member ]
usersQuery:
baseDN: "cn=users,cn=accounts,dc=ocp4,dc=example,dc=com"
scope: sub
derefAliases: never
pageSize: 0
userUIDAttribute: dn
userNameAttributes: [ uid ]
1.5. Run the synchronization without performing any action.
[student@workstation auth-ldapsync]$ oc adm groups sync \
> --sync-config ldap-sync.yml
apiVersion: v1
items:
- metadata:
annotations:
openshift.io/ldap.sync-time: 2020-07-07T15:15:34-0400
openshift.io/ldap.uid:
cn=admins,cn=groups,cn=accounts,dc=ocp4,dc=example,dc=com
openshift.io/ldap.url: idm.ocp4.example.com:636
creationTimestamp: null
labels:
openshift.io/ldap.host: idm.ocp4.example.com
name: admins
users:
- admin
...output omitted...
DO380-OCP4.4-en-2-20200803 155
Chapter 5 | Configuring Enterprise Authentication
2. Create a service account with suitable minimal permissions to manage groups.
2.1. Review the file named rbac.yml, which contains an ldap-group-syncer service
account, a cluster role with write access to groups, and a cluster role binding.
2.2. Deploy the OpenShift resources.
[student@workstation auth-ldapsync]$ oc create -f rbac.yml
serviceaccount/ldap-group-syncer created
clusterrole.rbac.authorization.k8s.io/ldap-group-syncer created
clusterrolebinding.rbac.authorization.k8s.io/ldap-group-syncer created
3. Create the configuration for the CronJob
3.1. Copy ldap-sync.yml to a new file named cron-ldap-sync.yml. Edit the ldapsync.
yml file and change the bindPassword and ca values as shown.
kind: LDAPSyncConfig
apiVersion: v1
url: ldaps://idm.ocp4.example.com
bindDN: uid=admin,cn=users,cn=accounts,dc=ocp4,dc=example,dc=com
bindPassword:
file: /etc/secrets/bindPassword
insecure: false
ca: /etc/config/ca.crt
rfc2307:
...output omitted...
Get the bind password from a secret.
Get the certificate from a ConfigMap.
3.2. Create a ConfigMap containing the LDAPSyncConfig and the trusted certificate.
[student@workstation auth-ldapsync]$ oc create configmap ldap-config \
> --from-file=ldap-group-sync.yaml=cron-ldap-sync.yml,ca.crt=ca.crt
configmap/ldap-config created
3.3. Create a Secret containing the LDAP bind password.
[student@workstation auth-ldapsync]$ oc create secret generic ldap-secret \
> --from-literal=bindPassword='Redhat123@!'
secret/ldap-secret created
4. Create a CronJob to automatically run the audit script on a schedule.
4.1. Edit the cronjob.yml file and change the configMap and secretName values as
shown.
apiVersion: batch/v1beta1
kind: CronJob
metadata:
name: group-sync
spec:
156 DO380-OCP4.4-en-2-20200803
Chapter 5 | Configuring Enterprise Authentication
schedule: "*/1 * * * *"
jobTemplate:
spec:
template:
spec:
restartPolicy: Never
containers:
- name: hello
image: registry.redhat.io/openshift4/ose-cli:v4.2.19
command:
- /bin/sh
- -c
- oc adm groups sync --sync-config /etc/config/ldap-group-sync.yaml --
confirm
volumeMounts:
- mountPath: "/etc/config"
name: "ldap-sync-volume"
- mountPath: "/etc/secrets"
name: "ldap-bind-password"
volumes:
- name: "ldap-sync-volume"
configMap:
name: ldap-config
- name: "ldap-bind-password"
secret:
secretName: ldap-secret
serviceAccountName: ldap-group-syncer
serviceAccount: ldap-group-syncer
4.2. Create the CronJob.
[student@workstation auth-ldapsync]$ oc create -f cronjob.yml
cronjob.batch/audit-cron created
4.3. Watch the CronJob, Job, and Pod resources until the group synchronization
completes. Press Ctrl+C to exit the watch command.
[student@workstation auth-ldapsync]$ watch oc get cronjobs,jobs,pods
NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE
cronjob.batch/group-sync */1 * * * * False 0 35s 50s
NAME COMPLETIONS DURATION AGE
job.batch/group-sync-1595239320 1/1 16s 28s
NAME READY STATUS RESTARTS AGE
pod/group-sync-1595239320-zl7h6 0/1 Completed 0 28s
4.4. Review that the user openshift-admin is in the openshift-admins group.
[student@workstation auth-ldapsync]$ oc get group openshift-admins
NAME USERS
openshift-admins openshift-admin
DO380-OCP4.4-en-2-20200803 157
Chapter 5 | Configuring Enterprise Authentication
4.5. Change to the /home/student/ directory.
[student@workstation auth-ldapsync]$ cd
5. Make the LDAP group openshift-admins cluster administrators temporarily.
5.1. Note that openshift-admin cannot view the OAuth cluster resource.
[student@workstation ~]$ oc policy who-can get oauth
resourceaccessreviewresponse.authorization.openshift.io/<unknown>
Namespace: default
Verb: get
Resource: oauths.config.openshift.io
Users: admin
system:admin
...output omitted...
Groups: system:cluster-admins
system:cluster-readers
system:masters
[student@workstation ~]$ oc --as openshift-admin --as-group=openshift-admins \
> get oauth cluster
Error from server (Forbidden): oauths.config.openshift.io "cluster" is
forbidden: User "openshift-admin" cannot get resource "oauths" in API group
"config.openshift.io" at the cluster scope
5.2. Grant the permissions.
[student@workstation ~]$ oc adm policy add-cluster-role-to-group \
> cluster-admin openshift-admins
clusterrole.rbac.authorization.k8s.io/cluster-admin added: "openshift-admins"
5.3. Note that openshift-admin can view the OAuth cluster resource. Use --asgroup
in addition to --as to apply the group-assigned role.
[student@workstation ~]$ oc policy who-can get oauth
...output omitted...
Groups: openshift-admins
...output omitted...
[student@workstation ~]$ oc --as openshift-admin --as-group=openshift-admins \
> get oauth cluster
NAME AGE
cluster 13d
5.4. Revoke the permissions. You can safely ignore the warning.
158 DO380-OCP4.4-en-2-20200803
Chapter 5 | Configuring Enterprise Authentication
[student@workstation ~]$ oc adm policy remove-cluster-role-from-group \
> cluster-admin openshift-admins
Warning: Your changes may get lost whenever a master is restarted, unless you
prevent reconciliation of this rolebinding using the following command: oc
annotate clusterrolebinding.rbac cluster-admin 'rbac.authorization.kubernetes.io/
autoupdate=false' --overwriteWarning: Your changes may get lost whenever a
master is restarted, unless you prevent reconciliation of this rolebinding
using the following command: oc annotate clusterrolebinding.rbac cluster-admins
'rbac.authorization.kubernetes.io/autoupdate=false' --overwrite
clusterrole.rbac.authorization.k8s.io/cluster-admin removed: "openshift-admins"
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab auth-ldapsync finish
This concludes the section.
DO380-OCP4.4-en-2-20200803 159
Chapter 5 | Configuring Enterprise Authentication
Summary
In this chapter, you learned:
• Enterprise Identity Management Configuration for OpenShift.
• OpenShift Roles, Identities, groups and their relations to Enterprise Identity Management.
• Automatization for Identity Management.
160 DO380-OCP4.4-en-2-20200803
Chapter 6
Configuring Trusted TLS
Certificates
Goal Configure trusted TLS certificates for external
access to cluster services and applications.
Objectives • Change the wildcard certificate used by the
ingress controller operator.
• Configure an application to use the trusted
certificate bundle.
• Describe common issues with OpenShift
certificates.
Sections • Integrating OpenShift with an Enterprise
Certificate Authority (and Guided Exercise)
• Configuring Applications to Trust the
Enterprise Certificate Authority (and Guided
Exercise)
• Troubleshooting OpenShift Certificates (and
Guided Exercise)
Lab Configuring Trusted TLS Certificates
DO380-OCP4.4-en-2-20200803 161
Chapter 6 | Configuring Trusted TLS Certificates
Integrating OpenShift with an Enterprise
Certificate Authority
Objectives
After completing this section, you should be able to:
• Change the wildcard certificate used by the ingress controller operator
• Replace the master API certificate
• Create an edge route using a wildcard certificate
Describing the Default Wildcard Certificate
The Red Hat OpenShift Container Platform installer creates an internal certificate authority (CA)
and uses this CA to sign additional certificates. The certificate used by the ingress controller
operator is a wildcard certificate for all routes in the .apps subdomain for your cluster, such as
.apps.ocp4.example.com. Routes for the web console, Grafana, Prometheus, and OAuth use
this same wildcard certificate.
When using the wildcard certificate signed by the internal OpenShift CA, the web console prompts
users with a warning indicating that the connection is not secure.
Figure 6.1: Certificate Warning
The View Certificate link provides details about the certificate.
162 DO380-OCP4.4-en-2-20200803
Chapter 6 | Configuring Trusted TLS Certificates
Figure 6.2: Default Ingress Certificate
If OpenShift certificates are not signed by a recognized certificate authority, users attempting
to access the cluster must add one or more exceptions to enable using the certificates. Although
using these exceptions might be acceptable for internal use, this solution is frequently insufficient
for client-facing URLs. Companies can use a certificate signed by a recognized certificate
authority, such as Let’s Encrypt, or use a certificate signed by their own enterprise certificate
authority.
Note
The only clients that implicitly trust certificates signed by the internal OpenShift
certificate authority are other components within the OpenShift cluster. Therefore,
these certificates should not be replaced.
Changing the Ingress Controller Operator Certificate
The ingress operator configures the ingress controller to route traffic into the OpenShift
environment. The certificate used by the ingress controller can be updated, so that it uses a
certificate signed by a recognized certificate authority or by your own enterprise CA. Changing
the ingress controller operator to use a different certificate and its associated key only requires a
handful of steps. Before starting this process, you need:
• The new certificate and key in PEM format.
• The certificate must have a subjectAltName extension of *.apps.<OPENSHIFT-DOMAIN>,
such as *.apps.ocp4.example.com, which enables using the certificate as a wildcard
certificate for the .apps subdomain.
DO380-OCP4.4-en-2-20200803 163
Chapter 6 | Configuring Trusted TLS Certificates
Changing the certificate used by the ingress controller operator does not affect certificates
signed by the internal OpenShift certificate authority.
To begin, create a new configuration map in the openshift-config namespace. Prefix the file
path with ca-bundle.crt= to name the data key in the configuration map as ca-bundle.crt.
This configuration map can contain one or more certificates. For example, you can combine the
new wildcard certificate and the certificate authority used to sign the wildcard certificate into one
file. Add these certificates to the certificate bundle if additional certificates are needed to proxy
out of your cluster.
Although it may not seem intuitive, some OpenShift components communicate with each other
using external-facing URLs. Adding your certificates to the cluster proxy ensures that your web
console pods can trust the authentication pods and vice versa. This step is not needed if the
certificate is signed by a certificate authority that already exists in the Red Hat CoreOS (RHCOS)
trust bundle.
[user@demo ~]$ oc create configmap <CONFIGMAP-NAME> \
> --from-file ca-bundle.crt=<PATH-TO-CERTIFICATE> \
> -n openshift-config
Configure the cluster proxy to use the new configuration map. This step injects the certificate
information contained in your configuration map into other configuration maps labeled with
config.openshift.io/inject-trusted-cabundle=true.
As with the preceding step, this step is not needed if the certificate is signed by a certificate
authority that already exists in the RHCOS trust bundle. There are several ways to modify the
cluster proxy, such as using oc edit, oc patch, or modifying a configuration file under version
control and then using oc apply. Additional changes to the cluster proxy can support a custom
PKI infrastructure.
[user@demo ~]$ oc patch proxy/cluster --type=merge \
> --patch='{"spec":{"trustedCA":{"name":"<CONFIGMAP-NAME>"}}}'
Important
If you fail to perform the preceding two steps when using a certificate signed by
your enterprise CA, then you will be unable to log into the web console.
Create a new TLS secret in the openshift-ingress namespace using the new certificate and
its corresponding key. The OpenShift ingress operator uses this secret.
[user@demo ~]$ oc create secret tls <SECRET-NAME> \
> --cert <PATH-TO-CERTIFICATE> \
> --key <PATH-TO-KEY> \
> -n openshift-ingress
Modify the default ingress controller operator in the openshift-ingress-operator
namespace so that defaultCertificate uses the newly created secret.
[user@demo ~]$ oc patch ingresscontroller.operator/default --type=merge \
> --patch='{"spec":{"defaultCertificate":{"name":"<SECRET-NAME>"}}}' \
> -n openshift-ingress-operator
164 DO380-OCP4.4-en-2-20200803
Chapter 6 | Configuring Trusted TLS Certificates
If the change is successful, then new router pods in the openshift-ingress namespace deploy
and change to a status of running.
[user@demo ~]$ watch oc get pods -n openshift-ingress
Replacing the Master API Certificate
The OpenShift master API uses a different certificate than the certificate used by the ingress
controller. Changing the master API certificate allows users to log in securely using the oc
command. As with the ingress controller certificate, a certificate signed by a recognized certificate
authority or by your company enterprise CA can replace the master API certificate.
To change the master API certificate, you need:
• The master API certificate and key in PEM format.
• The certificate is issued to the URL used to access the master API, such as
api.ocp4.example.com.
• The subjectAltName extension for the certificate contains the URL used to access the
master API, such as DNS:api.ocp4.example.com.
• If the certificate is signed by your enterprise CA, then you can create a combined certificate by
concatenating the master API certificate and the CA certificate into one PEM file. Concatenate
additional certificates into the combined PEM file as necessary to establish a chain of trust.
[user@demo ~]$ cat WILDCARD.pem CA.pem > COMBINED-CERT.pem
Change the master API certificate with the following steps. Create a new TLS secret in the
openshift-config namespace using the master API certificate and key. Use the combined
PEM certificate file for the --cert option if you created one.
[user@demo ~]$ oc create secret tls <SECRET-NAME> \
> --cert <PATH-TO-CERTIFICATE> --key <PATH-TO-KEY> \
> -n openshift-config
Modify the cluster apiserver to use the new secret. This can be accomplished by using oc edit,
oc patch, or by modifying a file under version control and then using oc apply.
[user@demo ~]$ oc patch apiserver cluster --type=merge --patch='{"spec": \
> {"servingCerts": {"namedCertificates": \
> [{"names": ["<API-SERVER-URL>"], \
> "servingCertificate": {"name": "<SECRET-NAME>"}}]}}}'
If the apiserver updated successfully, then new kube-apiserver pods in the openshiftkube-
apiserver namespace are created. The kube-apiserver cluster operator transitions to
a progressing state while the pods are created.
[user@demo ~]$ oc get co/kube-apiserver
NAME VERSION AVAILABLE PROGRESSING DEGRADED SINCE
kube-apiserver 4.4.8 True True False 28d
The pods are ready when the kube-apiserver cluster operator is no longer progressing.
DO380-OCP4.4-en-2-20200803 165
Chapter 6 | Configuring Trusted TLS Certificates
[user@demo ~]$ oc get co/kube-apiserver
NAME VERSION AVAILABLE PROGRESSING DEGRADED SINCE
kube-apiserver 4.4.8 True False False 28d
Note
It can take five minutes or more before all of the kube-apiserver pods are
updated.
Verifying the New Certificate
Your system can be configured to trust your enterprise CA using the following two steps:
• Copy your enterprise CA certificate to the /etc/pki/ca-trust/source/anchors/
directory. Change the name of the certificate if it conflicts with an existing file name in that
directory.
• Run the update-ca-trust extract command.
Access the web console to verify the new certificate. Use the oc whoami command to find the
the web console URL.
[user@demo ~]$ oc whoami --show-console
https://console-openshift-console.apps.ocp4.example.com
After you enter the URL in your web browser, the web console displays with a lock icon in the URL
address bar indicating that the connection is secure. If your enterprise certificate authority signed
the certificate, then you web browser might indicate that it does not recognize the certificate
issuer.
Figure 6.3: Secured Firefox Connection
If a recognized certificate authority signed the master API certificate, or if you configure your
system to trust your enterprise CA, then you can log in securely using the oc login command. If
you had previously logged in insecurely, you can delete the ~/.kube/ directory before using the
oc login command.
166 DO380-OCP4.4-en-2-20200803
Chapter 6 | Configuring Trusted TLS Certificates
Alternatively, use the --certificate-authority option to the oc login command to
specify the location of the CA certificate used to sign the master API certificate.
Creating an edge route using the new wildcard
certificate
Both new and existing OpenShift applications can immediately take advantage of the new wildcard
certificate by creating an edge route. While an edge route only secures traffic between the router
and the end-user, it does not require making any modifications to the application container image.
You can still use reencrypt routes and passthrough routes to secure traffic between the end-user
and the application pods. Use the oc create route edge to create a new edge route.
[user@demo ~]$ oc create route edge <ROUTE-NAME> --service <SERVICE>
Important
When using a wildcard certificate signed by an enterprise CA, any device not
configured to trust the CA will require the user to make an exception in order to
allow using the certificate. This is especially true for external customers.
References
• For more information, refer to the Configuring certificates chapter in the Red Hat
OpenShift Container Platform 4.4 Authentication documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/authentication/
index#configuring-certificates
• For more information, refer to the Configuring the cluster wide proxy chapter in
the Red Hat OpenShift Container Platform 4.4 Networking documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/networking/index#enablecluster-
wide-proxy
• For more information, refer to the Adding API server certificates section in the
Configuring certificates chapter in the Red Hat OpenShift Container Platform 4.4
Authentication documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/authentication/index#api-servercertificates
DO380-OCP4.4-en-2-20200803 167
Chapter 6 | Configuring Trusted TLS Certificates
Guided Exercise
Integrating OpenShift with an Enterprise
Certificate Authority
In this exercise you will configure your OpenShift cluster so that the ingress controller and
master API use a certificate signed by the classroom certificate authority.
Outcomes
You should be able to:
• Inspect a new certificate signed by the classroom certificate authority.
• Configure the ingress controller to use the new certificate.
• Configure the master API to use the new certificate.
• Validate the new certificate by accessing the web console and by running the oc login
command.
Before You Begin
To perform this exercise, ensure you have access to:
• A running OpenShift cluster.
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise.
This command ensures that the cluster API is reachable, downloads files needed for the
exercise, and generates a new certificate signed by the classroom certificate authority.
[student@workstation ~]$ lab certificates-enterprise-ca start
Instructions
1. Inspect the new certificate to view its subject, issuer, dates, and subject alternate name.
1.1. Change to the ~/DO380/labs/certificates-enterprise-ca/ directory.
[student@workstation ~]$ cd ~/DO380/labs/certificates-enterprise-ca/
1.2. The /usr/local/lib/ansible/certs/wildcard-api.yml Ansible Playbook
created the wildcard-api.pem certificate and the wildcard-api-key.pem
key using the classroom CA file located at /etc/pki/tls/certs/classroomca.
pem to sign the certificate.
Run the openssl command to view details about the certificate. The certificate is
a wildcard for any URL ending in .apps.ocp4.example.com. It also covers the
master API URL api.ocp4.example.com.
168 DO380-OCP4.4-en-2-20200803
Chapter 6 | Configuring Trusted TLS Certificates
[student@workstation certificates-enterprise-ca]$ openssl x509 -in \
> wildcard-api.pem --noout -subject -issuer -ext 'subjectAltName'
subject=C = US, ST = NC, L = Raleigh, O = "Red Hat, Inc.", OU = Training, CN =
*.apps.ocp4.example.com
issuer=C = US, ST = NC, L = Raleigh, O = "Red Hat, Inc.", OU = Training, CN = GLS
Training Classroom Certificate Authority
X509v3 Subject Alternative Name:
DNS:*.apps.ocp4.example.com, DNS:api.ocp4.example.com
2. Create an additional certificate combining the new certificate and the classroom CA
certificate. Add comments to make it easier to identify the certificates.
[student@workstation certificates-enterprise-ca]$ cat wildcard-api.pem \
> /etc/pki/tls/certs/classroom-ca.pem > combined-cert.pem
3. Create a new TLS secret using the combined certificate.
3.1. Log into your OpenShift cluster as the admin user.
[student@workstation certificates-enterprise-ca]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
3.2. Create a new TLS secret in the openshift-config namespace using combinedcert.
pem and wildcard-api-key.pem.
[student@workstation certificates-enterprise-ca]$ oc create secret tls \
> custom-tls --cert combined-cert.pem --key wildcard-api-key.pem \
> -n openshift-config
secret/custom-tls created
4. Configure the apiserver to use the custom-tls secret.
4.1. Modify the apiserver-cluster.yaml file.
[student@workstation certificates-enterprise-ca]$ vim apiserver-cluster.yaml
4.2. Update the file to match the bold lines below. Specify the URL of the master API and
the name of the TLS secret that you just created.
apiVersion: config.openshift.io/v1
kind: APIServer
metadata:
name: cluster
spec:
servingCerts:
namedCertificates:
- names:
DO380-OCP4.4-en-2-20200803 169
Chapter 6 | Configuring Trusted TLS Certificates
- api.ocp4.example.com
servingCertificate:
name: custom-tls
4.3. Apply the changes using the oc apply command.
[student@workstation certificates-enterprise-ca]$ oc apply \
> -f apiserver-cluster.yaml
Warning: oc apply should be used on resource created by either oc create --saveconfig
or oc apply
apiserver.config.openshift.io/cluster configured
5. If the change is successful, then the kube-apiserver pods in the openshift-kubeapiserver
namespace redeploy. It can take five minutes or more before the pods are
ready. Wait until the PROGRESSING column for the kube-apiserver cluster operator has
a value of True. Press Ctrl+C to exit the watch command.
[student@workstation certificates-enterprise-ca]$ watch oc get co/kube-apiserver
Every 2.0s: oc get co/kube-apiserver workstation.lab.example.com: ...
NAME VERSION AVAILABLE PROGRESSING DEGRADED SINCE
kube-apiserver 4.4.8 True True False 28d
Note
The kube-apiserver pods go into CrashLoopBackOff and Error states before
ultimately having a Running status with all four containers running in each pod.
Each pod might restart multiple times.
6. Create a new configuration map in the openshift-config namespace using the
combined certificate. Ensure that the configuration map uses a data key of cabundle.
crt.
[student@workstation certificates-enterprise-ca]$ oc create configmap \
> combined-certs --from-file ca-bundle.crt=combined-cert.pem \
> -n openshift-config
configmap/combined-certs created
7. Modify the cluster proxy so that it adds the new configuration map to the trusted certificate
bundle.
7.1. Modify the proxy-cluster.yaml file.
[student@workstation certificates-enterprise-ca]$ vim proxy-cluster.yaml
7.2. Update the file to match the bold line below. Specify the name of the configuration
map that you just created.
170 DO380-OCP4.4-en-2-20200803
Chapter 6 | Configuring Trusted TLS Certificates
apiVersion: config.openshift.io/v1
kind: Proxy
metadata:
name: cluster
spec:
trustedCA:
name: combined-certs
7.3. Apply the changes using the oc apply command.
[student@workstation certificates-enterprise-ca]$ oc apply -f proxy-cluster.yaml
Warning: oc apply should be used on resource created by either oc create --saveconfig
or oc apply
proxy.config.openshift.io/cluster configured
8. Create a TLS secret named custom-tls-bundle in the openshift-ingress
namespace. Use custom-combined-certs as the certificate and wildcard-apikey.
pem as the key.
[student@workstation certificates-enterprise-ca]$ oc create secret tls \
> custom-tls-bundle --cert combined-cert.pem --key wildcard-api-key.pem \
> -n openshift-ingress
secret/custom-tls-bundle created
9. Modify the ingress controller operator to use the custom-tls-bundle secret. A
successful change redeploys the router pods in the openshift-ingress namespace.
9.1. Modify the ingresscontrollers.yaml file.
[student@workstation certificates-enterprise-ca]$ vim ingresscontrollers.yaml
9.2. Update the file to match the bold line below. Specify the name of the TLS secret that
you just created.
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
finalizers:
- ingresscontroller.operator.openshift.io/finalizer-ingresscontroller
name: default
namespace: openshift-ingress-operator
spec:
defaultCertificate:
name: custom-tls-bundle
replicas: 2
9.3. Apply the changes using the oc apply command.
DO380-OCP4.4-en-2-20200803 171
Chapter 6 | Configuring Trusted TLS Certificates
[student@workstation certificates-enterprise-ca]$ oc apply \
> -f ingresscontrollers.yaml
Warning: oc apply should be used on resource created by either oc create --saveconfig
or oc apply
ingresscontroller.operator.openshift.io/default configured
9.4. Change to the /home/student/ directory.
[student@workstation certificates-enterprise-ca]$ cd
10. Wait until the new router-default pods in the openshift-ingress namespace
finish redeploying and the previous router pods disappear. Press Ctrl+C to exit the watch
command.
[student@workstation ~]$ watch oc get pods -n openshift-ingress
Important
If the previous router pods have not fully terminated, then accessing the OpenShift
web console URL in the next step might still produce a certificate warning.
11. Identify the route to the OpenShift web console, and then access the URL in Firefox.
Your system trusts any certificate signed by the /etc/pki/tls/certs/classroomca.
pem CA file.
11.1. Identify the OpenShift web console URL.
[student@workstation ~]$ oc whoami --show-console
https://console-openshift-console.apps.ocp4.example.com
11.2. Use Firefox to access https://api.ocp4.example.com:6443.
Firefox displays a secure lock icon in the address bar.
12. Previously in this exercise, the kube-apiserver pods in the openshift-kubeapiserver
namespace were redeployed. Confirm that the redeployment finished, and
then test the certificate used by the OpenShift master API.
12.1. Wait until the PROGRESSING column for the kube-apiserver cluster operator has a
value of False. Press Ctrl+C to exit the watch command.
[student@workstation certificates-enterprise-ca]$ watch oc get co/kube-apiserver
Every 2.0s: oc get co/kube-apiserver workstation.lab.example.com: ...
NAME VERSION AVAILABLE PROGRESSING DEGRADED SINCE
kube-apiserver 4.4.8 True False False 28d
12.2. Log out of OpenShift.
172 DO380-OCP4.4-en-2-20200803
Chapter 6 | Configuring Trusted TLS Certificates
[student@workstation ~]$ oc logout
Logged "admin" out on "https://api.ocp4.example.com:6443"
12.3. Remove the existing /home/student/.kube/ directory.
[student@workstation ~]$ rm -rf ~/.kube/
12.4. Log into your OpenShift cluster as the admin user. Because your system trusts the
master API certificate, you are not prompted to accept an insecure connection.
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab certificates-enterprise-ca finish
This concludes the section.
DO380-OCP4.4-en-2-20200803 173
Chapter 6 | Configuring Trusted TLS Certificates
Configuring Applications to Trust the
Enterprise Certificate Authority
Objectives
After completing this section, you should be able to:
• Include an enterprise CA certificate in the trusted certificate bundle.
• Configure an application to use the trusted certificate bundle.
Including Additional Trusted Certificate Authorities
When using your own enterprise CA, OpenShift can include the enterprise CA certificate in a
trusted certificate authority bundle. This approach is useful if an application running in OpenShift
must communicate with secure URLs that have been signed by your enterprise CA. By default,
applications do not trust the enterprise CA.
If your OpenShift cluster uses certificates signed by your enterprise CA, then check to see if your
enterprise CA certificate is already included. Identify the configuration map used by the cluster
proxy.
[user@demo ~]$ oc get proxy/cluster \
> -o jsonpath='{.spec.trustedCA.name}{"\n"}'
<CONFIGMAP-NAME>
Extract and then view the contents of the identified configuration map. Your own certificates, such
as a wildcard certificate and enterprise CA certificate, are listed at the top. Comments might exist
in the configuration map providing information about your certificates.
[user@demo ~]$ oc extract configmap <CONFIGMAP-NAME> \
> -n openshift-config --confirm
ca-bundle.crt
[user@demo ~]$ less ca-bundle.crt
...output omitted...
If the configuration map does not contain the enterprise CA certificate, then modify the
configuration map to append the certificate. Combine the wildcard certificate and the enterprise
CA certificate in a new PEM file. Adding comments, such as # Wildcard Cert above the
wildcard certificate and # Enterprise CA above the enterprise CA certificate, makes it easier
to identify the certificates when viewing the configuration map at a later time.
Replace the previously identified configuration map with the new certificate.
[user@demo ~]$ oc set data configmap <CONFIGMAP-NAME> \
> --from-file ca-bundle.crt=<PATH-TO-NEW-CERTIFICATE> -n openshift-config
174 DO380-OCP4.4-en-2-20200803
Chapter 6 | Configuring Trusted TLS Certificates
Injecting the Trusted CA Bundle
The Cluster Network Operator watches for changes to the trustedCA configuration map identified
in the cluster proxy. The pod injects additional certificates into all configuration maps with the
label of config.openshift.io/inject-trusted-cabundle=true. Within the configuration
map, additional certificates appear above the existing trusted certificate authority bundle.
To use the trusted CA bundle in a pod, create an empty configuration map and then use the oc
label command to add the config.openshift.io/inject-trusted-cabundle=true
label.
[user@demo ~]$ oc create configmap <CONFIGMAP-NAME>
[user@demo ~]$ oc label configmap <CONFIGMAP-NAME> \
> config.openshift.io/inject-trusted-cabundle=true
Mounting a Trusted CA Bundle
Mount a configuration map with the label of config.openshift.io/inject-trustedcabundle=
true into a pod so that the pod trusts the certificate authorities identified in the
configuration map. If the configuration map contains your enterprise CA certificate, then your
application trusts certificates signed by your enterprise CA. There are various was to mount the
configuration map. The oc set volume command can modify part of the configuration, but
you must follow it with the oc edit command to finish the configuration. Alternatively, if the
file is under version control, edit the configuration file and apply the changes with the oc apply
command.
[user@demo ~]$ oc set volume dc/<DC-NAME> -t configmap \
> --name trusted-ca --add --read-only=true \
> --mount-path /etc/pki/ca-trust/extracted/pem \
> --configmap-name <CONFIGMAP-NAME>
The oc set volume command makes the following changes in bold.
...output omitted...
spec:
containers:
- image: quay.io/redhattraining/hello-world-nginx@sha256:...
imagePullPolicy: IfNotPresent
name: hello
ports:
- containerPort: 8080
protocol: TCP
resources: {}
terminationMessagePath: /dev/termination-log
terminationMessagePolicy: File
volumeMounts:
- mountPath: /etc/pki/ca-trust/extracted/pem
name: trusted-ca
readOnly: true
dnsPolicy: ClusterFirst
restartPolicy: Always
schedulerName: default-scheduler
DO380-OCP4.4-en-2-20200803 175
Chapter 6 | Configuring Trusted TLS Certificates
securityContext: {}
terminationGracePeriodSeconds: 30
volumes:
- configMap:
defaultMode: 420
name: <CONFIGMAP-NAME>
name: trusted-ca
...output omitted...
If you made some of the changes with the oc set volume command, then use the oc edit
command to finish modifying the deployment or deployment configuration. Add the lines in bold
so that the pod mounts the certificate bundle at /etc/pki/ca-trust/extracted/pem/tlsca-
bundle.pem.
[user@demo ~]$ oc edit dc/<DC-NAME>
...output omitted...
volumes:
- configMap:
defaultMode: 420
items:
- key: ca-bundle.crt
path: tls-ca-bundle.pem
name: <CONFIGMAP-NAME>
name: trusted-ca
...output omitted...
Verifying the Mounted Bundle
If you successfully changed the application deployment or deployment configuration, then
application pods redeploy. Access an application pod to verify that it trusts certificates signed by
your enterprise CA.
[user@demo ~]$ oc rsh hello-3-65qs7
It is expected that /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem contains the
certificate for your enterprise CA. Depending on the container image, you might be able to inspect
the certificate file using less, head, cat, grep, or a similar command. The following example
assumes that your enterprise CA signed the certificate for hello.apps.ocp4.example.com.
sh-4.4$ curl https://hello.apps.ocp4.example.com
<html>
<body>
<h1>Hello, world from nginx!</h1>
</body>
</html>
176 DO380-OCP4.4-en-2-20200803
Chapter 6 | Configuring Trusted TLS Certificates
References
For more information, refer to the Configuring a custom PKI chapter in the Red Hat
OpenShift Container Platform 4.4 Networking documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/networking/index#configuring-acustom-
pki
DO380-OCP4.4-en-2-20200803 177
Chapter 6 | Configuring Trusted TLS Certificates
Guided Exercise
Configuring Applications to Trust the
Enterprise Certificate Authority
In this exercise you will modify the deployment configuration of an application so that it
trusts certificates signed by your enterprise certificate authority.
Outcomes
You should be able to:
• Identity the configuration map used by the cluster proxy.
• Verify the certificates in the cluster proxy configuration map.
• Create a new configuration map that is injected with the trusted certificate bundle.
• Mount your configuration map inside a pod so that it trusts certificates signed by your
enterprise CA.
Before You Begin
To perform this exercise, ensure you have access to:
• A running OpenShift cluster.
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise.
This command ensures that the cluster API is reachable and ensures that your cluster uses a
certificate signed by the classroom enterprise CA.
[student@workstation ~]$ lab certificates-app-trust start
Instructions
1. Determine if the cluster proxy trusts additional certificates. The listed name is a
configuration map in the openshift-config namespace.
1.1. Log into your OpenShift cluster as the admin user.
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
1.2. Display the cluster proxy resource definition in YAML format. The name attribute
under trustedCA is the name of the configuration map in the openshift-config
namespace.
178 DO380-OCP4.4-en-2-20200803
Chapter 6 | Configuring Trusted TLS Certificates
[student@workstation ~]$ oc get proxy/cluster -o yaml
apiVersion: config.openshift.io/v1
kind: Proxy
metadata:
creationTimestamp: "2020-05-28T19:17:20Z"
generation: 2
name: cluster
resourceVersion: "39884"
selfLink: /apis/config.openshift.io/v1/proxies/cluster
uid: c8d8a144-ded9-41d2-87d2-422bbfea0b47
spec:
trustedCA:
name: combined-certs
status: {}
2. Search for Classroom in the content of the data key of the identified configuration map. It
must contain the wildcard certificate and the certificate for your enterprise CA.
[student@workstation ~]$ oc get configmap combined-certs -n openshift-config \
> -o jsonpath='{.data.*}' | grep Classroom
# Classroom Wildcard Certificate
# GLS Training Classroom Certificate Authority
Note
If the comment for the wildcard certificate displays # Classroom Wildcard
& Master API Certificate instead of # Classroom Wildcard
Certificate, then the API server also uses the wildcard certificate. Either
wildcard certificate is valid, and the same comment appears a few times in this
exercise.
3. Create the certificates-app-trust project with two applications named hello1 and
hello2. Use the cluster wildcard certificate to create edge routes for each application.
3.1. Create the certificates-app-trust project.
[student@workstation ~]$ oc new-project certificates-app-trust
Now using project "certificates-app-trust" on server
"https://api.ocp4.example.com:6443".
...output omitted...
3.2. Use the container image located at quay.io/redhattraining/hello-worldnginx:
v1.0 to create the hello1 application.
[student@workstation ~]$ oc new-app --name hello1 \
> --docker-image quay.io/redhattraining/hello-world-nginx:v1.0
...output omitted...
--> Success
...output omitted...
DO380-OCP4.4-en-2-20200803 179
Chapter 6 | Configuring Trusted TLS Certificates
3.3. Use the cluster wildcard certificate to create an edge route for the hello1
application.
[student@workstation ~]$ oc create route edge --service hello1 \
> --hostname hello1-trust.apps.ocp4.example.com
route.route.openshift.io/hello1 created
3.4. Use the container image located at quay.io/redhattraining/hello-worldnginx:
v1.0 to create the hello2 application.
[student@workstation ~]$ oc new-app --name hello2 \
> --docker-image quay.io/redhattraining/hello-world-nginx:v1.0
...output omitted...
--> Success
...output omitted...
3.5. Use the cluster wildcard certificate to create an edge route for the hello2
application.
[student@workstation ~]$ oc create route edge --service hello2 \
> --hostname hello2-trust.apps.ocp4.example.com
route.route.openshift.io/hello2 created
4. Use the curl command to access both application routes from the workstation
machine. Both routes are secure because the workstation machine trusts any certificate
signed by the classroom enterprise CA. The edge routes use the wildcard certificate for the
cluster.
4.1. Identify the routes.
[student@workstation ~]$ oc get routes
NAME HOST/PORT ... PORT TERMINATION ...
hello1 hello1-trust.apps.ocp4.example.com ... 8080-tcp edge ...
hello2 hello2-trust.apps.ocp4.example.com ... 8080-tcp edge ...
4.2. Use the curl command to access hello1-trust.apps.ocp4.example.com.
[student@workstation ~]$ curl https://hello1-trust.apps.ocp4.example.com
<html>
<body>
<h1>Hello, world from nginx!</h1>
</body>
</html>
4.3. Use the curl command to access hello2-trust.apps.ocp4.example.com.
180 DO380-OCP4.4-en-2-20200803
Chapter 6 | Configuring Trusted TLS Certificates
[student@workstation ~]$ curl https://hello2-trust.apps.ocp4.example.com
<html>
<body>
<h1>Hello, world from nginx!</h1>
</body>
</html>
5. Connect to the hello1 application pod and attempt to access the route for the hello2
application. This attempt fails because the hello1 pods do not trust certificates signed by
the enterprise CA.
5.1. Identify the running application pods for the hello1 application.
[student@workstation ~]$ oc get pods -l deploymentconfig=hello1
NAME READY STATUS RESTARTS ...
hello1-1-4b7xs 1/1 Running 0 ...
5.2. Connect to the hello1 application pod.
[student@workstation ~]$ oc rsh hello1-1-4b7xs
5.3. Attempt to access the route for the hello2 application. This attempt fails because
the hello1 pod does not trust the enterprise CA.
sh-4.4$ curl https://hello2-trust.apps.ocp4.example.com
curl: (60) SSL certificate problem: self signed certificate in certificate chain
More details here: https://curl.haxx.se/docs/sslcerts.html
curl failed to verify the legitimacy of the server and therefore could not
establish a secure connection to it. To learn more about this situation and
how to fix it, please visit the web page mentioned above.
5.4. Exit from the pod.
sh-4.4$ exit
6. Inject the trusted certificate bundle into a new ca-certs configuration map.
6.1. Create the ca-certs configuration map.
[student@workstation ~]$ oc create configmap ca-certs
configmap/ca-certs created
6.2. Label the configuration map with config.openshift.io/inject-trustedcabundle=
true.
[student@workstation ~]$ oc label configmap ca-certs \
> config.openshift.io/inject-trusted-cabundle=true
configmap/ca-certs labeled
DO380-OCP4.4-en-2-20200803 181
Chapter 6 | Configuring Trusted TLS Certificates
6.3. Display the contents of the configuration map. Although it was originally empty, the
cluster network operator injected certificates into it. There are many certificates in
the configuration map; the certificates from combined-certs were injected at the
top.
[student@workstation ~]$ oc get configmap ca-certs -o yaml | head -n6
apiVersion: v1
data:
ca-bundle.crt: |
# Classroom Wildcard Certificate
-----BEGIN CERTIFICATE-----
MIIF/DCCA+SgAwIBAgIUfQkO2Jic+6NuP1KaDXejqqUM3I4wDQYJKoZIhvcNAQEL
7. Mount the ca-certs configuration map in the hello1 application pod, which ensures the
hello1 application trusts certificates signed by the classroom enterprise CA.
7.1. Use the oc set volume command to mount the ca-certs configuration map into
the hello1 deployment configuration.
[student@workstation ~]$ oc set volume dc/hello1 -t configmap \
> --name trusted-ca --add --read-only=true \
> --mount-path /etc/pki/ca-trust/extracted/pem \
> --configmap-name ca-certs
deploymentconfig.apps.openshift.io/hello1 volume updated
7.2. Modify the hello1 deployment configuration to add additional information about
the configuration map. Add the lines in bold and ensure the correct indentation (using
spaces rather than tabs) before saving.
[student@workstation ~]$ oc edit dc/hello1
...output omitted...
volumes:
- configMap:
defaultMode: 420
items:
- key: ca-bundle.crt
path: tls-ca-bundle.pem
name: ca-certs
name: trusted-ca
...output omitted...
This modification creates the tls-ca-bundle.pem file at the volume mount
point (/etc/pki/ca-trust/extracted/pem/) using the data from the cabundle.
crt key contained in the ca-certs configuration map.
8. Connect to the new hello1 application pod and attempt to access the route for the
hello2 application.
8.1. Wait until the pods in the hello1 application finish redeploying. Press Ctrl+C to exit
the watch command.
[student@workstation ~]$ watch oc get pods -l deploymentconfig=hello1
182 DO380-OCP4.4-en-2-20200803
Chapter 6 | Configuring Trusted TLS Certificates
8.2. List the pods for the hello1 application.
[student@workstation ~]$ oc get pods -l deploymentconfig=hello1
NAME READY STATUS RESTARTS ...
hello1-3-9v2vz 1/1 Running 0 ...
8.3. List the routes for the hello2 application.
[student@workstation ~]$ oc get routes -l app=hello2
NAME HOST/PORT ...
hello2 hello2-trust.apps.ocp4.example.com ...
8.4. Connect to the new hello1 application pod.
[student@workstation ~]$ oc rsh hello1-3-9v2vz
8.5. Check for classroom certificates in /etc/pki/ca-trust/extracted/pem/tlsca-
bundle.pem.
sh-4.4$ grep Classroom /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem
# Classroom Wildcard Certificate
# GLS Training Classroom Certificate Authority
8.6. Access the route for the hello2 application. The curl command works because the
pod now trusts certificates signed by the classroom enterprise CA.
sh-4.4$ curl https://hello2-trust.apps.ocp4.example.com
<html>
<body>
<h1>Hello, world from nginx!</h1>
</body>
</html>
8.7. Exit from the pod.
sh-4.4$ exit
9. Clean up your environment. Delete the certificates-app-trust project.
[student@workstation ~]$ oc delete project certificates-app-trust
project.project.openshift.io "certificates-app-trust" deleted
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab certificates-app-trust finish
This concludes the section.
DO380-OCP4.4-en-2-20200803 183
Chapter 6 | Configuring Trusted TLS Certificates
Troubleshooting OpenShift Certificates
Objectives
After completing this section, you should be able to:
• Describe common issues with OpenShift certificates.
• Renew expired OpenShift certificates.
Describing Common Issues with OpenShift
Certificates
OpenShift needs several TLS certificates to run. The required certificates include certificates for
internal communication between nodes and services, and certificates for external communication
that depends on OpenShift routes and ingresses.
OpenShift relies on several resources to be able to use your organization certificates, including:
• Configuration maps
• Secrets
• Custom resources, such as apiserver
When you configure OpenShift to use your organization certificates for the ingress controller
operator or the API server, there are many ways to troubleshoot these certificates, including
reviewing the resource via the web console, using the command-line interface, or using tools such
as openssl.
One critical administrative task is the monitoring of custom certificate expiry dates, and the
renewal of those certificates before production is affected.
OpenShift makes it possible to update certificates without disrupting the applications. Operators
monitor resources such as configuration maps or secrets and automatically redeploy the services
responsible for serving the certificates.
Troubleshooting Certificates in OpenShift
Multiple resources are responsible for ensuring that your organization certificates are properly
used, including secrets, configuration maps, and custom resources.
If the API server certificate expires, you can still log in to the web console or use the CLI, however,
the following message will appear:
The server is using an invalid certificate: x509: certificate has expired or is
not yet valid
If you add a certificate for the API server (when it previously used the default ingress controller
certificate), or you remove a specific API server certificate, then the kube-apiserver pods in
the openshift-kube-apiserver namespace redeploy. Watch the progressing status of the
184 DO380-OCP4.4-en-2-20200803
Chapter 6 | Configuring Trusted TLS Certificates
kube-apiserver cluster operator to monitor the deployment of the kube-apiserver pods.
During the redeployment, the progressing status is True. The progressing status changes to
False after the redeployment completes.
[user@demo ~]$ watch oc get co/kube-apiserver
Important
You can experience inconsistent behavior with oc commands during the
redeployment of the kube-apiserver pods. As the pods deploy, you might
connect to a pod that has a different certificate than the one used when you initially
logged in.
Running an oc command might return the message, "Unable to connect to the
server: x509: certificate signed by unknown authority". Eventually all oc commands
might return that message.
Run the oc logout command followed by the oc login command after all of the
kube-apiserver pods use the new certificate.
Renewing the API Server Certificate
To renew the API server certificate, you must identify the name of the secret containing the
certificate used by the API server.
[user@demo ~]$ oc get apiserver/cluster -o yaml
...output omitted...
spec:
servingCerts:
namedCertificates:
- names:
- <API-URL>
servingCertificate:
name: <SECRET-NAME>
The API server URL.
The name of the secret that contains the certificate.
Extract the secret, and then use the openssl command to inspect the certificate.
[user@demo ~]$ oc extract secret <SECRET-NAME> -n openshift-config --confirm
tls.crt
tls.key
[user@demo ~]$ openssl x509 -in tls.crt -noout -dates
notBefore=Apr 15 21:12:55 2015 GMT
notAfter=Apr 15 21:13:55 2020 GMT
If the certificate has expired or will expire soon, follow your company procedures to request a new
certificate. As with creating the initial certificate for the API server, the certificate signing request
for the certificate renewal must contain the subjectAltName extension for the URL used to
access the API server, such as DNS:api.ocp4.example.com.
DO380-OCP4.4-en-2-20200803 185
Chapter 6 | Configuring Trusted TLS Certificates
After obtaining a new certificate from your organization administrator, you can renew the
certificate in place by running the following command.
[user@demo ~]$ oc set data secret <SECRET-NAME> \
> --from-file tls.crt=<PATH-TO-NEW-CERTIFICATE> \
> --from-file tls.key=<PATH-TO-KEY> \
> -n openshift-config
secret/<SECRET-NAME> data updated
Renewing the Ingress Controller Certificate
The OpenShift ingress controller manages routes for internal services, such as OAuth, the web
console, and Prometheus. The ingress controller might use the cluster proxy, which also relies on
certificates.
To renew the ingress controller certificate, you must identify the name of the secret containing the
certificate used by the ingress controller.
[user@demo ~]$ oc get ingresscontroller/default -n openshift-ingress-operator \
> -o jsonpath='{.spec.defaultCertificate.name}{"\n"}'
<SECRET-NAME>
Extract the secret, and then use the openssl command to inspect the certificate.
[user@demo ~]$ oc extract secret <SECRET-NAME> -n openshift-ingress --confirm
tls.crt
tls.key
[user@demo ~]$ openssl x509 -in tls.crt -noout -dates
notBefore=May 15 11:12:23 2018 GMT
notAfter=May 15 11:13:23 2020 GMT
If the certificate has expired or will expire soon, follow your company procedures to request a new
certificate. As with creating the initial wildcard certificate, the certificate signing request for the
certificate renewal must contain the subjectAltName extension of *.apps.<OPENSHIFTDOMAIN>,
such as *.apps.ocp4.example.com. After obtaining a new certificate, the secret can
be updated in place using the oc set data secret command.
[user@demo ~]$ oc set data secret <SECRET-NAME> \
> --from-file tls.crt=<PATH-TO-NEW-CERTIFICATE> \
> --from-file tls.key=<PATH-TO-KEY> \
> -n openshift-config
secret/<SECRET-NAME> data updated
Updating the certificate instructs the router pods to redeploy in the openshift-ingress
namespace.
After the router-default pods finish redeploying, you can use the following curl command to
confirm the renewal of the certificate. Although this example checks the certificate for the OAuth
URL, the same certificate is used for the web console, Prometheus, Grafana, and more.
186 DO380-OCP4.4-en-2-20200803
Chapter 6 | Configuring Trusted TLS Certificates
[user@demo ~]$ curl -k -v \
> https://oauth-openshift.apps.ocp4.example.com 2>&1 | grep -w date
* start date: Jul 21 18:15:22 2020 GMT
* expire date: Jul 21 18:15:22 2022 GMT
If the cluster proxy uses the same certificate, then the configuration map identified for the cluster
proxy must be updated as well. Identify the name of the configuration map used by the cluster
proxy.
[user@demo ~]$ oc get proxy/cluster -o jsonpath='{.spec.trustedCA.name}{"\n"}'
<CONFIGMAP-NAME>
The configuration map exists in the openshift-config namespace.
The configuration map can be updated in place using the oc set data configmap command.
[user@demo ~]$ oc set data configmap <CONFIGMAP-NAME> \
> --from-file ca-bundle.crt=<PATH-TO-NEW-CERTIFICATE> -n openshift-config
configmap/<CONFIGMAP-NAME> data updated
Troubleshooting Certificates Renewal
Should the certificate not update in the cluster, verify the following items.
Use the openssl command to ensure that the new certificate is valid. The notBefore date must
be in the past and the notAfter date must be in the future.
[user@demo ~]$ openssl x509 -in <PATH-TO-CERTIFICATE> -noout -dates
notBefore=Jul 21 19:07:50 2020 GMT
notAfter=Jul 19 19:07:50 2025 GMT
After updating a certificate, you can compare the certificate serial number of the secret with the
certificate file to make sure that they match. A mismatch indicates that the secret did not update
properly.
[user@demo ~]$ oc get secret <SECRET-NAME> -n openshift-config \
> -o jsonpath='{.data.*}' | base64 -di | openssl x509 -noout -serial
serial=7730293A5E0590039EC8B94B85954C4DFC8CEB60
[user@demo ~]$ openssl x509 -in <PATH-TO-CERTIFICATE> -noout -serial
serial=7730293A5E0590039EC8B94B85954C4DFC8CEB60
Troubleshooting API Server Certificates
The kube-apiserver pods do not redeploy for an in place certificate update. Run the oc get
events command in the openshift-kube-apiserver namespace to verify that the kubeapiserver
pods use the updated certificate.
DO380-OCP4.4-en-2-20200803 187
Chapter 6 | Configuring Trusted TLS Certificates
[user@demo ~]$ oc get events --sort-by='.lastTimestamp' \
> -n openshift-kube-apiserver
...output omitted...
30s ... CertificateUpdated ... pod/kube-apiserver-master01 ... Wrote updated
secret: openshift-kube-apiserver/user-serving-cert-000
30s ... CertificateUpdated ... pod/kube-apiserver-master02 ... Wrote updated
secret: openshift-kube-apiserver/user-serving-cert-000
30s ... CertificateUpdated ... pod/kube-apiserver-master03 ... Wrote updated
secret: openshift-kube-apiserver/user-serving-cert-000
Troubleshooting Ingress Controller Certificates
For the ingress controller, ensure that new router-default pods in the openshift-ingress
namespace finished redeploying. It is expected that each router pod has a status of Running and
that each pod displays 1/1 in the READY column. It is also expected that router pods associated
with a previous replica set are not displayed.
[user@demo ~]$ oc get pods -n openshift-ingress
NAME READY STATUS RESTARTS AGE
router-default-55df6c587-96dhv 1/1 Running 0 9m57s
router-default-55df6c587-bf4qp 1/1 Running 0 9m24s
References
• For more information, refer to the Configuring Certificates chapter in the Red Hat
OpenShift Container Platform 4.4 Authentication documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/authentication/index#replacingdefault-
ingress
• How do I configure a CA and sign certificates using OpenSSL in Red Hat
Enterprise Linux?
https://access.redhat.com/solutions/15497
188 DO380-OCP4.4-en-2-20200803
Chapter 6 | Configuring Trusted TLS Certificates
Guided Exercise
Troubleshooting OpenShift Certificates
In this exercise, you will renew expired certificates and apply the renewed certificates to the
default ingress controller operator and master API.
Outcomes
You should be able to:
• Identify expired OpenShift certificates.
• Configure the OpenShift cluster to use renewed certificates.
Before You Begin
To perform this exercise, ensure you have access to:
• A running OpenShift cluster.
On the workstation machine, use the lab command to prepare your system for this
exercise.
The command ensures that the cluster API is reachable and configures the cluster to use
certificates that are set to expire one minute in the future.
[student@workstation ~]$ lab certificates-troubleshoot start
Instructions
1. Review the TLS certificates for the ingress controller.
1.1. Log into your OpenShift cluster as the admin user.
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
1.2. Use the curl command to display information about the TLS certificate that secures
traffic to the OpenShift web console.
[student@workstation ~]$ curl -k -v \
> https://console-openshift-console.apps.ocp4.example.com 2>&1 \
> | grep -E 'date|expired'
* start date: Jun 9 12:49:23 2020 GMT
* expire date: Jun 9 12:50:23 2020 GMT
* SSL certificate verify result: certificate has expired (10), continuing anyway.
< date: Tue, 09 Jun 2020 12:50:25 GMT
DO380-OCP4.4-en-2-20200803 189
Chapter 6 | Configuring Trusted TLS Certificates
The certificate expired at the date and time listed on the expire date line.
Note
The router pods in the openshift-ingress namespace must finish redeploying
with the new (expired) certificate. If you do not see a certificate has
expired message, then wait for a minute and try again.
1.3. If your enterprise CA signed the wildcard certificate for your OpenShift cluster, then
the cluster-wide proxy stores certificate information in a configuration map. Run the
oc get command to retrieve the name of the configuration map used by the cluster
proxy.
[student@workstation ~]$ oc get proxy/cluster \
> -o jsonpath='{.spec.trustedCA.name}{"\n"}'
wildcard-bundle
1.4. Extract the wildcard-bundle configuration map to the /tmp/ directory. The --
confirm option overwrites the file if it already exist in the destination.
[student@workstation ~]$ oc extract configmap/wildcard-bundle \
> -n openshift-config --to /tmp/ --confirm
/tmp/ca-bundle.crt
1.5. Use the openssl command to read the certificate. The -dates option displays the
dates and times of certificate validity and the -serial option displays the certificate
serial number.
[student@workstation ~]$ openssl x509 -in /tmp/ca-bundle.crt -noout -dates -serial
notBefore=Jun 9 12:49:23 2020 GMT
notAfter=Jun 9 12:50:23 2020 GMT
serial=7730293A5E0590039EC8B94B85954C4DFC8CEB60
Note
Although your certificate displays different information, it is expected that the
notAfter date (in GMT) is in the past.
1.6. Run the oc get command to retrieve the ingress controller secret name. The secret
contains the TLS certificate and the associated private key.
[student@workstation ~]$ oc get ingresscontroller/default \
> -n openshift-ingress-operator \
> -o jsonpath='{.spec.defaultCertificate.name}{"\n"}'
wildcard-tls
1.7. Extract the wildcard-tls secret and save the output to the /tmp/ directory. The
--confirm option overwrites the files if they already exist in the destination.
190 DO380-OCP4.4-en-2-20200803
Chapter 6 | Configuring Trusted TLS Certificates
[student@workstation ~]$ oc extract secret/wildcard-tls -n openshift-ingress \
> --to /tmp/ --confirm
/tmp/tls.crt
/tmp/tls.key
1.8. Use the openssl command to read the certificate. The -dates option displays the
dates and times of certificate validity and the -serial option displays the certificate
serial number.
[student@workstation ~]$ openssl x509 -in /tmp/tls.crt -noout -dates -serial
notBefore=Jun 9 12:49:23 2020 GMT
notAfter=Jun 9 12:50:23 2020 GMT
serial=7730293A5E0590039EC8B94B85954C4DFC8CEB60
The dates and serial line match the output from the /tmp/ca-bundle.crt
certificate The notAfter line indicates that the certificate expired just after starting
this exercise.
2. Renew the ingress controller wildcard certificate. Ensure the certificate is valid for 10 years.
2.1. Change to the ~/DO380/labs/certificates-troubleshoot/ directory.
[student@workstation ~]$ cd ~/DO380/labs/certificates-troubleshoot/
2.2. Run the renew_wildcard.sh script. The script uses an Ansible Playbook to create a
new wildcard certificate with an expiration date set to 3650 days from now (about ten
years). The script stores files in the current directory.
[student@workstation certificates-troubleshoot]$ ./renew_wildcard.sh
...output omitted...
TASK [Create a combined certificate] ***************************************
ok: [localhost]
TASK [Create a hard-link to the combined certificate] **********************
ok: [localhost]
PLAY RECAP *****************************************************************
localhost : ok=18 changed=6 unreachable=0 failed=0
skipped=3 rescued=0 ignored=0
2.3. Use the openssl command to verify that the wildcard-combined.pem certificate
expires in about ten years.
[student@workstation certificates-troubleshoot]$ openssl x509 \
> -in wildcard-combined.pem -noout -dates
notBefore=Jun 9 12:55:43 2020 GMT
notAfter=Jun 7 12:55:43 2030 GMT
3. Update the OpenShift cluster proxy and ingress controller operator.
3.1. Update the existing wildcard-bundle configuration map in the openshiftconfig
namespace using the renewed wildcard-combined.pem certificate.
DO380-OCP4.4-en-2-20200803 191
Chapter 6 | Configuring Trusted TLS Certificates
[student@workstation certificates-troubleshoot]$ oc set data \
> configmap/wildcard-bundle --from-file ca-bundle.crt=wildcard-combined.pem
> -n openshift-config
configmap/wildcard-bundle data updated
3.2. You must also update the TLS secret that the ingress controller uses. The secret
resides in the openshift-ingress namespace.
Update the existing wildcard-tls secret in the openshift-ingress namespace
using the renewed wildcard-combined.pem certificate and the existing
wildcard-key.pem private key.
[student@workstation certificates-troubleshoot]$ oc set data \
> secret/wildcard-tls --from-file tls.crt=wildcard-combined.pem \
> --from-file tls.key=wildcard-key.pem -n openshift-ingress
secret/wildcard-tls data updated
4. Confirm the renewal of the ingress controller wildcard certificate. From the workstation
machine, run the following curl command. The output indicates an expiration date of
about ten years in the future.
[student@workstation certificates-troubleshoot]$ curl -k -v \
> https://console-openshift-console.apps.ocp4.example.com 2>&1 | grep -w date
* start date: Jun 9 12:55:43 2020 GMT
* expire date: Jun 7 12:55:43 2030 GMT
< date: Tue, 09 Jun 2020 13:09:32 GMT
Note
It can take a minute before the router pods use the updated certificate.
5. Review the TLS certificates of the API server.
5.1. Delete the ~/.kube/ directory and run the oc login command. The message
indicates that the API certificate has expired or is not yet valid. Type y to bypass the
certificate check.
[student@workstation certificates-troubleshoot]$ rm -rf ~/.kube/
[student@workstation certificates-troubleshoot]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
The server is using an invalid certificate: x509: certificate has expired or is
not yet valid
You can bypass the certificate check, but any data you send to the server could be
intercepted by others.
Use insecure connections? (y/n): y
Login successful.
...output omitted...
192 DO380-OCP4.4-en-2-20200803
Chapter 6 | Configuring Trusted TLS Certificates
Note
The start script redeploys the apiserver pods in the openshift-kube-apiserver
namespace using an expiring certificate. If the pods have not finished redeploying,
then the invalid certificate warning does not display. It is safe to continue
the exercise even if you do not see the warning.
5.2. The cluster resource is a custom resource of type apiserver and defines the
certificate that the API server uses. Use the oc get command to review the resource
configuration.
[student@workstation certificates-troubleshoot]$ oc get apiserver/cluster \
> -o yaml
...output omitted...
spec:
servingCerts:
namedCertificates:
- names:
- api.ocp4.example.com
servingCertificate:
name: api-tls
The secret that contains the certificates.
5.3. Extract the api-tls secret and save the output to the /tmp/ directory. The --
confirm option overwrites the files if they already exist in the destination.
[student@workstation certificates-troubleshoot]$ oc extract secret/api-tls \
> -n openshift-config --to /tmp/ --confirm
/tmp/tls.crt
/tmp/tls.key
5.4. Use the openssl command to read the certificate. The -issuer and -dates
options display which certificate authority issued the certificate and the dates and
times of certificate validity.
[student@workstation certificates-troubleshoot]$ openssl x509 -in /tmp/tls.crt \
> -noout -issuer -dates
issuer=C = US, ST = NC, L = Raleigh, O = "Red Hat, Inc.", OU = Training, CN = GLS
Training Classroom Certificate Authority
notBefore=Jun 9 12:49:32 2020 GMT
notAfter=Jun 9 12:50:32 2020 GMT
The GLS Training Classroom Certificate Authority CA issued the
certificate. Although your dates are different, it is expected that the certificate
expired because it was only valid for one minute.
6. Renew the API certificate.
6.1. Run the renew_api.sh script. The script uses an Ansible Playbook to create a new
API certificate with an expiration date set to 3650 days from now (about ten years).
The script stores files in the current directory.
DO380-OCP4.4-en-2-20200803 193
Chapter 6 | Configuring Trusted TLS Certificates
[student@workstation certificates-troubleshoot]$ ./renew_api.sh
...output omitted...
TASK [Create a combined certificate] ***************************************
ok: [localhost]
TASK [Create a hard-link to the combined certificate] **********************
ok: [localhost]
PLAY RECAP *****************************************************************
localhost : ok=18 changed=6 unreachable=0 failed=0
skipped=3 rescued=0 ignored=0
6.2. List the certificates in the ~/DO380/labs/certificates-troubleshoot/
directory.
[student@workstation certificates-troubleshoot]$ ls *.pem | grep -v key
api-combined.pem
api.pem
wildcard-combined.pem
wildcard.pem
6.3. Use the openssl command to display information about the api-combined.pem
certificate.
[student@workstation certificates-troubleshoot]$ openssl x509 \
> -in api-combined.pem -noout -dates
notBefore=Jun 9 13:29:21 2020 GMT
notAfter=Jun 7 13:29:21 2030 GMT
The output indicates an expiration date of about ten years from now.
7. Update the secret for the API certificate.
7.1. Update the existing api-tls secret in the openshift-config namespace so that
it uses the renewed api-combined.pem certificate and the existing api-key.pem
private key.
[student@workstation certificates-troubleshoot]$ oc set data secret/api-tls \
> --from-file tls.crt=api-combined.pem --from-file tls.key=api-key.pem \
> -n openshift-config
secret/api-tls data updated
7.2. If the oc set data command succeeds, then the certificate used by each apiserver
pod is updated. Run the following command to consult the latest events of the
openshift-kube-apiserver namespace.
194 DO380-OCP4.4-en-2-20200803
Chapter 6 | Configuring Trusted TLS Certificates
[student@workstation certificates-troubleshoot]$ oc get events \
> --sort-by='.lastTimestamp' -n openshift-kube-apiserver
...output omitted...
30s ... CertificateUpdated ... pod/kube-apiserver-master01 ... Wrote updated
secret: openshift-kube-apiserver/user-serving-cert-000
30s ... CertificateUpdated ... pod/kube-apiserver-master02 ... Wrote updated
secret: openshift-kube-apiserver/user-serving-cert-000
30s ... CertificateUpdated ... pod/kube-apiserver-master03 ... Wrote updated
secret: openshift-kube-apiserver/user-serving-cert-000
7.3. Change to the /home/student/ directory.
[student@workstation certificates-troubleshoot]$ cd
7.4. Run the following curl command to ensure that the certificate is renewed.
[student@workstation ~]$ curl -k -v https://api.ocp4.example.com:6443 2>&1 \
> | grep -w date
* start date: Jun 9 13:29:21 2020 GMT
* expire date: Jun 7 13:29:21 2030 GMT
< date: Tue, 09 Jun 2020 13:41:30 GMT
Note
You might need to run the the curl command more than once before you see
the updated certificate. Although your dates are different, it is expected that the
certificate expires in about ten years.
7.5. Delete the ~/.kube/ directory and run the oc login command. Ensure that the
warning about an expired certificate no longer displays.
[student@workstation ~]$ rm -rf ~/.kube/
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab certificates-troubleshoot finish
This concludes the section.
DO380-OCP4.4-en-2-20200803 195
Chapter 6 | Configuring Trusted TLS Certificates
Lab
Configuring Trusted TLS Certificates
In this lab, you will configure OpenShift to use certificates signed by an enterprise certificate
authority.
Outcomes
You should be able to:
• Update the ingress controller operator and the master API to use certificates signed by
the classroom certificate authority.
• Confirm that https://console-openshift-console.apps.ocp4.example.com
and https://api.ocp4.example.com:6443 use the updated certificates.
Before You Begin
To perform this exercise, ensure you have access to:
• A running OpenShift cluster.
On the workstation machine, use the lab command to prepare your system for this
exercise.
The command:
• Configures the OpenShift cluster to use default certificates.
• Creates a classroom CA certificate at /etc/pki/tls/certs/classroom-ca.pem.
• Downloads sample YAML files to /home/student/DO380/labs/certificatesreview/.
• Creates a combined wildcard and master API certificate signed by /etc/pki/tls/
certs/classroom-ca.pem.
[student@workstation ~]$ lab certificates-review start
Important
If the lab command returns the message, "Cannot login to OpenShift as
the 'kubeadmin' user with the password of '<PASSWORD>'", then the kubeapiserver
pods in the openshift-kube-apiserver namespace have
not finished redeploying. Monitor the progress of the kube-apiserver
cluster operator with the watch oc get co/kube-apiserver command.
Eventually you will only see the message, "Unable to connect to the server:
x509: certificate signed by unknown authority". Press Ctrl+C to exit the
watch command and then run the lab command again.
196 DO380-OCP4.4-en-2-20200803
Chapter 6 | Configuring Trusted TLS Certificates
Note
• Consider using the Configuring certificates chapter of the Red Hat
OpenShift Container Platform Authentication guide as a reference.
Instructions
1. As the OpenShift admin user, update the cluster-wide proxy to use the /home/student/
DO380/labs/certificates-review/review-combined.pem certificate.
2. Update the ingress controller operator to use the /home/student/DO380/labs/
certificates-review/review-combined.pem certificate and /home/student/
DO380/labs/certificates-review/review-key.pem key.
3. Update the API server to use the /home/student/DO380/labs/certificatesreview/
review-combined.pem certificate and /home/student/DO380/labs/
certificates-review/review-key.pem key.
4. Verify that https://console-openshift-console.apps.ocp4.example.com uses
the updated certificate.
5. Verify that https://api.ocp4.example.com:6443 uses the updated certificate.
Evaluation
As the student user on the workstation machine, use the lab command to grade your work.
Correct any reported failures and rerun the command until successful.
[student@workstation ~]$ lab certificates-review grade
Finish
As the student user on the workstation machine, use the lab command to complete this
exercise. This is important to ensure that resources from previous exercises do not impact
upcoming exercises.
[student@workstation ~]$ lab certificates-review finish
This concludes the section.
DO380-OCP4.4-en-2-20200803 197
Chapter 6 | Configuring Trusted TLS Certificates
Solution
Configuring Trusted TLS Certificates
In this lab, you will configure OpenShift to use certificates signed by an enterprise certificate
authority.
Outcomes
You should be able to:
• Update the ingress controller operator and the master API to use certificates signed by
the classroom certificate authority.
• Confirm that https://console-openshift-console.apps.ocp4.example.com
and https://api.ocp4.example.com:6443 use the updated certificates.
Before You Begin
To perform this exercise, ensure you have access to:
• A running OpenShift cluster.
On the workstation machine, use the lab command to prepare your system for this
exercise.
The command:
• Configures the OpenShift cluster to use default certificates.
• Creates a classroom CA certificate at /etc/pki/tls/certs/classroom-ca.pem.
• Downloads sample YAML files to /home/student/DO380/labs/certificatesreview/.
• Creates a combined wildcard and master API certificate signed by /etc/pki/tls/
certs/classroom-ca.pem.
[student@workstation ~]$ lab certificates-review start
Important
If the lab command returns the message, "Cannot login to OpenShift as
the 'kubeadmin' user with the password of '<PASSWORD>'", then the kubeapiserver
pods in the openshift-kube-apiserver namespace have
not finished redeploying. Monitor the progress of the kube-apiserver
cluster operator with the watch oc get co/kube-apiserver command.
Eventually you will only see the message, "Unable to connect to the server:
x509: certificate signed by unknown authority". Press Ctrl+C to exit the
watch command and then run the lab command again.
198 DO380-OCP4.4-en-2-20200803
Chapter 6 | Configuring Trusted TLS Certificates
Note
• Consider using the Configuring certificates chapter of the Red Hat
OpenShift Container Platform Authentication guide as a reference.
Instructions
1. As the OpenShift admin user, update the cluster-wide proxy to use the /home/student/
DO380/labs/certificates-review/review-combined.pem certificate.
1.1. Log into your OpenShift cluster as the admin user.
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
1.2. Change to the ~/DO380/labs/certificates-review/ directory.
[student@workstation ~]$ cd ~/DO380/labs/certificates-review/
1.3. Create the review-bundle configuration map in the openshift-config
namespace using the review-combined.pem certificate.
[student@workstation certificates-review]$ oc create configmap review-bundle \
> --from-file ca-bundle.crt=review-combined.pem -n openshift-config
configmap/review-bundle created
1.4. Modify the proxy-cluster.yml file.
[student@workstation certificates-review]$ vim proxy-cluster.yml
Replace <CONFIGMAP-NAME> with the name of the configuration map that you created
(review-bundle).
apiVersion: config.openshift.io/v1
kind: Proxy
metadata:
name: cluster
spec:
trustedCA:
name: review-bundle
1.5. Update the proxy/cluster resource to use the review-bundle configuration map.
DO380-OCP4.4-en-2-20200803 199
Chapter 6 | Configuring Trusted TLS Certificates
[student@workstation certificates-review]$ oc apply -f proxy-cluster.yml
Warning: oc apply should be used on resource created by either oc create --saveconfig
or oc apply
proxy.config.openshift.io/cluster configured
Note
The following oc patch command is an alternative to editing proxycluster.
yml and using the oc apply command.
$ oc patch proxy/cluster --type=merge \
-p '{"spec":{"trustedCA":{"name":"review-bundle"}}}'
2. Update the ingress controller operator to use the /home/student/DO380/labs/
certificates-review/review-combined.pem certificate and /home/student/
DO380/labs/certificates-review/review-key.pem key.
2.1. Create the review-tls secret in the openshift-ingress namespace using the
review-combined.pem certificate and review-key.pem key.
[student@workstation certificates-review]$ oc create secret tls review-tls \
> --cert review-combined.pem --key review-key.pem -n openshift-ingress
secret/review-tls created
2.2. Modify the ingresscontrollers.yml file.
[student@workstation certificates-review]$ vim ingresscontrollers.yml
Replace <SECRET-NAME> with the name of the secret that you created (reviewtls).
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
finalizers:
- ingresscontroller.operator.openshift.io/finalizer-ingresscontroller
name: default
namespace: openshift-ingress-operator
spec:
defaultCertificate:
name: review-tls
replicas: 2
2.3. Update the ingresscontrollers/default resource to use the review-tls
secret.
[student@workstation certificates-review]$ oc apply -f ingresscontrollers.yml
Warning: oc apply should be used on resource created by either oc create --saveconfig
or oc apply
ingresscontroller.operator.openshift.io/default configured
200 DO380-OCP4.4-en-2-20200803
Chapter 6 | Configuring Trusted TLS Certificates
Note
The following oc patch command is an alternative to editing
ingresscontrollers.yml and using the oc apply command.
$ oc patch ingresscontroller.operator default \
--type=merge -p '{"spec":{"defaultCertificate": \
{"name": "review-tls"}}}' -n openshift-ingress-operator
3. Update the API server to use the /home/student/DO380/labs/certificatesreview/
review-combined.pem certificate and /home/student/DO380/labs/
certificates-review/review-key.pem key.
3.1. Create the review-tls secret in the openshift-config namespace using the
review-combined.pem certificate and review-key.pem key.
[student@workstation certificates-review]$ oc create secret tls review-tls \
> --cert review-combined.pem --key review-key.pem -n openshift-config
secret/review-tls created
3.2. Modify the apiserver-cluster.yml file.
[student@workstation certificates-review]$ vim apiserver-cluster.yml
Replace <MASTER-API> with api.ocp4.example.com. Replace <SECRET-NAME>
with the name of the secret that you created (review-tls).
apiVersion: config.openshift.io/v1
kind: APIServer
metadata:
name: cluster
spec:
servingCerts:
namedCertificates:
- names:
- api.ocp4.example.com
servingCertificate:
name: review-tls
3.3. Update the apiserver/cluster resource to use the review-tls secret.
[student@workstation certificates-review]$ oc apply -f apiserver-cluster.yml
Warning: oc apply should be used on resource created by either oc create --saveconfig
or oc apply
apiserver.config.openshift.io/cluster configured
DO380-OCP4.4-en-2-20200803 201
Chapter 6 | Configuring Trusted TLS Certificates
Note
The following oc patch command is an alternative to editing apiservercluster.
yml and using the oc apply command.
$ oc patch apiserver cluster --type=merge \
-p '{"spec":{"servingCerts": {"namedCertificates": \
[{"names": ["api.ocp4.example.com"], "servingCertificate": \
{"name": "review-tls"}}]}}}'
4. Verify that https://console-openshift-console.apps.ocp4.example.com uses
the updated certificate.
4.1. A successful update to the ingresscontrollers/default resource redeploys the
router pods in the openshift-ingress namespace. Verify the redeployment of the
router pods. Press Ctrl+C to exit the watch command.
[student@workstation certificates-review]$ watch oc get pods \
> -n openshift-ingress
NAME READY STATUS RESTARTS AGE
router-default-567c74bcdd-6pvzz 1/1 Running 0 4m3s
router-default-567c74bcdd-prv7m 1/1 Running 0 3m45s
4.2. Access the web console URL at https://console-openshiftconsole.
apps.ocp4.example.com using either the curl command or Firefox.
[student@workstation certificates-review]$ curl -v $(oc whoami --show-console) \
> 2>&1 | grep 'SSL certificate verify ok.'
* SSL certificate verify ok.
If you access the URL using Firefox, then the URL field displays a green lock icon.
5. Verify that https://api.ocp4.example.com:6443 uses the updated certificate.
5.1. A successful update to the apiserver/cluster resource redeploys the apiserver
pods in the openshift-kube-apiserver namespace. Verify the redeployment
of the apiserver pods. The kube-apiserver cluster operator should no longer be
progressing. Press Ctrl+C to exit the watch command.
[student@workstation certificates-review]$ watch oc get co/kube-apiserver
NAME VERSION AVAILABLE PROGRESSING DEGRADED SINCE
kube-apiserver 4.4.8 True False False 28d
Note
It can take between five to ten minutes before the kube-apiserver pods finish
deploying.
5.2. Change to the /home/student/ directory.
[student@workstation certificates-review]$ cd
202 DO380-OCP4.4-en-2-20200803
Chapter 6 | Configuring Trusted TLS Certificates
5.3. Remove the ~/.kube/ directory.
[student@workstation ~]$ rm -rf ~/.kube/
5.4. Log into your OpenShift cluster as the admin user.
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
Evaluation
As the student user on the workstation machine, use the lab command to grade your work.
Correct any reported failures and rerun the command until successful.
[student@workstation ~]$ lab certificates-review grade
Finish
As the student user on the workstation machine, use the lab command to complete this
exercise. This is important to ensure that resources from previous exercises do not impact
upcoming exercises.
[student@workstation ~]$ lab certificates-review finish
This concludes the section.
DO380-OCP4.4-en-2-20200803 203
Chapter 6 | Configuring Trusted TLS Certificates
Summary
In this chapter, you learned:
• The OpenShift installer creates an internal certificate authority (CA) and uses this CA to sign
additional certificates.
• Because the default OpenShift certificates are not signed by a recognized certificate authority,
users that access the cluster must add one or more exceptions to accept OpenShift certificates.
• The ingress operator configures the ingress controller to route traffic into the OpenShift
environment. You can update the certificate that the ingress controller uses with a certificate
signed by a recognized certificate authority, or by your own enterprise CA.
• Changing the ingress controller operator to use a different certificate and its associated key
only requires a handful of steps. These steps include the new certificate and key in PEM format,
and the certificate needs to define a wildcard domain in the subjectAltName field. This allows
OpenShift to use that certificate as a wildcard certificate for the .apps subdomain.
• OpenShift makes it possible to update certificates in place without disrupting the applications
because operators monitor resources such as configuration or secrets, and automatically
redeploy the services responsible for serving the certificates.
204 DO380-OCP4.4-en-2-20200803
Chapter 7
Configuring Dedicated Node
Pools
Goal Add nodes to an OpenShift cluster with custom
configurations.
Objectives • Add nodes to an OpenShift cluster on userprovisioned
infrastructure.
• Create custom machine configuration pools
and edit existing machine configurations.
Sections • Adding Worker Nodes (and Guided Exercise)
• Creating Custom Machine Config Pools (and
Guided Exercise)
DO380-OCP4.4-en-2-20200803 205
Chapter 7 | Configuring Dedicated Node Pools
Adding Worker Nodes
Objectives
After completing this section, you should be able to add nodes to an OpenShift cluster on userprovisioned
infrastructure.
Discussing Node Scaling
The steps for performing instance scaling operations differ for installer-provisioned and userprovisioned
OpenShift clusters.
The Machine API automatically performs scaling operations for supported cloud providers. Modify
the number of replicas specified in a Machine Set, and OpenShift communicates with the cloud
provider to provision or deprovision instances.
In user-provisioned infrastructure, you must manually provision new instances. The exact strategy
for provisioning is specific to your data center and your IT processes, however the base steps
remain the same:
1. Update the worker Ignition file with an updated TLS certificate.
2. Install Red Hat Enterprise Linux CoreOS (RHCOS) from an ISO image or using a Preboot
eXecution Environment (PXE) boot.
3. Add the new instance to the ingress load balancer.
4. Approve Certificate Signing Requests (CSRs) to allow the worker to join the cluster.
Discussing Ignition
RHCOS is a RHEL-based operating system managed by the OpenShift Container Platform. Red
Hat discourages directly manipulating a RHCOS configuration. Instead, provide initial instance
configuration in the form of Ignition files.
Ignition is a provisioning tool designed to configure RHCOS on first boot. Supply configuration
in a JSON-formatted .ign file to declare changes to the disk such as partitions, systemd units,
certificates, and custom files.
The OpenShift installer generates the following Ignition files:
• bootstrap.ign
• master.ign
• worker.ign
Specify the worker.ign Ignition file as a kernel parameter when adding a new worker node.
After the instance is provisioned, changes to RHCOS are managed by the Machine Config
Operator. These changes are also specified in the Ignition format, but are limited in scope.
206 DO380-OCP4.4-en-2-20200803
Chapter 7 | Configuring Dedicated Node Pools
Updating the Worker TLS Certificate
The OpenShift installer creates the Ignition files with an initial bootstrapping TLS certificate. In
most production installations, this certificate expires 24 hours after installation.
Fetch the latest TLS certificate from the Machine Config Server listening on the control plane port
22623. Run the openssl s_client -connect api-int.ocp4.example.com:22623 -
showcerts command to retrieve the certificate. Extract the text wrapped between the BEGIN
CERTIFICATE and END CERTIFICATE markers.
Replace the base64-encoded certificate in the worker.ign as follows. Use the base64
command to encode the certificate text.
{
"ignition": {
"config": {
"append": [
{
"source": "https://api-int.ocp4.example.com:22623/config/worker",
"verification": {}
}
]
},
"security": {
"tls": {
"certificateAuthorities": [
{
"source": "data:text/plain;charset=utf-8;base64,LS0...==",
"verification": {}
}
]
}
},
"timeouts": {},
"version": "2.2.0"
},
"networkd": {},
"passwd": {},
"storage": {},
"systemd": {}
}
Specify the TLS certificate as a base64-encoded value in the TLS source field of the
certificateAuthorities list.
Note
Use the openssl x509 -text -noout command to view information about the
certificate such as the issuer, subject, and expiration date.
Installing with PXE boot
Install RHCOS from an ISO image or by using PXE boot. PXE booting performs a network-based
installation and is a common choice in an enterprise data center.
DO380-OCP4.4-en-2-20200803 207
Chapter 7 | Configuring Dedicated Node Pools
PXE relies on a set of very basic technologies:
• Dynamic Host Configuration Protocol (DHCP) for locating instances.
• Trivial File Transfer Protocol (TFTP) for serving the PXE files.
• HTTP for the ISO images and configuration files.
The RHCOS PXE configuration specifies the location of a kernel file, OS images, and an Ignition
configuration file.
Discussing Ingress Load Balancing
When adding worker nodes to a cluster, add the instance to external load balancer back-end pools.
For example, OpenShift schedules router pods on worker nodes. The ingress load balancer must
direct traffic to the new worker node instance.
OpenShift supports a variety of load balancers. The choice of specific load balancer technology is
outside the scope of this course.
The following is an example of an HAProxy configuration. HAProxy listens on port 443 and routes
traffic to either of the two servers.
Example /etc/haproxy/haproxy.cfg HAProxy
configuration file
frontend ingress_secure
bind *:443
mode tcp
default_backend ingress_secure_backend
backend ingress_secure_backend
balance roundrobin
mode tcp
server worker01 192.168.50.13:443 check
server worker02 192.168.50.14:443 check
Approving Certificate Signing Requests
When a new node attempts to join the cluster, OpenShift creates two Certificate
Signing Requests (CSRs). First, the Machine Config Operator client requests a CSR:
system:serviceaccount:openshift-machine-config-operator:nodebootstrapper.
Then, after you approve the node bootstrapper CSR, OpenShift creates a CSR
with the new node as the requestor, such as system:node:worker04.
Approve the certificate using oc adm certificate approve CSR command as follows.
208 DO380-OCP4.4-en-2-20200803
Chapter 7 | Configuring Dedicated Node Pools
[student@demo ~]$ oc get csr -A
NAME AGE REQUESTOR
CONDITION
csr-87kbl 75m system:node:master01
Approved,Issued
csr-knhlw 106m system:node:worker03
Approved,Issued
csr-lqcbd 4m19s system:serviceaccount:openshift-machine-config-operator:nodebootstrapper
Pending
[student@demo ~]$ oc adm certificate approve csr-lqcbd
certificatesigningrequest.certificates.k8s.io/csr-lqcbd approved
Alternatively, approve all CSRs using xargs.
[student@demo ~]$ oc get csr -A -o name | xargs oc adm certificate approve
Adding RHEL Compute Nodes
RHEL instances can join an OpenShift cluster as worker machines. RHEL instances are not
automatically managed and updated by OpenShift operators. A system administer must be
responsible for update management and operations.
References
For more information on adding workers, refer to the Red Hat Enterprise Linux
CoreOS (RHCOS) chapter in the Red Hat OpenShift Container Platform 4.4
Architecture documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html/architecture/architecture-rhcos
For more information on RHEL compute nodes, refer to the Adding RHEL compute
machines to an OpenShift Container Platform cluster chapter in the Red Hat
OpenShift Container Platform 4.4 Machine management documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/machine_management/
index#adding-rhel-compute
DO380-OCP4.4-en-2-20200803 209
Chapter 7 | Configuring Dedicated Node Pools
Guided Exercise
Adding Worker Nodes
In this exercise you will add worker nodes to an OpenShift cluster.
Outcomes
You should be able to:
• Examine PXE and Ignition files required for adding new nodes to a cluster.
• Update the worker Ignition file with a new certificate.
• Update the HAProxy configuration to add the new workers to the load balancer.
Before You Begin
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise.
[student@workstation ~]$ lab pools-adding-workers start
Instructions
1. From workstation, use ssh to connect to the utility machine as the lab user.
[student@workstation ~]$ ssh lab@utility
2. Examine the PXE files in the /var/lib/tftpboot/pxelinux.cfg directory.
2.1. List the files, and notice that three of the file names end with -NOOP. Worker
machines attempt to load a file that matches 01- followed by the MAC address of the
machine. The -NOOP suffix prevents the new workers from loading the file.
[lab@utility ~]$ ls /var/lib/tftpboot/pxelinux.cfg/
01-52-54-00-00-32-09 01-52-54-00-00-32-0b 01-52-54-00-00-32-0d
01-52-54-00-00-32-0f 01-52-54-00-00-32-0a 01-52-54-00-00-32-0c
01-52-54-00-00-32-0e 01-52-54-00-00-32-10-NOOP 01-52-54-00-00-32-11-NOOP
01-52-54-00-00-32-12-NOOP
2.2. View the contents of one of the PXE files.
[lab@utility ~]$ cat /var/lib/tftpboot/pxelinux.cfg/01-52-54-00-00-32-10-NOOP
default menu.c32
prompt 0
timeout 50
menu title **** OpenShift 4 Worker PXE Boot Menu ****
210 DO380-OCP4.4-en-2-20200803
Chapter 7 | Configuring Dedicated Node Pools
label Install CoreOS 4.4.8 Worker Node
kernel /openshift4/4.4.8/rhcos-4.4.3-x86_64-installer-kernel-x86_64
append ip=dhcp rd.neednet=1 coreos.inst.install_dev=vda console=tty0
console=ttyS0 coreos.inst=yes coreos.inst.image_url=http://192.168.50.254:8080/
openshift4/images/rhcos-4.4.3-x86_64-metal.x86_64.raw.gz
coreos.inst.ignition_url=http://192.168.50.254:8080/openshift4/4.4.8/
ignitions/worker.ign initrd=/openshift4/4.4.8/rhcos-4.4.3-x86_64-installerinitramfs.
x86_64.img
The PXE file references an Ignition file that is also hosted on the utility machine.
3. View the worker Ignition file and examine the embedded certificate.
3.1. List the contents of the /var/www/html/openshift4/4.4.8/ignitions/
worker.ign Ignition file. Pipe the output to jq for readability.
[lab@utility ~]$ ls /var/www/html/openshift4/4.4.8/ignitions/
bootstrap.ign master.ign worker.ign
[lab@utility ~]$ cat /var/www/html/openshift4/4.4.8/ignitions/worker.ign | jq
{
"ignition": {
...output omitted...
"security": {
"tls": {
"certificateAuthorities": [
{
"source": "data:text/plain;charset=utf-8;base64,LS0t...Cg==",
"verification": {}
}
...output omitted...
3.2. Copy the base64-encoded value of the source data to your clipboard by
selecting it and pressing Ctrl+Shift+C. Do not include the data:text/
plain;charset=utf-8;base64, prefix.
3.3. Use the echo command to print the copied base64-encoded certificate and pipe the
output to the base64 -d command.
[lab@utility ~]$ echo "LS0tL...g==" | base64 -d
-----BEGIN CERTIFICATE-----
MIIDEDCCAfigAwIBAgIIZeDuWKYIu6YwDQYJKoZIhvcNAQELBQAwJjESMBAGA1UE
CxMJb3BlbnNoaWZ0MRAwDgYDVQQDEwdyb290LWNhMB4XDTIwMDUyODE5MTAwNFoX
...ouput omitted...
O1PU1obMUx/pTZerQb0067kcMPo=
-----END CERTIFICATE-----
3.4. Use the openssl command to examine the x509 certificate.
[lab@utility ~]$ echo "LS0tL...g==" | base64 -d | openssl x509 -noout -text
Certificate:
Data:
Version: 3 (0x2)
Serial Number: 7341129457124031398 (0x65e0ee58a608bba6)
Signature Algorithm: sha256WithRSAEncryption
DO380-OCP4.4-en-2-20200803 211
Chapter 7 | Configuring Dedicated Node Pools
Issuer: OU = openshift, CN = root-ca
Validity
Not Before: May 28 19:10:04 2020 GMT
Not After : May 26 19:10:04 2030 GMT
Subject: OU = openshift, CN = root-ca
Subject Public Key Info:
Public Key Algorithm: rsaEncryption
...output omitted...
In a production environment, the certificate embedded in the worker Ignition file
might expire 24 hours after initial installation.
4. Exit the SSH session on utility to return to workstation.
5. Inspect and then run the prep-utility.yml Ansible Playbook.
5.1. Change to the ~/DO380/labs/pools-adding-workers/ directory and open the
prep-utility.yml file.
[student@workstation ~]$ cd ~/DO380/labs/pools-adding-workers/
[student@workstation pools-adding-workers]$ vim prep-utility.yml
5.2. Review the tasks defined in the playbook. Ansible updates the worker Ignition file with
a new certificate from api-int.ocp4.example.com, copies the PXE files to paths
without the "-NOOP" suffix, and restarts the TFTP socket. The final playbook tasks
update the HAProxy configuration to include the new workers.
5.3. Run the Ansible Playbook.
[student@workstation pools-adding-workers]$ ansible-playbook prep-utility.yml
PLAY [Add new workers] *******************************************************
TASK [Find OpenShift version directory] **************************************
ok: [utility]
...output omitted...
PLAY RECAP *******************************************************************
utility : ok=10 changed=6 unreachable=0 failed=0 skipped=0 ...
6. Reboot workers 04-06 using the ROL Lab Environment controls. The unconfigured
machines automatically reboot every five minutes. Manually reboot the machines from ROL
to avoid waiting.
6.1. Navigate to the ROL Lab Environment tab to control the classroom virtual
machines.
6.2. Click Open Console next to worker04, and then press any key or press Ctrl+Alt
+Del to reboot the machine. Return to the ROL Lab Environment tab and repeat
the process for worker05 and worker06.
212 DO380-OCP4.4-en-2-20200803
Chapter 7 | Configuring Dedicated Node Pools
Note
The workers will reboot a couple times during the PXE boot process.
7. Review the HAProxy configuration on the utility machine to view the new worker nodes.
The classroom runs HAProxy to round-robin load balance ingress traffic to the worker
nodes. In a previous step, the Ansible Playbook added the workers to the ingress back end.
7.1. From workstation, use ssh to run the cat command on the utility machine as
the lab user.
[student@workstation pools-adding-workers]$ ssh lab@utility \
> cat /etc/haproxy/haproxy.cfg
...output omitted...
7.2. Find and review the servers listed for the ingress_insecure_backend and
ingress_secure_backend near the end of the file.
8. Inspect and then run the approve-csrs.sh Bash script to approve the worker CSRs.
8.1. Log in to your OpenShift cluster as the admin user.
[student@workstation pools-adding-workers]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
8.2. Review the approve-csrs.sh script. The script approves pending CSRs in a while
loop until it counts six worker nodes.
8.3. Execute the script.
Note
It will take several minutes for the new worker machines to join the cluster.
[student@workstation pools-adding-workers]$ ./approve-csrs.sh
Worker count is 3.
certificatesigningrequest.certificates.k8s.io/csr-8mstv approved
Worker count is 4.
certificatesigningrequest.certificates.k8s.io/csr-g6sjn approved
certificatesigningrequest.certificates.k8s.io/csr-4j84s approved
certificatesigningrequest.certificates.k8s.io/csr-n4hhq approved
Worker count is 5.
certificatesigningrequest.certificates.k8s.io/csr-wh64h approved
Worker count is 6.
certificatesigningrequest.certificates.k8s.io/csr-kvhtg approved
9. Run oc get nodes to list the three new nodes.
DO380-OCP4.4-en-2-20200803 213
Chapter 7 | Configuring Dedicated Node Pools
Note
It will take a minute or two for all workers to reach a Ready state.
[student@workstation pools-adding-workers]$ oc get nodes
NAME STATUS ROLES AGE VERSION
master01 Ready master,worker 21d v1.17.1+3f6f40d
master02 Ready master,worker 21d v1.17.1+3f6f40d
master03 Ready master,worker 21d v1.17.1+3f6f40d
worker01 Ready worker 21d v1.17.1+3f6f40d
worker02 Ready worker 21d v1.17.1+3f6f40d
worker03 Ready worker 21d v1.17.1+3f6f40d
worker04 Ready worker 94s v1.17.1+3f6f40d
worker05 Ready worker 89s v1.17.1+3f6f40d
worker06 Ready worker 91s v1.17.1+3f6f40d
10. Change to the /home/student/ directory.
[student@workstation pools-adding-workers]$ cd
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
The workers added in this exercise will remain available for the rest of the course. They are used in
subsequent chapters.
[student@workstation ~]$ lab pools-adding-workers finish
This concludes the section.
214 DO380-OCP4.4-en-2-20200803
Chapter 7 | Configuring Dedicated Node Pools
Creating Custom Machine Config Pools
Objectives
After completing this section, you should be able to create custom machine configuration pools
and edit existing machine configurations.
Discussing the Machine Config Operator
The Machine Config Operator (MCO) manages instance configuration changes and operating
system upgrades. OpenShift administrators can set custom node configurations by declaring
MachineConfig and MachineConfigPool resources.
The MCO defines two custom resources.
MachineConfig (MC)
Machine Configs declare instance customizations using the Ignition config format. Machine
Configs are labeled with a role such as worker or infra.
MachineConfigPool (MCP)
Machine Config Pools use labels to match one or more Machine Configs to one or more
nodes. This creates a pool of nodes with the same configuration. The Machine Config
Operator uses the Machine Config Pool to track update status as it applies Machine Configs
to the nodes.
Creating Machine Configs
Machine Configs use the Ignition configuration format. Ignition was originally designed as an
immutable configuration tool to execute only during initial instance provisioning. The Machine
Config Daemon supports a limited set of configuration changes including the following:
• Configuring core user SSH authorized keys.
• Declaring Systemd units.
• Writing custom files.
The Machine Config Operator reads Machine Configs alphanumerically, from 00-* to 99-\*.
Changes to the same file are overwritten by Machine Configs named with a higher number. The
resulting compilation of Machine Configs are stored in a "rendered" Machine Config resource.
Machine Configs are specified with a machineconfiguration.openshift.io/role label.
List Machine Configs for a specific role using the --selector argument.
[student@demo ~]$ oc get machineconfig \
> --selector=machineconfiguration.openshift.io/role=worker
NAME GENERATEDBYCONTROLLER IGNITIONVERSION AGE
00-worker 910f22cb1550a...2b1566ce5f 2.2.0 20d
01-worker-container-runtime 910f22cb1550a...2b1566ce5f 2.2.0 20d
01-worker-kubelet 910f22cb1550a...2b1566ce5f 2.2.0 20d
99-worker-...-registries 910f22cb1550a...2b1566ce5f 2.2.0 20d
99-worker-ssh 2.2.0 20d
DO380-OCP4.4-en-2-20200803 215
Chapter 7 | Configuring Dedicated Node Pools
Writing Custom Files
Specify custom file contents, such as the Systemd configuration or TLS certificates, in an escaped
format following the data URL scheme standard. File contents are typically Base64 encoded in
ignition files. Other content, like Systemd units or ssh keys, are not Base64 encoded. In a Terminal,
use the base64 and base64 -d commands to encode files or standard input.
Journald MachineConfig example
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
labels:
machineconfiguration.openshift.io/role: worker
name: 60-journald
spec:
config:
ignition:
version: 2.2.0
storage:
files:
- contents:
source: data:text/plain;charset=utf-8;base64,VGVzdGl...E8gKItAo=
filesystem: root
mode: 0644
path: /etc/systemd/journald.conf
Label Machine Config resources by role.
Prefix the name with a two-digit number that specifies when to apply the configuration,
relative to Machine Configs belonging to the same Machine Config Pool.
Use the data URL format to embed escaped file content. Base64 encoding is common for
Ignition files.
Creating Machine Config Pools
Machine Config Pools specify a machineConfigSelector and a nodeSelector. By default,
OpenShift includes only master and worker Machine Config Pools, but custom pools can be
added.
Custom pools are a composition of worker and custom Machine Configs. The
machineConfigSelector match expression selects both worker and the custom role label.
Nodes assigned to the custom pool use Machine Configs from the worker with the additions
supplied by the custom role. This is critical so that worker operating system updates managed by
OpenShift are applied to the machines in the pool.
The following MachineConfigPool specification demonstrates creating a separate pool for
machine learning (ml) nodes.
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
name: ml
spec:
machineConfigSelector:
matchExpressions:
216 DO380-OCP4.4-en-2-20200803
Chapter 7 | Configuring Dedicated Node Pools
- key: machineconfiguration.openshift.io/role
operator: In
values: [worker, ml]
nodeSelector:
matchLabels:
node-role.kubernetes.io/ml: ""
Include both worker and ml Machine Configs.
Apply to nodes with the node-role.kubernetes.io/ml label.
Labeling Nodes
Add a custom role to a node by labeling the node with a node-role.kubernetes.io/ROLE key.
Nodes can have multiple roles.
The example below adds an infra node commonly used for scheduling infrastructure workloads,
such as cluster logging.
[student@demo ~]$ oc label node/worker03 node-role.kubernetes.io/infra=
node/worker03 labeled
[student@demo ~]$ oc get nodes
NAME STATUS ROLES AGE VERSION
master01 Ready master 5d3h v1.17.1
master02 Ready master 5d3h v1.17.1
master03 Ready master 5d3h v1.17.1
worker01 Ready worker 5d3h v1.17.1
worker02 Ready worker 5d3h v1.17.1
worker03 Ready infra,worker 119s v1.17.1
Notice that nodes can be assigned multiple roles, and Machine Config Pools can select multiple
Machine Configs. Because the infra Machine Config Pool includes both worker and infra
Machine Configs, you can remove the worker role from the node.
[student@demo ~]$ oc label node/worker03 node-role.kubernetes.io/workernode/
worker03 labeled
[student@demo ~]$ oc get nodes
NAME STATUS ROLES AGE VERSION
master01 Ready master 5d3h v1.17.1
master02 Ready master 5d3h v1.17.1
master03 Ready master 5d3h v1.17.1
worker01 Ready worker 5d3h v1.17.1
worker02 Ready worker 5d3h v1.17.1
worker03 Ready infra 2m11s v1.17.1
Alternatively, add custom roles to the existing worker roles.
DO380-OCP4.4-en-2-20200803 217
Chapter 7 | Configuring Dedicated Node Pools
[student@demo ~]$ oc get nodes
NAME STATUS ROLES AGE VERSION
master01 Ready master 5d3h v1.17.1
master02 Ready master 5d3h v1.17.1
master03 Ready master 5d3h v1.17.1
worker01 Ready worker,app 5d3h v1.17.1
worker02 Ready worker,app 5d3h v1.17.1
worker03 Ready worker,infra 2m11s v1.17.1
Configuring Pod Scheduling
Specify a nodeSelector to assign workloads to nodes that match the label.
apiVersion: apps/v1
kind: Deployment
metadata:
name: example
spec:
replicas: 4
selector:
matchLabels:
app: example
template:
metadata:
labels:
app: example
spec:
nodeSelector:
node-role.kubernetes.io/infra: ""
containers:
- image: example:v1.0
name: example
ports:
- containerPort: 8080
protocol: TCP
To define a default node selector, edit the defaultNodeSelector using the oc edit
scheduler cluster command. After editing the scheduler resource, wait several minutes for all
API server replicas to restart and the change to take effect.
If a defaultNodeSelector is defined, you must specify a node selector to override the default.
The oc debug node command cannot schedule on nodes other than the default. To work around
this issue, create a new project with the --node-selector="" flag and run the debug in that
namespace.
[student@demo ~]$ oc debug node/master01
Starting pod/master01-debug ...
To use host binaries, run `chroot /host`
Removing debug pod ...
Error from server (BadRequest): container "container-00" in pod "master01-debug"
is not available
218 DO380-OCP4.4-en-2-20200803
Chapter 7 | Configuring Dedicated Node Pools
[student@demo ~]$ oc adm new-project debug --node-selector=""
Created project debug
[student@demo ~]$ oc debug node/master01 -n debug
Starting pod/master01-debug ...
To use host binaries, run `chroot /host`
Pod IP: 192.168.50.10
If you don't see a command prompt, try pressing enter.
sh-4.2#
Node taints prevent workloads from being scheduled on nodes without the proper tolerations.
However, several OpenShift DaemonSets declare specific tolerations that will cause unexpected
behavior if these tolerances conflict with custom taints. For example, the Machine Config Daemon
tolerates master and etcd taints. If a custom NoSchedule taint is added, the Machine Config
Daemon will not be able to apply operating system updates to the machine.
Observing Machine Config Pool Updates
The Machine Config Daemon manages the node update, using several node annotations that are
useful for understanding the state of the update.
• machine-config-daemon.v1.openshift.com/currentConfig
• machine-config-daemon.v1.openshift.com/desiredConfig
• machine-config-daemon.v1.openshift.com/state
Remember that you can use the oc describe command to view annotations.
[student@demo ~]$ oc describe node worker01
Name: worker01
Roles: worker
Labels: beta.kubernetes.io/arch=amd64
beta.kubernetes.io/os=linux
kubernetes.io/arch=amd64
kubernetes.io/hostname=worker01
kubernetes.io/os=linux
node-role.kubernetes.io/worker=
node.openshift.io/os_id=rhcos
Annotations: machinecon...openshift.io/currentConfig: rendered-worker-370...bfd
machinecon...openshift.io/desiredConfig: rendered-worker-370...bfd
machineconfiguration.openshift.io/reason:
machineconfiguration.openshift.io/state: Done
volumes.kubernetes.io/controller-managed-attach-detach: true
...output omitted...
When the desired config does not match the current config, the Machine Config Daemon will:
1. Apply the rendered Machine Config.
2. Drain pods from the node.
3. Reboot the machine.
List the overall status of pool updates using the oc get machineconfigpool command.
During the first update the CONFIG field will remain blank.
DO380-OCP4.4-en-2-20200803 219
Chapter 7 | Configuring Dedicated Node Pools
[student@demo ~]$ oc get machineconfigpool
NAME CONFIG UPDATED UPDATING DEGRADED
infra False True False
master rendered-master-716...267 True False False
worker rendered-worker-573...3db True False False
References
For more information on pod scheduling, refer to the Placing pods on specific
nodes using node selectors section in the Working with pods chapter in the Red Hat
OpenShift Container Platform 4.4 Nodes documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/nodes/index#nodes-pods-nodeselectors
For more information on managing nodes, refer to the Working with nodes chapter in
the Red Hat OpenShift Container Platform 4.4 Nodes documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/nodes/index#working-with-nodes
OpenShift Container Platform 4: How does Machine Config Pool work?
https://www.redhat.com/en/blog/openshift-container-platform-4-how-doesmachine-
config-pool-work
The "data" URL scheme
https://tools.ietf.org/html/rfc2397
220 DO380-OCP4.4-en-2-20200803
Chapter 7 | Configuring Dedicated Node Pools
Guided Exercise
Creating Custom Machine Config Pools
In this exercise you will create an infra Machine Config Pool and add a custom Machine
Config.
Outcomes
You should be able to:
• Create custom Machine Config Pools and Machine Configs.
• Observe the status of Machine Config updates.
Before You Begin
To perform this exercise, ensure you have completed Guided Exercise: Adding Worker Nodes.
Nodes worker04, worker05, and worker06 must be added to the cluster and in a Ready
state.
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise.
[student@workstation ~]$ lab pools-creating start
Instructions
1. Label nodes worker04, worker05, and worker06 with the infra role, and then remove
the worker role.
1.1. Use the oc label node command to add the node-role.kubernetes.io/
infra= label.
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
[student@workstation ~]$ oc get nodes
NAME STATUS ROLES AGE VERSION
master01 Ready master,worker 21d v1.17.1+3f6f40d
master02 Ready master,worker 21d v1.17.1+3f6f40d
master03 Ready master,worker 21d v1.17.1+3f6f40d
worker01 Ready worker 21d v1.17.1+3f6f40d
worker02 Ready worker 21d v1.17.1+3f6f40d
worker03 Ready worker 21d v1.17.1+3f6f40d
worker04 Ready worker 94s v1.17.1+3f6f40d
worker05 Ready worker 89s v1.17.1+3f6f40d
worker06 Ready worker 91s v1.17.1+3f6f40d
[student@workstation ~]$ oc label node/worker04 node-role.kubernetes.io/infra=
DO380-OCP4.4-en-2-20200803 221
Chapter 7 | Configuring Dedicated Node Pools
node/worker04 labeled
[student@workstation ~]$ oc label node/worker05 node-role.kubernetes.io/infra=
node/worker05 labeled
[student@workstation ~]$ oc label node/worker06 node-role.kubernetes.io/infra=
node/worker06 labeled
1.2. Use the oc label node command to remove the node-role.kubernetes.io/
worker label.
[student@workstation ~]$ oc label node/worker04 node-role.kubernetes.io/workernode/
worker04 labeled
[student@workstation ~]$ oc label node/worker05 node-role.kubernetes.io/workernode/
worker05 labeled
[student@workstation ~]$ oc label node/worker06 node-role.kubernetes.io/workernode/
worker06 labeled
1.3. Use the oc get nodes command to review the updated roles.
[student@workstation ~]$ oc get nodes
NAME STATUS ROLES AGE VERSION
master01 Ready master,worker 21d v1.17.1+3f6f40d
master02 Ready master,worker 21d v1.17.1+3f6f40d
master03 Ready master,worker 21d v1.17.1+3f6f40d
worker01 Ready worker 21d v1.17.1+3f6f40d
worker02 Ready worker 21d v1.17.1+3f6f40d
worker03 Ready worker 21d v1.17.1+3f6f40d
worker04 Ready infra 3m54s v1.17.1+3f6f40d
worker05 Ready infra 3m49s v1.17.1+3f6f40d
worker06 Ready infra 3m51s v1.17.1+3f6f40d
2. Change to the ~/DO380/labs/pools-creating/ directory, and update the Machine
Config defined in motd-mc.yml to add a custom /etc/motd file.
2.1. Encode the contents of motd.txt to Base64. Copy the encoded value to the
clipboard.
[student@workstation ~]$ cd ~/DO380/labs/pools-creating
[student@workstation pools-creating]$ base64 motd.txt
VGhpcyBpcyBhIGN1c3RvbSBtZXNzYWdlIG9mIHRoZSBkYXkgZmlsZS4K
2.2. Replace the text in motd-mc.yml marked CHANGE_ME. Update the metadata label
machineconfiguration.openshift.io/role to infra and the name to 50-
motd. Update the data portion of the source value to the base64-encoded value of
motd.txt stored in your clipboard.
2.3. Verify that the completed file reads as follows.
222 DO380-OCP4.4-en-2-20200803
Chapter 7 | Configuring Dedicated Node Pools
[student@workstation pools-creating]$ cat motd-mc.yml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
labels:
machineconfiguration.openshift.io/role: infra
name: 50-motd
spec:
config:
ignition:
version: 2.2.0
storage:
files:
- contents:
source: data:text/plain;charset=utf-8;base64,VGhpcpcyBh...9mIHRZS4K
filesystem: root
mode: 0644
path: /etc/motd
3. Update the Machine Config Pool defined in infra-mcp.yml to apply worker and infra
Machine Configs to nodes matching the node-role.kubernetes.io/infra label.
3.1. Replace CHANGE_ME in the machineConfigSelector with the text
[worker,infra].
3.2. Replace CHANGE_ME in the nodeSelector with the text noderole.
kubernetes.io/infra: "".
3.3. Verify the completed file reads as follows.
[student@workstation pools-creating]$ cat infra-mcp.yml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
name: infra
spec:
machineConfigSelector:
matchExpressions:
- key: machineconfiguration.openshift.io/role
operator: In
values: [worker,infra]
nodeSelector:
matchLabels:
node-role.kubernetes.io/infra: ""
4. Create both resources in the cluster.
[student@workstation pools-creating]$ oc create -f .
machineconfigpool.machineconfiguration.openshift.io/infra created
machineconfig.machineconfiguration.openshift.io/50-motd created
5. List the Machine Configs and verify that the 50-motd Machine Config is included.
DO380-OCP4.4-en-2-20200803 223
Chapter 7 | Configuring Dedicated Node Pools
[student@workstation pools-creating]$ oc get mc
NAME GENERATEDBYCONTROLLER IGNITIONVERSION AGE
00-master 8af4f...314 2.2.0 17d
00-worker 8af4f...314 2.2.0 17d
01-master-container-runtime 8af4f...314 2.2.0 17d
01-master-kubelet 8af4f...314 2.2.0 17d
01-worker-container-runtime 8af4f...314 2.2.0 17d
01-worker-kubelet 8af4f...314 2.2.0 17d
50-motd 2.2.0 2m
99-master-fa9d55f3-...-registries 8af4f...314 2.2.0 17d
99-master-ssh 2.2.0 17d
99-worker-92b9381f-...-registries 8af4f...314 2.2.0 17d
99-worker-ssh 2.2.0 17d
rendered-infra-f25cb509a35...ba2 8af4f...314 2.2.0 15s
rendered-master-71638c3844...267 8af4f...314 2.2.0 17d
rendered-worker-5732daa0c2...3db 8af4f...314 2.2.0 17d
6. List the Machine Config Pools, and then describe the infra Machine Config Pool.
6.1. Use the oc get mcp command to list the MachineConfigPool resources. Notice
that the Machine Config Pool is currently in an UPDATING state.
[student@workstation pools-creating]$ oc get mcp
NAME CONFIG UPDATED UPDATING DEGRADED ...
infra rendered-infra-f25... False True False ...
master rendered-master-71... True False False ...
worker rendered-worker-57... True False False ...
6.2. Use the oc describe mcp/infra command to view details about the infra
Machine Config Pool. Review the source Machine Configs added to the Spec.
[student@workstation pools-creating]$ oc describe mcp/infra
Name: infra
Namespace:
Labels: <none>
Annotations: <none>
API Version: machineconfiguration.openshift.io/v1
Kind: MachineConfigPool
Metadata:
Creation Timestamp: 2020-06-14T23:55:46Z
Generation: 2
Resource Version: 211637
Self Link: /apis/machine...openshift.io/v1/machineconfigpools/infra
UID: d7c2b26a-1bb9-4f8a-84fe-57ba1cb1596e
Spec:
Configuration:
Name: rendered-infra-f25cb509a35fdca08b5fb488a8a68ba2
Source:
API Version: machineconfiguration.openshift.io/v1
Kind: MachineConfig
Name: 00-worker
API Version: machineconfiguration.openshift.io/v1
224 DO380-OCP4.4-en-2-20200803
Chapter 7 | Configuring Dedicated Node Pools
Kind: MachineConfig
Name: 01-worker-container-runtime
API Version: machineconfiguration.openshift.io/v1
Kind: MachineConfig
Name: 01-worker-kubelet
API Version: machineconfiguration.openshift.io/v1
Kind: MachineConfig
Name: 50-motd
...output omitted...
7. Check the state of the updating nodes.
7.1. Use the oc get nodes command to list the nodes. Notice that the infra node
reports a SchedulingDisabled status. The node is updating with the new Machine
Config.
[student@workstation pools-creating]$ oc get nodes
NAME STATUS ROLES AGE VERSION
master01 Ready master,worker 17d v1.17.1
master02 Ready master,worker 17d v1.17.1
master03 Ready master,worker 17d v1.17.1
worker01 Ready worker 17d v1.17.1
worker02 Ready worker 17d v1.17.1
worker03 Ready worker 17d v1.17.1
worker04 Ready infra 44m v1.17.1
worker05 Ready,SchedulingDisabled infra 44m v1.17.1
worker06 Ready infra 44m v1.17.1
7.2. Describe the node to review the machineconfiguration.openshift.io
annotations.
[student@workstation pools-creating]$ oc describe node/worker05
Name: worker05
Roles: infra,worker
Labels: beta.kubernetes.io/arch=amd64
beta.kubernetes.io/os=linux
kubernetes.io/arch=amd64
kubernetes.io/hostname=worker05
kubernetes.io/os=linux
node-role.kubernetes.io/infra=
node-role.kubernetes.io/worker=
node.openshift.io/os_id=rhcos
Annotations: ma...openshift.io/currentConfig: rendered-infra-f25cb...ba2
ma...openshift.io/desiredConfig: rendered-infra-f25cb...ba2
machineconfiguration.openshift.io/reason:
machineconfiguration.openshift.io/state: Done
...output omitted...
8. After the update is complete, use oc debug node to verify that the file updated. Retry in
a few minutes if the first attempt fails.
DO380-OCP4.4-en-2-20200803 225
Chapter 7 | Configuring Dedicated Node Pools
[student@workstation pools-creating]$ oc debug node/worker05
Starting pod/worker05-debug ...
To use host binaries, run `chroot /host`
Pod IP: 192.168.50.17
If you don't see a command prompt, try pressing enter.
sh-4.2# chroot /host
sh-4.4# cat /etc/motd
This is a custom message of the day file.
9. Change to the /home/student/ directory.
[student@workstation pools-creating]$ cd
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab pools-creating finish
This concludes the section.
226 DO380-OCP4.4-en-2-20200803
Chapter 7 | Configuring Dedicated Node Pools
Summary
In this chapter, you learned:
• How to use Ignition as a provisioning tool for Red Hat Enterprise Linux CoreOS.
• How to add nodes to an existing user-provisioned OpenShift cluster by updating the worker
Ignition file and PXE booting new nodes.
• How to use Machine Config Operator, MachineConfigs, and MachineConfigPools, and how they
are related.
• How to create custom MachineConfigs and MachineConfig pools and how they get applied to
nodes in the cluster.
DO380-OCP4.4-en-2-20200803 227
228 DO380-OCP4.4-en-2-20200803
Chapter 8
Configuring Persistent Storage
Goal Configure storage providers and storage classes
to ensure cluster user access to persistent volume
resources.
Objectives • Describe the components of the OpenShift
storage architecture.
• Configure an application with shared file
storage.
• Configure a database application with block
storage.
• Install and configure the Local Storage
operator to create local storage volumes for an
OpenShift cluster.
Sections • Describing the OpenShift Storage Architecture
(and Quiz)
• Provisioning Shared Storage for Applications
(and Guided Exercise)
• Provisioning Block Storage for Databases (and
Guided Exercise)
• Provisioning Local Block Storage (and Guided
Exercise)
Lab Configuring Persistent Storage
DO380-OCP4.4-en-2-20200803 229
Chapter 8 | Configuring Persistent Storage
Describing the OpenShift Storage
Architecture
Objectives
After completing this section, you should be able to describe the components of the OpenShift
storage architecture.
Persistent Storage Overview
Many applications require long-term data storage, and each application has unique storage needs.
Many storage technologies exist to address application storage requirements, each with different
benefits, drawbacks, and costs.
Cluster administrators can provide a variety of storage resources to cluster users, using the
OpenShift storage architecture.
Persistent Volumes
A persistent volume (PV), is an OpenShift cluster resource that describes an allocatable storage
volume. A persistent volume defines metadata and configuration, which OpenShift uses to mount
the volume on a node. After OpenShift mounts a volume on a node, then it can deploy pods that
use the storage volume.
Each persistent volume identifies a volume plug-in, which is automation that OpenShift uses
to mount a storage volume. The persistent volume resource defines parameters that the volume
plug-in requires to mount storage. For example, an NFS persistent volume defines an nfs section
that contains parameters to mount an NFS share. To mount the persistent volume on a node,
OpenShift executes the NFS volume plug-in with parameters defined in the NFS persistent
volume.
By default, only privileged users can view persistent volume resources. Persistent volumes may
contain sensitive information, such as remote storage endpoints or authentication details.
Storage Classes
OpenShift uses storage class resources to describe different types of persistent volumes in a
cluster. A storage class is a cluster resource and does not contain sensitive information. Nonprivileged
users can view the storage classes that are available in a cluster.
A storage class describes high-level storage characteristics, such as quality of service,
throughput, or storage technology. A storage class resource does not contain low-level storage
implementation details. A storage class is a public resource describing a type of storage volume in
the cluster that does not expose sensitive configuration or information.
A storage class must define a provisioner attribute. OpenShift uses a provisioner to create
persistent volumes on demand. OpenShift does not contain a built-in provisioner for all storage
technologies.
Persistent Volume Claims
Non-privileged users create a persistent volume claim (PVC), to request a persistent volume
resource. A persistent volume claim specifies high-level storage criteria for a volume, such as:
230 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
• storage size
• storage class
• access mode
OpenShift attempts to match each persistent volume claim with an available persistent volume in
the cluster. If OpenShift identifies a persistent volume match for a persistent volume claim, then
OpenShift binds the persistent volume to the persistent volume claim.
Describing Persistent Volumes
An example persistent volume resource definition follows:
apiVersion: v1
kind: PersistentVolume
metadata:
name: pv0003
spec:
capacity:
storage: 5Gi
volumeMode: Filesystem
accessModes:
- ReadWriteOnce
persistentVolumeReclaimPolicy: Recycle
storageClassName: slow
mountOptions:
- hard
- nfsvers=4.1
nfs:
path: /tmp
server: 172.17.0.2
The name of the persistent volume.
The size of the underlying storage volume.
Supported Block and Filesystem volume modes. A Filesystem volume is mounted as a
directory, while a Block volume is mounted as a raw block device.
The access mode list describes how the volume can be accessed from one or more cluster
nodes. A ReadWriteOnce volume can only be mounted as read-write on a single cluster
node. Access modes are discussed elsewhere in this course.
The reclaim policy is one of: Retain, Recycle, or Delete. The reclaim policy determines the
actions to perform on a persistent volume when that persistent volume is released.
The storage class name for the persistent volume. A persistent volume claim must request
the slow storage class to bind to this persistent volume.
This persistent volume uses the NFS volume plug-in. The nfs section defines parameters
that the NFS volume plug-in needs to mount the volume on a node. This section includes
sensitive NFS configuration information.
Provisioning and Binding Persistent Volumes
A persistent volume claim uses a storage class name to request a particular type of storage.
OpenShift examines the pool of existing persistent volumes. If the storage class name for an
existing persistent volume matches the request, then the persistent volume becomes a match
candidate for the request.
DO380-OCP4.4-en-2-20200803 231
Chapter 8 | Configuring Persistent Storage
Note
A persistent volume claim can request storage without specifying a storage class.
When a PVC does not specify a storage class, then OpenShift uses a default
storage class. The cluster administrator designates one of the storage classes as the
default storage class.
OpenShift further filters the match candidate list, based on other request criteria. If any persistent
volumes satisfy all of the request criteria, then OpenShift binds one of the persistent volumes to
the persistent volume claim. A persistent volume claim is exclusive; a persistent volume cannot
bind to more than one persistent volume claim.
If no existing persistent volumes satisfy the request criteria, OpenShift attempts to provision a
volume on demand. OpenShift provides the storage class provisioner with parameters defined in
the storage class, ensuring that the correct type of storage is created. Dynamic provisioning
refers to the creation of persistent volumes on-demand.
Note
If the provisioner attribute of a storage class is set to kubernetes.io/noprovisioner,
then a provisioner is not available to create storage volumes.
For technologies that do not have a provisioner, you must find other methods to create storage
volumes. Recommended methods include:
• Install a storage operator. Many third-party and open source operators exist that manage the
provisioning of persistent volumes and related storage resources.
In a later exercise, you install an operator to manage local storage persistent volumes.
• Write and use Ansible Playbooks. Playbooks enable configuration management at scale. Use
a playbook to create and manage storage volumes, persistent volumes, and storage classes.
In a later exercise, you use a playbook to create iSCSI targets, iSCSI persistent volumes, and an
iSCSI storage class.
Releasing a Persistent Volume
When you delete a persistent volume claim, you indicate that you no longer need the bound
persistent volume. If no pods are using the persistent volume, then OpenShift deletes the
persistent volume claim immediately. OpenShift delays deletion until there are no running pods
using the persistent volume.
After deleting a persistent volume claim resource, OpenShift releases the associated persistent
volume.
Reclaiming a Persistent Volume
OpenShift initiates a reclaim policy for a persistent volume after OpenShift releases it.
OpenShift defines three reclaim policies:
232 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
Delete
The persistent volume is not put back into the pool of available persistent volumes. The
persistent volume resource is deleted, and the underlying storage volume is also deleted. All
dynamically-provisioned persistent volumes use a Delete reclaim policy.
Retain
OpenShift takes no immediate action to reclaim the volume. A cluster administrator must
ensure proper handling of this volume, which may include data backup and volume deletion.
Recycle
OpenShift deletes all data on the volume, which enables volume reuse. After deletion, the
volume is available to use again.
References
For more information about the OpenShift storage architecture, refer to the
Understanding Persistent Storage chapter in the Red Hat OpenShift Container
Platform 4.4 Storage documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/storage/index#understandingpersistent-
storage
For more information about storage classes, refer to the Defining a StorageClass
section in the Dynamic Provisioning chapter in the Red Hat OpenShift Container
Platform 4.4 Storage documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/storage/index#defining-storageclasses_
dynamic-provisioning
DO380-OCP4.4-en-2-20200803 233
Chapter 8 | Configuring Persistent Storage
Quiz
Describing the OpenShift Storage
Architecture
Match the items below to their counterparts in the table.
A cluster resource that defines characteristics for a particular type of storage, which
users can request.
A cluster resource that defines information that OpenShift needs to mount a volume
to a node.
A project resource that defines a request for storage with specific storage
characteristics.
Code that OpenShift uses to create a storage resource.
Code that OpenShift uses to mount a storage resource on a node.
OpenShift Storage
Component Description
Provisioner
Persistent Volume
Storage Class
Persistent Volume Claim
Volume Plug-in
This concludes the section.
234 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
Solution
Describing the OpenShift Storage
Architecture
Match the items below to their counterparts in the table.
OpenShift Storage
Component Description
Provisioner Code that OpenShift uses to create a storage resource.
Persistent Volume
A cluster resource that defines information that
OpenShift needs to mount a volume to a node.
Storage Class
A cluster resource that defines characteristics for a
particular type of storage, which users can request.
Persistent Volume Claim
A project resource that defines a request for storage with
specific storage characteristics.
Volume Plug-in
Code that OpenShift uses to mount a storage resource on
a node.
This concludes the section.
DO380-OCP4.4-en-2-20200803 235
Chapter 8 | Configuring Persistent Storage
Provisioning Shared Storage for
Applications
Objectives
After completing this section, you should be able to configure an application with shared file
storage.
Describing Shared Storage
Many modern applications use shared storage. Shared storage enables application scaling and
parallel processing of data.
Examples of applications using shared storage:
Continuous Integration/Continuous Delivery pipelines
An application clones source code to a shared location. Agents execute tests concurrently on
the shared files.
Artificial Intelligence Workloads
Model training algorithms process large data sets in parallel. Each parallel process has access
to the same shared data set.
Streaming applications
An application collects unstructured data and writes that data to a common location. That
data is processed at a later time.
Modern storage technologies fall into three broad categories:
File Storage
File storage technologies store data as files in a file hierarchy. Network-attached Storage
(NAS) is an example of a file storage technology. Common NAS technologies include:
• Network File System (NFS)
• Server Message Block (SMB)
• Common Internet File System (CIFS)
Block Storage
Block storage technologies store data in blocks on a block storage device. Examples include
Storage Area Networks (SAN), Amazon Elastic Block Store (EBS), and Google Persistent
Disks.
Object Storage
Object storage technologies store data as an object with arbitrary metadata. Objects are
not stored in a file system hierarchy. Examples include Amazon Simple Storage Service (S3),
Red Hat Ceph Storage, and OpenStack Object Storage (Swift).
Use file or object storage technologies for shared storage.
Because block storage technologies allow only single-client access to a storage volume, block
storage is not shareable.
236 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
Accessing Persistent Shared Storage
Before Kubernetes deploys a pod that requires persistent storage, Kubernetes first mounts the
storage volume on the scheduled node. A persistent volume contains information that Kubernetes
needs to mount the storage volume on a node. That information includes a list of access modes
supported by the underlying storage volume.
An access mode indicates the ability (or inability) to mount a storage volume on many different
nodes based on the capabilities of the storage provider. Kubernetes defines three access modes:
Persistent Volume Access Modes
Access Mode CLI Abbreviation Description
ReadWriteMany RWX Kubernetes can mount the
volume as read-write on
many nodes.
ReadOnlyMany ROX Kubernetes can mount the
volume as read-only on many
nodes.
ReadWriteOnce RWO Kubernetes can mount the
volume as read-write on only
a single node.
Note
An application that requires shared storage must request a persistent volume with a
ReadOnlyMany or ReadWriteMany access mode.
Kubernetes uses a volume plug-in to mount a storage volume on a node. Each different
storage technology corresponds to a separate volume plug-in. Every persistent volume specifies
a volume plug-in. A persistent volume must also define metadata that the volume plug-in requires
to mount the storage volume on a node.
The following table lists the built-in OpenShift volume plug-ins, along with the supported access
modes for the plug-in:
OpenShift Volume Plug-in support for Access Modes
Volume Plug-in ReadWriteOnce ReadOnlyMany ReadWriteMany
AWS EBS Yes No No
Azure File Yes Yes Yes
Azure Disk Yes No No
Cinder Yes No No
Fibre Channel Yes Yes No
GCE Persistent Disk Yes No No
DO380-OCP4.4-en-2-20200803 237
Chapter 8 | Configuring Persistent Storage
Volume Plug-in ReadWriteOnce ReadOnlyMany ReadWriteMany
HostPath Yes No No
iSCSI Yes Yes No
LocalVolume Yes No No
NFS Yes Yes Yes
VMware vSphere Yes No No
Note
OpenShift can only mount a persistent volume with one access mode at a time.
For example, OpenShift can mount an iSCSI volume as either ReadWriteOnce or
ReadOnlyMany, but OpenShift cannot simultaneously mount a given iSCSI volume
as ReadWriteOnce on one node, and ReadOnlyMany on other nodes.
Managing Shared Storage Resources
OpenShift uses storage classes to manage different types of storage, including shared storage.
Each storage class defines a provisioner attribute. A provisioner automates many persistent
volume life cycle tasks, namely creating a persistent volume on demand.
OpenShift contains several internal provisioners. OpenShift does not, however, provide an internal
provisioner for each volume plug-in.
Internal Provisioner support
Volume Plug-in Internal Provisioner Storage Class Provisioner
Value
AWS EBS Yes kubernetes.io/aws-ebs
Azure File Yes kubernetes.io/azure-file
Azure Disk Yes kubernetes.io/azure-disk
Cinder Yes kubernetes.io/cinder
Fibre Channel No
GCE Persistent Disk Yes kubernetes.io/gce-pd
HostPath No
iSCSI No
LocalVolume No
NFS No
238 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
Volume Plug-in Internal Provisioner Storage Class Provisioner
Value
VMware vSphere Yes kubernetes.io/vspherevolume
Note
If you create a storage class for a volume plug-in that does not have a
corresponding provisioner, use a storage class provisioner value of
kubernetes.io/no-provisioner.
OpenShift contains two built-in volume plug-ins for shared storage: NFS and Azure File.
OpenShift has built-in support to provision Azure File persistent volumes dynamically, but not NFS
persistent volumes.
Additional shared storage options are available in OpenShift. You must install additional
components in the cluster.
Managing Azure File Persistent Volumes
OpenShift has a built-in provisioner for Azure File persistent volumes. To use Azure File volumes in
OpenShift, first create a secret that contains credentials for your Azure Storage account. Second,
create a storage class for each Azure File tier you require for the OpenShift cluster.
Each storage class definition contains a reference to the Azure storage credentials secret. An
example Azure File storage class definition follows:
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
name: azurefile-lrs
provisioner: kubernetes.io/azure-file
parameters:
skuName: Standard_LRS
storageAccount: azure_storage_account_name
secretNamespace: my-secret-namespace
secretName: azure-storage-credentials
The name of the storage class. To request this type of storage, a persistent volume claim uses
a StorageClassName value of azurefile-lrs.
A provisioner value of kubernetes.io/azure-file instructs OpenShift to use the
Azure Files provisioner to create persistent volumes. The Azure File provisioner uses values in
the parameters section to create persistent volumes with specific characteristics.
The skuName parameter specifies an Azure File storage tier. The Standard_LRS value
corresponds to standard locally-redundant storage (LRS).
The provisioner requires an account name to authenticate to the Azure Files API.
The provisioner requires sensitive credentials to authenticate to the Azure Files API. The
provisioner retrieves sensitive credentials from a secret resource, and then authenticates to
the Azure Files API.
OpenShift then uses the Azure File provisioner to manage all Azure File persistent volumes in the
cluster.
DO380-OCP4.4-en-2-20200803 239
Chapter 8 | Configuring Persistent Storage
Managing NFS Persistent Volumes
NFS is another option for shared file storage in OpenShift. OpenShift does not, however, have a
built-in NFS provisioner.
You have two options to manage NFS persistent volumes:
1. Create and use playbooks (or other automation) to provision a pool of static NFS
persistent volumes. Write playbooks to create a set of NFS storage classes and persistent
volumes. Ensure each persistent volume defines a storage class, and also configures NFS
mount options according to that storage class.
If the cluster requires additional NFS persistent volumes, then re-run the playbook with updated
parameters. 2. Add external components, such as an operator or custom provisioner, to
automate the management of NFS persistent volumes. Many third party and open source
solutions exist to help with the management of NFS shares.
Red Hat Container Storage uses operators to manage the dynamic provisioning of NFS persistent
volumes.
Note
The classroom environment uses an external, open source NFS provisioner. The
provisioner dynamically creates NFS persistent volumes from an existing NFS
server. See the References for more information.
Red Hat does not recommend using this provisioner in production environments.
Providing Other Shared Storage Resources
Red Hat works with partners and the open source community to provide other shared storage
solutions. Use the OpenShift marketplace to install other storage operators that can manage
shared persistent volumes resources. == Setting a Default Storage Class To simplify storage
requests, OpenShift allows users to create a PVC that does not specify a storage class. With
this feature, users do not need to know the various storage tiers that are available in a particular
OpenShift cluster.
If a PVC does not specify a storage class, then OpenShift tries to match the PVC to a persistent
volume from the default storage class. You must designate an appropriate default storage class
for the OpenShift cluster. Use a default storage class that is scalable, cost-effective, and easy to
manage.
The default storage class resource for a cluster contains a specific metadata annotation:
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
annotations:
storageclass.kubernetes.io/is-default-class: "true"
...output omitted...
The default storage class must define a storageclass.kubernetes.io/is-defaultclass
annotation, which has a value of "true".
Use the oc annotate command to create or update an annotation. As an example, the following
command sets the standard storage class as the default storage class:
240 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
[user@demo ~]$ oc annotate storageclass standard \
> --overwrite "storageclass.kubernetes.io/is-default-class=true"
You can also store the annotation YAML snippet in a file and use the content of the file to patch
the storage class resource:
[user@demo ~]$ cat set_default.yml
metadata:
annotations:
storageclass.kubernetes.io/is-default-class: "true"
[user@demo ~]$ oc patch storageclass standard -p "$(cat set_default.yml)"
OpenShift labels the default storage class in the output of the oc get storageclass
command:
[user@demo ~]$ oc get storageclass
NAME PROVISIONER RECLAIMPOLICY ...
standard (default) nfs-storage Delete ...
gold azure-file-storage Delete ...
You must define only one default storage class. If more than one default exists, then OpenShift
does not provision a persistent volume for a PVC that does not specify a storage class.
Restricting Access to Storage Resources
Because all storage resources are not the same, you might need to implement restrictions on the
use of storage resources in the cluster. Use a resource quota to restrict resource consumption per
project. Quotas are often used to restrict CPU and memory usage in a project, but you can also
use quotas to restrict storage resource usage.
For example, consider a hypothetical cluster that provides both a standard and a fast storage
class. The standard storage class provides inexpensive yet scalable storage, with average I/O
performance. The fast storage class provides low latency and high throughput storage, but it is
expensive. To encourage users to consume standard storage resources, you must use a resource
quota to restrict the use of the fast storage resources.
Storage Resources Managed by Resource Quotas
Resource Name Description
requests.storage The sum of storage space requests across all
persistent volume claims can not exceed this
value.
persistentvolumeclaims The total number of persistent volume claim
resources that can exist in a project.
<storage-classname>.
storageclass.storage.k8s.io/
requests.storage
The sum of storage space requests for the
across all persistent volume claims, but only
for the <storage-class-name> storage
class.
DO380-OCP4.4-en-2-20200803 241
Chapter 8 | Configuring Persistent Storage
Resource Name Description
<storage-classname>.
storageclass.storage.k8s.io/
persistentvolumeclaims
The total number of persistent volume claim
resources for the <storage-class-name>
storage class that can exist in a project.
An example resource quota follows:
apiVersion: v1
kind: ResourceQuota
metadata:
name: storage
namespace: test
spec:
hard:
persistentvolumeclaims: 4
requests.storage: 100G
A 'storage' resource quota that restricts resources for the test namespace (project).
The quota restricts the project to four persistent volume claims.
The quota restricts the project to 100 GB of total persistent storage.
You can also use the oc create quota command to create a resource quota from the command
line.
242 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
References
For more information about Persistent Volume access modes, refer to the Access
Modes section in the Understanding Persistent Storage chapter in the Red Hat
OpenShift Container Platform 4.4 Storage documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/storage/index#pv-accessmodes_
understanding-persistent-storage
For more information about Azure File persistent storage, refer to the Persistent
storage using Azure File section in the Configuring persistent storage chapter in the
Red Hat OpenShift Container Platform 4.4 Storage documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/storage/configuring-persistentstorage#
persistent-storage-using-azure-file
For more information about NFS persistent storage, refer to the Persistent storage
using NFS section in the Configuring persistent storage chapter in the Red Hat
OpenShift Container Platform 4.4 Storage documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/storage/configuring-persistentstorage#
persistent-storage-using-nfs
Kubernetes NFS-Client Provisioner
https://github.com/kubernetes-incubator/external-storage/tree/nfs-clientprovisioner-
v3.1.0-k8s1.11/nfs-client
For more information about resource quotas, refer to the Resource Quotas
per Project section in the Quotas chapter in the Red Hat OpenShift Container
Platform 4.4 Applications documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/applications/index#quotas-settingper-
project
DO380-OCP4.4-en-2-20200803 243
Chapter 8 | Configuring Persistent Storage
Guided Exercise
Provisioning Shared Storage for
Applications
In this exercise you will deploy a backend batch processing job to access shared file storage.
Outcomes
You should be able to:
• Identify the default storage class for an OpenShift cluster.
• Set the default storage class for an OpenShift cluster.
• Create a storage quota for a project.
• Create a Persistent Volume claim for shared storage.
Before You Begin
To perform this exercise, ensure you have access to:
• A running OpenShift cluster.
On the workstation machine, use the lab command to prepare your system for this
exercise.
The command ensures that the cluster API is reachable and that no default storage class
exists.
[student@workstation ~]$ lab storage-file start
Instructions
1. As the admin user, create a storage-file project with a restriction of three persistent
volume claims and 5 GB of storage. Add the developer user as a project administrator.
1.1. Log in in as the admin user and create the storage-file project.
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
[student@workstation ~]$ oc new-project storage-file
Now using project "storage-file" on server "https://api.ocp4.example.com:6443".
...output omitted...
1.2. Create a storage quota for the project. Restrict the project to 3 persistent volume
claims and 5 GB of storage.
244 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
[student@workstation ~]$ oc create quota storage \
> --hard=requests.storage=5G,persistentvolumeclaims=3
resourcequota/storage created
1.3. Add the developer user as a project administrator.
[student@workstation ~]$ oc policy add-role-to-user \
> admin developer
clusterrole.rbac.authorization.k8s.io/admin added: "developer"
Note
If you have not previously authenticated as the developer user, then you will see a
warning message that indicates the developer user is not found.
OpenShift creates a developer user resource only after the developer user
authenticates for the first time.
2. The developer user manages an application that requires shared storage. The application
consists of three components: a HTTPD front end, a back-end image processing
application, and a Redis service to manage a work queue for the back-end application. The
~/DO380/labs/storage-file directory contains resource definitions to deploy these
components.
The front-end application defines a persistent volume claim for shared storage in the
resources/httpd/pvc.yml file. Create a photo-share persistent volume claim from
the resources/httpd/pvc.yml file. Expect the photo-share persistent volume claim
to have a Pending status.
2.1. Change to the ~/DO380/labs/storage-file directory.
[student@workstation ~]$ cd ~/DO380/labs/storage-file
[student@workstation storage-file]$
2.2. Log in as the developer user and switch to the storage-file project.
[student@workstation storage-file]$ oc login -u developer -p developer
Login successful.
...output omitted...
[student@workstation storage-file]$ oc project storage-file
Already on project "storage-file" on server "https://api.ocp4.example.com:6443".
2.3. Display and review the contents of the resources/httpd/pvc.yml file.
[student@workstation storage-file]$ cat resources/httpd/pvc.yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: photo-share
spec:
accessModes:
DO380-OCP4.4-en-2-20200803 245
Chapter 8 | Configuring Persistent Storage
- ReadWriteMany
resources:
requests:
storage: 3Gi
2.4. Use the resources/httpd/pvc.yml file to create the photo-share persistent
volume claim.
[student@workstation storage-file]$ oc apply -f resources/httpd/pvc.yml
persistentvolumeclaim/photo-share created
2.5. Display the status of all persistent volume claims in the project.
[student@workstation storage-file]$ oc get pvc
NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS ...
photo-share Pending ...
2.6. Use the oc describe command to display events for the photo-share persistent
volume claim.
[student@workstation storage-file]$ oc describe pvc photo-share
...output omitted...
Events:
Type Reason ... Message
---- ------ ... -------
Normal FailedBinding ... no persistent volumes available for this claim and
no storage class is set
The resources/httpd/pvc.yml file does not declare a storage class, and a
default storage class is not defined for the cluster.
3. Edit the resources/httpd/pvc.yml file to specify shared file storage from the cluster
NFS storage provider. After you edit the file, reprovision the photo-share persistent
volume claim.
3.1. Display the list of available storage classes.
[student@workstation storage-file]$ oc get storageclass
NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ...
nfs-storage nfs-storage-provisioner Delete Immediate ...
3.2. Use your preferred editor to modify the resources/httpd/pvc.yml file. Specify
the nfs-storage storage class for the photo-share peristent volume:
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: photo-share
spec:
accessModes:
- ReadWriteMany
storageClassName: nfs-storage
246 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
resources:
requests:
storage: 3Gi
Save the changes.
3.3. Delete the existing photo-share persistent volume claim. Next, use the modified
resources/httpd/pvc.yml file to re-create the photo-share persistent volume
claim.
[student@workstation storage-file]$ oc delete pvc photo-share
persistentvolumeclaim "photo-share" deleted
[student@workstation storage-file]$ oc apply -f resources/httpd/pvc.yml
persistentvolumeclaim/photo-share created
3.4. Verify that an NFS persistent volume binds to the photo-share persistent volume
claim.
[student@workstation storage-file]$ oc get pvc
NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE
photo-share Bound pvc-e571... 3Gi RWX nfs-storage 3s
4. Deploy the front-end HTTPD application and Redis service. Execute the
simulate_activity.sh script to simulate users uploading photos from the front-end
application. After the simulation, one hundred photos are present on the NFS persistent
volume.
4.1. The resources/httpd directory contains resource definitions for the front-end
application. Use the oc apply -k command to deploy the front-end application.
[student@workstation storage-file]$ oc apply -k resources/httpd/
service/httpd created
deployment.apps/httpd created
route.route.openshift.io/httpd created
persistentvolumeclaim/photo-share configured
The front-end application exposes a route. If you navigate to http://httpdstorage-
file.apps.ocp4.example.com/data, then you see that no images
exist in the data directory.
4.2. The resources/redis directory contains resource definitions for the Redis service,
which hosts a work queue for the back-end application. Use the oc apply -k
command to deploy the Redis service.
[student@workstation storage-file]$ oc apply -k resources/redis/
service/redis created
deployment.apps/redis created
4.3. Wait for the Redis pod to become ready.
DO380-OCP4.4-en-2-20200803 247
Chapter 8 | Configuring Persistent Storage
[student@workstation storage-file]$ oc get pods
NAME READY STATUS RESTARTS AGE
httpd-5c57dcd69f-vnl7p 1/1 Running 0 2m
redis-59d8d5c4bb-xcsgn 1/1 Running 0 115s
Note
If you execute the show_queue.sh script after the Redis service is ready, then the
output indicates that the processing queue is an empty array.
4.4. Execute the ./simulate_activity.sh script to simulate users uploading photos
from the front-end application. The script uploads 100 photos to the uploads
subdirectory on the shared volume.
[student@workstation storage-file]$ ./simulate_activity.sh
HTTP pod: httpd-5c57dcd69f-vnl7p
Redis pod: redis-59d8d5c4bb-xcsgn
df2212f6e1ad2862.jpg
df8a8403e4545ab5.jpg
...output omitted...
97
98
99
100
If you navigate to http://httpd-storage-file.apps.ocp4.example.com/
data in a browser, then a uploads subdirectory is present with 100 images. If you
execute the show_queue.sh script, then the same set of image file names is present
in the processing queue.
5. Deploy the back-end image processing job to detect objects in uploaded photos. Verify
that all three back-end worker pods are able to process files from the shared persistent
volume.
5.1. Use the oc apply -k command to deploy the resources in the resources/
backend subdirectory. Wait until the worker pods have a Running status. The pods
may be in a ContainerCreating status for a couple minutes while the application
container image dowloads to each node from the registry.
[student@workstation storage-file]$ oc apply -k resources/backend/
job.batch/worker created
[student@workstation storage-file]$ oc get pods
NAME READY STATUS RESTARTS AGE
httpd-5c57dcd69f-dh6ms 1/1 Running 0 39m
redis-59d8d5c4bb-zrtcz 1/1 Running 0 38m
redis-client 1/1 Running 0 10m
worker-4kv28 1/1 Running 0 59s
worker-h7z65 1/1 Running 0 58s
worker-v6n8h 1/1 Running 0 59s
5.2. The job completes approximately two minutes after you start the job. Wait for the job
to finish.
248 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
[student@workstation storage-file]$ oc get pods
NAME READY STATUS RESTARTS AGE
httpd-5c57dcd69f-dh6ms 1/1 Running 0 41m
redis-59d8d5c4bb-zrtcz 1/1 Running 0 40m
redis-client 1/1 Running 0 12m
worker-4kv28 0/1 Completed 0 3m5s
worker-h7z65 0/1 Completed 0 3m4s
worker-v6n8h 0/1 Completed 0 3m5s
5.3. Review the logs of each worker pod to verify that each worker pod processes files
from the shared storage volume.
[student@workstation storage-file]$ oc logs worker-4kv28
...output omitted...
Working on /data/uploads/e0d3452ed12d6c68.jpg
Working on /data/uploads/e0581d87281bb10a.jpg
Working on /data/uploads/df8a8403e4545ab5.jpg
Waiting for work
Queue empty, exiting
[student@workstation storage-file]$ oc logs worker-4kv28 | grep "Working on" \
> | wc -l
50
[student@workstation storage-file]$ oc logs worker-h7z65 | grep "Working on" \
> | wc -l
25
[student@workstation storage-file]$ oc logs worker-v6n8h | grep "Working on" \
> | wc -l
25
The three worker pods are able to access shared file storage to process all one
hundred image files.
6. As the admin user, set the nfs-storage storage class as the default storage class. When
you set a default storage class, cluster users do not need to specify a storage class to
create a persistent volume claim.
6.1. Log in as the admin user.
[student@workstation storage-file]$ oc login -u admin -p redhat
Login successful.
...output omitted...
6.2. Review the nfs-storage storage class resource definition. Verify that the nfsstorage
storage class is not the default storage class.
DO380-OCP4.4-en-2-20200803 249
Chapter 8 | Configuring Persistent Storage
[student@workstation storage-file]$ oc get storageclass nfs-storage -o yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
annotations:
storageclass.kubernetes.io/is-default-class: "false"
creationTimestamp: "2020-05-28T19:33:31Z"
...output omitted...
The storageclass.kubernetes.io/is-default-class annotation is set to a
value of false.
6.3. Display and review the contents of the set-default-storageclass.yml patch
file.
[student@workstation storage-file]$ cat set-default-storageclass.yml
metadata:
annotations:
storageclass.kubernetes.io/is-default-class: "true"
6.4. Use the contents of the set-default-storageclass.yml patch file to set the
nfs-storage storage class as the default storage class.
[student@workstation storage-file]$ oc patch storageclass nfs-storage \
> -p "$(cat set-default-storageclass.yml)"
storageclass.storage.k8s.io/nfs-storage patched
6.5. Verify that the nfs-storage storage class is the default storage class for the cluster.
Also verify that only one storage class is labeled as the default storage class.
[student@workstation storage-file]$ oc get storageclass
NAME PROVISIONER RECLAIMPOLICY ...
nfs-storage (default) nfs-storage-provisioner Delete ...
The nfs-storage storage class is now the default storage class for the cluster.
7. Change to the /home/student/ directory.
[student@workstation storage-file]$ cd
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab storage-file finish
This concludes the section.
250 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
Provisioning Block Storage for Databases
Objectives
After completing this section, you should be able to configure a database application with block
storage.
Overview of Block Storage
Block storage refers to any storage technology that stores data in fixed-size blocks. Common
examples include hard disk drives (HHDs) and solid state drives (SSDs).
Block storage is not restricted to physically attached hardware. A Storage Area Network (SAN)
presents storage to networked systems as locally attached block devices. Network protocols, such
as iSCSI and Fibre Channel, provide block-level access to storage devices.
Most cloud providers also have scalable, block storage solutions. Examples include Amazon Elastic
Block Store (EBS), Google Persistent Disk, and Azure Disks.
Describing Block Storage Use Cases
Block storage technologies are designed for speed and efficiency. The storage system writes
a data record to multiple blocks. When a data record is needed, the storage system retrieves
multiple blocks at the same time. The blocks are reassembled and presented to the requesting
user as a complete data record.
Databases and transactional systems are examples of applications that require high throughput
and I/O performance. Many of these system use block storage technologies because of the
increased I/O performance requirements.
There are a few drawbacks to block storage:
• Block storage is not a shared read-write storage technology. Only one host at a time can write
data.
• Block storage can be expensive.
• Block storage does not provide robust metadata about stored data. Often you add a file
system to a block device to provide additional metadata for each record, such as a file name or
permissions.
Accessing Block Storage
In many application scenarios, a file system is installed on a block storage device. Applications use
the file system to read, write, and organize data on the block device. Without a file system, the
application is responsible for organizing and writing data to the block device.
In select scenarios, applications require direct access to the block device. With direct access,
applications bypass file system overhead to achieve improved I/O performance. These
applications must implement custom I/O capabilities, which are tuned for increased performance.
As an example, some database technologies configure raw block devices rather than specifying a
database data directory.
DO380-OCP4.4-en-2-20200803 251
Chapter 8 | Configuring Persistent Storage
Kubernetes and OpenShift support both block and file system persistent volumes for block
storage technologies. Use the volumeMode attribute in a persistent volume specification to
indicate the type of volume. Allowable values for the volume mode are: Filesystem and Block.
Block Volumes
As an example, the following defines a fibre channel block volume:
apiVersion: v1
kind: PersistentVolume
metadata:
name: fc-block-pv-01
spec:
accessModes:
- ReadWriteOnce
capacity:
storage: 10Gi
storageClassName: fc-block
volumeMode: Block
fc:
targetWWNs: ["500...fd1"]
lun: 0
readOnly: false
The name of the persistent volume.
The storage class name is fc-block, which indicates a fibre channel (fc) block volume.
OpenShift uses the fibre channel volume plug-in to mount the volume on a node.
To request a block volume, an application developer must create a PVC that declares a Block
volume mode. The following example defines a request for a block volume.
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: block-pvc
spec:
accessModes:
- ReadWriteOnce
volumeMode: Block
resources:
requests:
storage: 10Gi
storageClassName: fc-block
The volume mode must be set to a value of Block. If the PVC definition does not include a
volume mode, the default value is Filesystem.
The persistent volume must come from the fc-block storage class. If you do not specify a
storage class, then OpenShift uses the default storage class, which might not support block
volumes.
To bind to a block volume, both the PVC and PV must explicitly define a Block volume mode. The
table that follows outlines all possible binding scenarios for PV and PVC volume mode values.
252 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
Binding scenarios by Volume Mode
PVC Volume Mode PV Volume Mode Result
Unspecified Block Does not bind.
Filesystem Block Does not bind.
Block Block The PV binds to the PVC.
Unspecified Unspecified The PV binds to the PVC, but
the PV is not a block volume.
Unspecified values default to
Filesystem.
If an application requires access to a raw block device, rather than a filesystem, you must modify
the pod specification. The pod specification that follows attaches a raw block device at the /dev/
xvda device path.
apiVersion: v1
kind: Pod
metadata:
name: pod-with-block-volume
spec:
containers:
- name: fc-container
image: fedora:26
...output omitted...
volumeDevices:
- name: data
devicePath: /dev/xvda
volumes:
- name: data
persistentVolumeClaim:
claimName: block-pvc
Use volumeDevices to attach block volumes as a raw block device.
Attach the data volume as a block device at the /dev/xvda device path.
The data volume corresponds to persistent volume bound to the block-pvc persistent
volume claim.
File System Volumes for Block Storage
Many applications require access to a file system, not a raw block device. For this reason,
Kubernetes and OpenShift use a default volume mode of Filesystem for PVs and PVCs.
Both shared file storage and block storage provide file system volumes. A file system volume,
which is based on block storage, often provides better I/O performance than volumes based on
shared file storage. To request block storage, an application developer must specify a storage
class that corresponds to block storage. If a storage class is omitted, then OpenShift uses the
default storage class, which might not correspond to a block storage technology.
DO380-OCP4.4-en-2-20200803 253
Chapter 8 | Configuring Persistent Storage
Note
In this course, all example applications request file system storage.
Sharing Block Storage
Unlike shared file storage, block storage technologies only attach to a single host at a time. Some
block storage technologies can attach to multiple hosts at the same time with read-only access.
However, block storage technologies restrict write access to a single host to ensure data integrity.
All block storage in Kubernetes and Openshift has a ReadWriteOnce access mode. Some
technologies, such as iSCSI, also support ReadOnlyMany access mode.
If an application, such as a database, requires write access to block storage, then the persistent
volume claim must request an access mode of ReadWriteOnce. Because the volume only
attaches to one node at a time, you can safely use a volume with a single pod.
To share a block storage volume with a different application, you must stop the current application
that is using the volume. For example, to restore a database from a backup, you must first scale
down the database to zero pods. A restoration application can mount the database volume after
the database is scaled to zero.
Before the database application is restarted, the restoration application must finish and release
the database volume.
Restricting Allocation of Block Storage Resources
Block storage resources are often expensive, and as a result, a limited cluster resource. You might
want to restrict the use of block storage resources on a project by project basis. In this way,
you ensure the availability of limited storage resources to those projects with unique storage
requirements.
Use resource quotas for each project to restrict the use particular storage resources. The following
resource quota definition restricts the database project to a single block storage persistent
volume.
apiVersion: v1
kind: ResourceQuota
metadata:
name: storage
namespace: test
spec:
hard:
fc-block.storageclass.storage.k8s.io/persistentvolumeclaims: 1
fc-block.storageclass.storage.k8s.io/requests.storage: 100G
The storage resource quota restricts storage resources for the test project.
The test project is restricted to one persistent volume claim for the fc-block storage
class.
The test project is restricted to 100 GB of storage from the fc-block storage class.
You can also restrict access to storage class resources entirely.
254 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
apiVersion: v1
kind: ResourceQuota
metadata:
namespace: dev
spec:
hard:
fc-block.storageclass.storage.k8s.io/persistentvolumeclaims: 0
The dev project can not create a persistent volume claim that requests storage for the fcblock
storage class.
References
For more information about block volumes, refer to the Block Volume Support
section in the Understanding Persistent Storage chapter in the Red Hat Openshift
Container Platform 4.4 Storage documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/storage/index#block-volumesupport_
understanding-persistent-storage
For more information about persistent iSCSI storage, refer to the Persistent Storage
Using iSCSI section in the Configuring persistent storage chapter in the Red Hat
OpenShift Container Platform 4.4 Storage documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/storage/configuring-persistentstorage#
persistent-storage-using-iscsi
DO380-OCP4.4-en-2-20200803 255
Chapter 8 | Configuring Persistent Storage
Guided Exercise
Provisioning Block Storage for Databases
In this exercise you will create block storage resources for the cluster and deploy a database
that uses block storage.
Outcomes
You should be able to:
• Create a storage class to make block storage resources visible to non-privileged cluster
users.
• Use a storage class in a persistent volume claim to request block storage.
• Create a storage quota that restricts storage resources by storage class.
Before You Begin
To perform this exercise, ensure you have access to:
• A running OpenShift cluster.
On the workstation machine, use the lab command to prepare your system for this
exercise.
The command ensures that the cluster API is reachable.
[student@workstation ~]$ lab storage-block start
Instructions
1. The ~/DO380/labs/storage-block/iscsi directory contains playbooks to configure
iSCSI block storage resources for the cluster. Review and execute the site.yml playbook.
1.1. Change to the ~/DO380/labs/storage-block/iscsi directory and display the
site.yml file.
[student@workstation ~]$ cd ~/DO380/labs/storage-block/iscsi
[student@workstation iscsi]$ cat site.yml
- name: Install iSCSI packages
import_playbook: install-iscsi.yml
- name: Configure a logical volume for each PV
import_playbook: configure-lvm.yml
- name: Configure each logical volume as an iSCSI LUN
import_playbook: configure-iscsi.yml
- name: Create a PV resource file for each LUN
import_playbook: render-templates.yml
256 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
The site.yml file executes four other playbooks:
• The install-iscsi.yml playbook ensures that iSCSI packages are installed on
the iSCSI server.
• The configure-lvm.yml playbook configures Logical Volume Management
on the iSCSI server. Each persistent volume resource corresponds to one logical
volume on the iSCSI server. The playbook creates the logical volumes in the
iSCSI_vg volume group.
• The configure-iscsi.yml playbook configures each logical volume in the
iSCSI_vg volume group as an iSCSI LUN.
• The render-templates.yml playbook creates a persistent volume resource file
in the ~/DO380/labs/storage-block/PVs directory for each iSCSI LUN. The
playbook also creates a storageclass.yml file and a kustomization.yml file
in the same directory.
1.2. Execute the site.yml playbook.
[student@workstation iscsi]$ ansible-playbook site.yml
...output omitted...
TASK [Render PV YAML files] ****************************************************
changed: [workstation] => (item=1G)
changed: [workstation] => (item=2G)
...output omitted...
TASK [Render kustomization.yml] ************************************************
changed: [workstation] => (item=kustomization.yml)
changed: [workstation] => (item=storageclass.yml)
PLAY RECAP *********************************************************************
utility.lab.example.com : ok=14 changed=11 ... rescued=1 ignored=0
workstation : ok=3 changed=3 ... rescued=0 ignored=0
2. The render-templates.yml playbook creates a resource file for each persistent volume
in the ~/DO380/labs/storage-block/PVs directory. Review one of the persistent
volume resource files. Use the oc apply -k command to create persistent volumes from
the resource files. Verify that iSCSI persistent volumes exist in the cluster.
2.1. Change to the ~/DO380/labs/storage-block/PVs directory. List the files in the
directory.
[student@workstation iscsi]$ cd ~/DO380/labs/storage-block/PVs
[student@workstation PVs]$ ls
iscsi_pv_00.yml iscsi_pv_03.yml iscsi_pv_06.yml kustomization.yml
iscsi_pv_01.yml iscsi_pv_04.yml iscsi_pv_07.yml storageclass.yml
iscsi_pv_02.yml iscsi_pv_05.yml iscsi_pv_08.yml
2.2. Display the content of the iscsi_pv_00.yml file.
DO380-OCP4.4-en-2-20200803 257
Chapter 8 | Configuring Persistent Storage
[student@workstation PVs]$ cat iscsi_pv_00.yml
apiVersion: v1
kind: PersistentVolume
metadata:
name: iscsi-pv-00
spec:
capacity:
storage: 1Gi
volumeMode: Filesystem
storageClassName: iscsi-blk
accessModes:
- ReadWriteOnce
iscsi:
targetPortal: 172.25.250.253:3260
iqn: iqn.2020-06.com.example:utility.lab.example.com
lun: 0
initiatorName: iqn.2020-06.com.example:openshift
fsType: 'ext4'
readOnly: false
The persistent volume name is iscsi-pv-00.
The size of persistent volume is set to equal the size of the corresponding logical
volume on the iSCSI server.
The persistent volume has a storage class label of iscsi-blk.
The persistent volume uses the iSCSI volume plug-in. This section defines
parameters that OpenShift needs to mount an iSCSI volume. Each persistent
volume uses a different iSCSI LUN.
2.3. Log in as the admin user.
[student@workstation PVs]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
2.4. Use the oc apply -k command to create persistent volumes from the resource
definitions in the ~/DO380/labs/storage-block/PVs directory.
[student@workstation PVs]$ oc apply -k ~/DO380/labs/storage-block/PVs
persistentvolume/iscsi-pv-00 created
persistentvolume/iscsi-pv-01 created
persistentvolume/iscsi-pv-02 created
persistentvolume/iscsi-pv-03 created
persistentvolume/iscsi-pv-04 created
persistentvolume/iscsi-pv-05 created
persistentvolume/iscsi-pv-06 created
persistentvolume/iscsi-pv-07 created
persistentvolume/iscsi-pv-08 created
2.5. Verify that nine persistent volumes exist with an iscsi-blk storage class attribute
value.
258 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
[student@workstation PVs]$ oc get pv \
> -o=custom-columns='NAME:metadata.name,STORAGECLASS:spec.storageClassName'
NAME STORAGECLASS
iscsi-pv-00 iscsi-blk
iscsi-pv-01 iscsi-blk
iscsi-pv-02 iscsi-blk
iscsi-pv-03 iscsi-blk
iscsi-pv-04 iscsi-blk
iscsi-pv-05 iscsi-blk
iscsi-pv-06 iscsi-blk
iscsi-pv-07 iscsi-blk
iscsi-pv-08 iscsi-blk
...output omitted...
2.6. Display the list of storage classes.
[student@workstation PVs]$ oc get storageclass
NAME PROVISIONER RECLAIMPOLICY ...
nfs-storage (default) nfs-storage-provisioner Delete ...
An iscsi-blk storage class resource does not exist, yet nine persistent volumes
exist with an iscsi-blk storage class label.
Note
If a persistent volume claim contains a storageClassName value of iscsi-blk,
then OpenShift binds one of the nine iscsi-blk persistent volumes to the claim.
Non-privileged users do not know that iSCSI persistent volumes exist in the cluster,
because they can not inspect persistent volumes.
You did not create an iscsi-blk storage class resource in this step to simulate the
preceding scenario. In a later step, you request iSCSI storage as a non-privileged
user when no iSCSI storage class exists.
Important
You must create an iSCSI storage class resource to ensure that non-privileged
users know that iSCSI storage is available in the cluster. You create the iscsi-blk
storage class resource in a later step.
3. As the admin user, create a storage-block project with a restriction of two persistent
volume claims and 6 GB of storage. Further restrict the project to a single persistent
volume claim for the iscsi-blk storage class. Add the developer user as a project
administrator.
3.1. Create the storage-block project.
DO380-OCP4.4-en-2-20200803 259
Chapter 8 | Configuring Persistent Storage
[student@workstation PVs]$ oc whoami
admin
[student@workstation PVs]$ oc new-project storage-block
Now using project "storage-block" on server "https://api.ocp4.example.com:6443".
...output omitted...
3.2. Create a storage quota resource for the project. Restrict the project to: 2 persistent
volume claims, 6 GB of requested storage, and 1 persistent volume claim for the
iscsi-blk storage class.
[student@workstation PVs]$ limit1="requests.storage=6G"
[student@workstation PVs]$ limit2="persistentvolumeclaims=2"
[student@workstation PVs]$ sclass="iscsi-blk.storageclass.storage.k8s.io"
[student@workstation PVs]$ limit3="${sclass}/persistentvolumeclaims=1"
[student@workstation PVs]$ oc create quota storage \
> --hard=${limit1},${limit2},${limit3}
resourcequota/storage created
3.3. Use the oc describe command to review the storage quota resource.
[student@workstation PVs]$ oc describe quota storage
Name: storage
Namespace: storage-block
Resource Used Hard
-------- ---- ----
iscsi-blk.storageclass.storage.k8s.io/persistentvolumeclaims 0 1
persistentvolumeclaims 0 2
requests.storage 0 6G
The project is restricted to one persistent volume claim for the iscsi-blk
storage class.
The project is restricted to two persistent volume claims, regardless of storage
class.
The project is restricted to 6 GB of requested storage, across all persistent
volume claims.
3.4. Add the developer user as a storage-block project administrator.
[student@workstation PVs]$ oc policy add-role-to-user admin developer
clusterrole.rbac.authorization.k8s.io/admin added: "developer"
Note
If you have not previously authenticated as the developer user, then you see a
warning message that indicates the developer user is not found.
You can safely ignore this message.
4. The developer user manages a set of applications to operate a database. The database
requires block storage, while the backup and restore applications can use shared storage.
260 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
As the developer user, switch to the storage-block project. Review storage resources
that are available to the storage-block project.
4.1. Login as the developer user and ensure you are using the storage-block project.
[student@workstation PVs]$ oc login -u developer -p developer
Login successful.
You have one project on this server: "storage-block"
Using project "storage-block".
4.2. Display a list of persistent volumes.
[student@workstation PVs]$ oc get pv
Error from server (Forbidden): persistentvolumes is forbidden: User "developer"
cannot list resource "persistentvolumes" in API group "" at the cluster scope
Note
Persistent volume resources can contain sensitive information, such as connection
credentials to remote storage. For this reason, non-privileged users do not have
access to persistent volume resources.
4.3. Display the list of storage class resources.
[student@workstation PVs]$ oc get storageclass
NAME PROVISIONER RECLAIMPOLICY ...
nfs-storage (default) nfs-storage-provisioner Delete ...
Only the nfs-storage storage class resource exists. The developer user is not
aware that iSCSI persistent volumes exist in the cluster.
Note
Each of the iSCSI persistent volumes declare a storage class value of iscsi-blk.
OpenShift does not create a storage class resource from persistent volume
resource definitions. As an administrator, you must ensure a storage class resource
exists for any persistent volumes you manually provision.
4.4. As the developer user, use the oc describe command to review the storage
quota resource.
[student@workstation PVs]$ oc describe quota storage
Name: storage
Namespace: storage-block
Resource Used Hard
-------- ---- ----
iscsi-blk.storageclass.storage.k8s.io/persistentvolumeclaims 0 1
persistentvolumeclaims 0 2
requests.storage 0 6G
DO380-OCP4.4-en-2-20200803 261
Chapter 8 | Configuring Persistent Storage
For the developer user, the storage quota provides the only indication that
an iSCSI storage class exists.
5. The ~/DO380/labs/storage-block/test-db directory contains resource definitions
to deploy database management applications.
The postgres subdirectory contains resource definition files to deploy a Postgresql
database. Ensure that the database uses iSCSI block storage, which is defined in the
postgres/statefulset.yml file. Deploy the database.
Inspect the storage quota object, which updates after you deploy the database.
5.1. Change to the ~/DO380/labs/storage-block/test-db subdirectory.
[student@workstation PVs]$ cd ~/DO380/labs/storage-block/test-db
[student@workstation test-db]$
5.2. Edit the volumeClaimTemplates section of the postgres/statefulset.yml
file to match the following:
volumeClaimTemplates:
- metadata:
name: data
spec:
storageClassName: iscsi-blk
accessModes: [ "ReadWriteOnce" ]
resources:
requests:
storage: 2Gi
Save the file.
Note
A solution file is available at: ~/DO380/solutions/storage-block/test-db/
postgres/statefulset.yml.
5.3. Deploy the database.
[student@workstation test-db]$ oc apply -k postgres/
secret/postgresql created
service/postgresql created
statefulset.apps/postgresql created
5.4. Verify that the persistent volume claim binds to one of the iSCSI persistent volumes.
[student@workstation test-db]$ oc get pvc
NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS ...
data-postgresql-0 Bound iscsi-pv-01 2Gi RWO iscsi-blk ...
5.5. Inspect the storage quota resource after deploying the database.
262 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
[student@workstation test-db]$ oc describe quota storage
Name: storage
Namespace: storage-block
Resource Used Hard
-------- ---- ----
iscsi-blk.storageclass.storage.k8s.io/persistentvolumeclaims 1 1
persistentvolumeclaims 1 2
requests.storage 2Gi 6G
The database persistent volume claim increments the persistent volume claim
count for the iscsi-blk storage class.
6. As the admin user, create an iscsi-blk storage class resource.
The ~/DO380/labs/storage-block/PVs/storageclass.yml file contains a
resource definition for the iscsi-blk storage class.
6.1. Change to the ~/DO380/labs/storage-block/PVs directory. Display the
contents of the storageclass.yml file.
[student@workstation test-db]$ cd ~/DO380/labs/storage-block/PVs
[student@workstation PVs]$ cat storageclass.yml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
name: iscsi-blk
provisioner: kubernetes.io/no-provisioner
allowVolumeExpansion: false
The storage class name is iscsi-blk.
The storage class does not provision volumes on demand.
6.2. Log in as the admin user.
[student@workstation PVs]$ oc login -u admin
Logged into "https://api.ocp4.example.com:6443" as "admin" ...
...output omitted...
6.3. Use the storageclass.yml file to create the iscsi-blk storage class.
[student@workstation PVs]$ oc create -f storageclass.yml
storageclass.storage.k8s.io/iscsi-blk created
7. As the developer user, verify that the iscsi-blk storage class displays in the storage
class list.
7.1. Log in as the developer user, and switch to the storage-block project.
DO380-OCP4.4-en-2-20200803 263
Chapter 8 | Configuring Persistent Storage
[student@workstation PVs]$ oc login -u developer
Logged into "https://api.ocp4.example.com:6443" as "developer" ...
...output omitted...
[student@workstation PVs]$ oc project storage-block
...output omitted...
7.2. Display the list of storage classes.
[student@workstation PVs]$ oc get storageclass
NAME PROVISIONER RECLAIMPOLICY ...
iscsi-blk kubernetes.io/no-provisioner Delete ...
nfs-storage (default) nfs-storage-provisioner Delete ...
8. [OPTIONAL] Simulate database operations.
The ~/DO380/labs/storage-block/test-db directory contains subdirectories, each
with resource definitions to deploy a job that implements a specific database operation.
These jobs demonstrate how to share different types of persistent volumes between pods.
In the storage-block project, use the oc apply -k command to:
• Deploy a job to initialize the database. Resources for the initialization job are in the
init_job subdirectory. This job does not mount any persistent volumes.
• Deploy a database backup job. Resources for the backup job are in the backup_job
subdirectory. This job mounts a shared storage persistent volume, different than the
database block storage persistent volume.
• Deploy a database restoration job. Resources for the database restoration job are in the
restore_job subdirectory. This job creates a pod that mounts both the shared storage
and block storage persistent volumes.
8.1. As the developer user, use the oc apply -k command to deploy a database
initialization job. The initialization job resources are in the init_job subdirectory.
[student@workstation PVs]$ cd ~/DO380/labs/storage-block/test-db
[student@workstation test-db]$ oc whoami
developer
[student@workstation test-db]$ oc apply -k init_job/
configmap/db-init created
job.batch/database-init created
8.2. After the initialization job completes, then execute the get_all.sh script to retrieve
all data from the database table.
[student@workstation test-db]$ oc get pods
NAME READY STATUS RESTARTS AGE
database-init-b5sr7 0/1 Completed 0 20s
postgresql-0 1/1 Running 0 18m
[student@workstation test-db]$ ./get_all.sh
You are now connected to database "sampledb" as user "postgres".
id | first_name | last_name | age | email
----+------------+-----------+-----+-------------------
264 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
1 | John | Doe | 99 | jdoe@example.com
2 | Jane | Doe | 99 | jdoe2@example.com
(2 rows)
8.3. Use the oc apply -k command to deploy a database backup job. The backup job
resources are in the backup_job subdirectory.
[student@workstation test-db]$ oc apply -k backup_job/
serviceaccount/backup created
role.rbac.authorization.k8s.io/pod-exec created
rolebinding.rbac.authorization.k8s.io/pod-exec created
configmap/backup-scripts created
job.batch/backup created
persistentvolumeclaim/data-postgresql-backup created
8.4. After the backup job completes, execute the add_new.sh script to add new data to
the database.
[student@workstation test-db]$ oc get pods
NAME READY STATUS RESTARTS AGE
backup-g2d2v 0/1 Completed 0 8s
database-init-b5sr7 0/1 Completed 0 19m
postgresql-0 1/1 Running 0 28m
[student@workstation test-db]$ ./add_new.sh
You are now connected to database "sampledb" as user "postgres".
INSERT 0 1
Because the new data is not present in the backup, the database does not contain
this data when you restore from the backup.
8.5. Execute the get_all.sh script to retrieve all data from the database table.
[student@workstation test-db]$ ./get_all.sh
You are now connected to database "sampledb" as user "postgres".
id | first_name | last_name | age | email
----+------------+-----------+-----+----------------------
1 | John | Doe | 99 | jdoe@example.com
2 | Jane | Doe | 99 | jdoe2@example.com
3 | Anna | Smith | 98 | someuser@example.com
(3 rows)
8.6. Use the oc apply -k command to deploy a database restoration job. The
restoration job resources are in the restore_job subdirectory.
[student@workstation test-db]$ oc apply -k restore_job/
serviceaccount/backup unchanged
role.rbac.authorization.k8s.io/create-jobs created
role.rbac.authorization.k8s.io/scale-statefulsets created
rolebinding.rbac.authorization.k8s.io/create-jobs created
rolebinding.rbac.authorization.k8s.io/scale-statefulsets created
configmap/restore-scripts created
job.batch/restore-controller created
DO380-OCP4.4-en-2-20200803 265
Chapter 8 | Configuring Persistent Storage
8.7. Wait for the restoration job to complete.
Note
The restoration job deploys a restore-controller pod. First, the controller pod
scales the database pod to zero pods, which takes the database offline. After the
database is offline, another pod can mount the database volume.
Second, the controller pod deploys a restore-db pod, which mounts the database
and backup volumes. The restore-db pod then copies the backup archive to the
database volume.
Third, the controller pod deletes the restore-db pod, which allows the database
pod to remount the database volume.
Finally, the controller pod scales the database pod back to one pod. The restoration
job is complete when the restore-controller pod has a Completed status and
the postgresql-0 pod has a Running status.
[student@workstation test-db]$ oc get pods
NAME READY STATUS RESTARTS AGE
backup-g2d2v 0/1 Completed 0 8m14s
database-init-b5sr7 0/1 Completed 0 27m
postgresql-0 1/1 Running 0 33s
restore-controller-527b4 0/1 Completed 0 52s
8.8. Execute the get_all.sh script to retrieve all data from the restored database table.
[student@workstation test-db]$ ./get_all.sh
You are now connected to database "sampledb" as user "postgres".
id | first_name | last_name | age | email
----+------------+-----------+-----+-------------------
1 | John | Doe | 99 | jdoe@example.com
2 | Jane | Doe | 99 | jdoe2@example.com
(2 rows)
The database does not contain any data added after the backup job executes.
9. Change to the /home/student/ directory.
[student@workstation test-db]$ cd
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab storage-block finish
This concludes the section.
266 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
Provisioning Local Block Storage
Objectives
After completing this section, you should be able to install and configure the Local Storage
operator to create local storage volumes for an OpenShift cluster.
Describing Local Volumes
Network-based block storage technologies enable remote access to storage with better I/O
performance than shared file storage. In some scenarios, applications require better performance.
Locally-attached block devices can often achieve better I/O performance than remote block
storage technologies. Kubernetes and OpenShift provide capability to create persistent volumes
from locally-attached block devices. The persistent volume definition that follows mounts a
locally-attached block device.
apiVersion: v1
kind: PersistentVolume
metadata:
name: local-pv-01
spec:
capacity:
storage: 500Gi
accessModes:
- ReadWriteOnce
persistentVolumeReclaimPolicy: Retain
storageClassName: local-storage
local:
path: /mnt/disks/vol1
nodeAffinity:
required:
nodeSelectorTerms:
- matchExpressions:
- key: kubernetes.io/hostname
operator: In
values:
- my-node
The local volume plug-in defines a path attribute, which is the location of the locallymounted
storage.
The /mnt/disks/vol1 path is the mount point for the locally attached block device. You
must ensure the device is formatted and mounted to the node.
A local persistent volume must define node affinity rules that restrict the persistent volume to
particular node. OpenShift uses the node affinity rules to schedule pods to the same node as
the local persistent volume. In this example, the persistent volume corresponds to the block
storage that is mounted on the my-node cluster node.
DO380-OCP4.4-en-2-20200803 267
Chapter 8 | Configuring Persistent Storage
Managing Local Volumes
Local volumes do not support dynamic provisioning. You must find an appropriate management
solution to use local volumes at scale.
In a previous exercise, you used Ansible Playbooks to manage the provisioning of iSCSI block
storage resources for the cluster. You can use a similar methodology to manage local volumes.
Alternatively, you can use the Local Storage Operator to manage the life cycle of local persistent
volumes.
Describing the Local Storage Operator
The Local Storage Operator creates and manages local volumes attached to nodes in the cluster.
The operator owns all LocalVolume custom resources that define configuration for local
volumes. The LocalVolume custom resource definition that follows defines a fast-localstorage
and standard-local-storage storage class.
apiVersion: "local.storage.openshift.io/v1"
kind: "LocalVolume"
metadata:
name: "local-disks"
namespace: "local-storage"
spec:
storageClassDevices:
- storageClassName: "fast-local-storage"
volumeMode: Filesystem
devicePaths:
- /dev/sdb
- storageClassName: "standard-local-storage"
volumeMode: Filesystem
devicePaths:
- /dev/hda
The API specification for LocalVolume resources.
The local-disks resource exists in the local-storage project.
The fast-local-storage storage class, which uses the /dev/sdb block device on each
node.
The standard-local-storage storage class, which uses the /dev/hda block device on
each node.
268 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
References
Kubernetes 1.14: Local Persistent Volumes GA
https://kubernetes.io/blog/2019/04/04/kubernetes-1.14-local-persistent-volumesga/
For more information about the Local Storage Operator, refer to the Persistent
storage using local volumes section in the Configuring persistent storage chapter in
the Red Hat OpenShift Container Platform 4.4 Images documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/storage/index#persistent-storageusing-
local-volume
Volumes
https://kubernetes.io/docs/concepts/storage/volumes/#local
DO380-OCP4.4-en-2-20200803 269
Chapter 8 | Configuring Persistent Storage
Guided Exercise
Installing the Local Storage Operator
In this exercise you will create persistent volumes from locally-attached block storage.
Outcomes
You should be able to
• Install the Local Storage operator.
• Create a LocalVolume custom resource that is managed by the Local Storage Operator.
Before You Begin
To perform this exercise, ensure you have access to:
• A running OpenShift cluster.
• The additional worker nodes (worker04, worker05, and worker06), which are active
and in a Ready state. If the additional worker nodes are not active, then ensure you have
completed Guided Exercise: Adding Worker Nodes.
On the workstation machine, use the lab command to prepare your system for this
exercise.
The command ensures that the cluster API is reachable.
[student@workstation ~]$ lab storage-local start
Instructions
1. Verify that an additional block device is attached to each of the worker04, worker05, and
worker06 nodes.
As the admin user, open a debug shell to one of the worker nodes. Execute the lsblk
command to list the attached block devices for the node.
1.1. Log in as the admin user.
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login Successful.
...output omitted...
1.2. Display the cluster nodes.
[student@workstation ~]$ oc get nodes
NAME STATUS ROLES ...
NAME STATUS ROLES AGE VERSION
270 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
master01 Ready master,worker 30h v1.17.1+3f6f40d
master02 Ready master,worker 30h v1.17.1+3f6f40d
master03 Ready master,worker 30h v1.17.1+3f6f40d
worker01 Ready worker 30h v1.17.1+3f6f40d
worker02 Ready worker 30h v1.17.1+3f6f40d
worker03 Ready worker 30h v1.17.1+3f6f40d
worker04 Ready infra 1m v1.17.1+3f6f40d
worker05 Ready infra 1m v1.17.1+3f6f40d
worker06 Ready infra 1m v1.17.1+3f6f40d
1.3. Use a debug shell to execute the lsblk command on the worker06 node. Verify
that the node contains a 20GB, unpartitioned vdb device.
[student@workstation ~]$ oc debug node/worker06
Starting pod/worker06-debug ...
...output omitted...
sh-4.2# lsblk
...output omitted...
NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
vdb 252:16 0 20G 0 disk
sr0 11:0 1 1G 0 rom
vda 252:0 0 40G 0 disk
|-vda4 252:4 0 39.5G 0 part
|-vda2 252:2 127M 0 part /host/boot/efi
|-vda3 252:3 1M 0 part
`-vda1 252:1 384M 0 part /host/boot
sh-4.2# exit
exit
Removing debug pod ...
[student@workstation ~]$
1.4. Verify that the worker01 node does not contain an additional vdb block device.
[student@workstation ~]$ oc debug node/worker01 -- lsblk
Starting pod/worker01-debug ...
...output omitted...
NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
vda 252:0 0 40G 0 disk
|-vda4 252:4 0 39.5G 0 part
|-vda2 252:2 127M 0 part /host/boot/efi
|-vda3 252:3 1M 0 part
`-vda1 252:1 384M 0 part /host/boot
Removing debug pod ...
[student@workstation ~]$
No vdb device is listed in the output.
2. As the admin user, create a storage-local project.
DO380-OCP4.4-en-2-20200803 271
Chapter 8 | Configuring Persistent Storage
[student@workstation ~]$ oc whoami
admin
[student@workstation ~]$ oc new-project storage-local
Now using project "storage-local" on server "https://api.ocp4.example.com:6443".
...output omitted...
3. To install the Local Storage operator, first create an operator group resource in the
storage-local project.
The ~/DO380/labs/storage-local/resources directory contains an operator group
resource definition file. Review the file and create the operator group resource.
Note
Although you can install the Local Storage Operator from the web console, this
exercise uses resource definition files to install the operator from the command line.
A file-based installation method integrates with version control, GitOps, and other
infrastructure-as-code techniques.
3.1. Change to the ~/DO380/labs/storage-local directory.
[student@workstation ~]$ cd ~/DO380/labs/storage-local
[student@workstation storage-local]$
3.2. Review the resources/operator-group.yml file.
[student@workstation storage-local]$ cat resources/operator-group.yml
---
apiVersion: operators.coreos.com/v1alpha2
kind: OperatorGroup
metadata:
name: local-operator-group
namespace: storage-local
spec:
targetNamespaces:
- storage-local
The file defines a operator group resource that targets the storage-local
namespace. This operator group configures RBAC access for member operators in
the storage-local namespace.
3.3. Create the operator group resource.
[student@workstation storage-local]$ oc create \
> -f resources/operator-group.yml
operatorgroup.operators.coreos.com/local-operator-group created
4. Create the subscription resource for the local storage operator.
You must add missing information to the resources/subscription.yml template
before you create the subscription resources.
272 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
4.1. Review the resources/subscription.yml subscription template file.
[student@workstation storage-local]$ cat resources/subscription.yml
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
name: local-storage-operator
namespace: storage-local
spec:
channel: <CHANNEL_VALUE>
installPlanApproval: Automatic
name: <PACKAGE_MANIFEST_NAME>
source: <SOURCE>
sourceNamespace: <SOURCE_NAMESPACE>
After you create the resource, the subscription name is local-storageoperator
and it exists in the storage-local namespace.
Before you create the resource, you must replace <CHANNEL_VALUE> with the
channel value for the local storage operator.
Before you create the resource, you must replace <PACKAGE_MANIFEST_NAME>
with the name of the package manifest for the local storage operator.
Before you create the resource, you must replace <SOURCE> and
<SOURCE_NAMESPACE> with the correct values, which are provided in a later
step.
4.2. Display all storage-related package manifests.
[student@workstation storage-local]$ oc get packagemanifest | grep storage
...output omitted...
local-storage-operator Red Hat Operators 31h
...output omitted...
The package manifest name for the local storage operator is local-storageoperator.
In a later step, you replace <PACKAGE_MANIFEST_NAME> in the
subscription template with local-storage-operator.
4.3. Find the source values that you need for the subscription resource template.
Search for Source in the output of the oc describe packagemanifest
command.
[student@workstation storage-local]$ oc describe packagemanifests \
> local-storage-operator | grep Source
Catalog Source: redhat-operators
Catalog Source Display Name: Red Hat Operators
Catalog Source Namespace: openshift-marketplace
Catalog Source Publisher: Red Hat
In a later step, you replace <SOURCE> in the subscription template with redhatoperators.
In a later step, you replace <SOURCE_NAMESPACE> in the subscription template
with openshift-marketplace.
4.4. Find the channel value that you need for the subscription resource template.
DO380-OCP4.4-en-2-20200803 273
Chapter 8 | Configuring Persistent Storage
Search for Channel in the output of the oc describe packagemanifest
command.
[student@workstation storage-local]$ oc describe packagemanifests \
> local-storage-operator | grep Channel
Channels:
Default Channel: 4.4
In a later step, you replace <CHANNEL> in the subscription template with 4.4.
4.5. Modify the resources/subscription.yml template file to match the correct
values for the local storage operator. Then save the file.
Ensure the content of the resources/subscription.yml file matches the
following:
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
name: local-storage-operator
namespace: storage-local
spec:
channel: "4.4"
installPlanApproval: Automatic
name: local-storage-operator
source: redhat-operators
sourceNamespace: openshift-marketplace
You must enclose the 4.4 channel value in quotes. Otherwise, the channel value
is interpreted as a floating point number.
The package manifest name, which is local-storage-operator.
The source values for the subscription.
Note
A solution file is available at ~/DO380/solutions/storage-local/
resources/subscription.yml.
4.6. Use the resources/subscription.yml file to create a subscription resource for
the local storage operator.
[student@workstation storage-local]$ oc apply -f resources/subscription.yml
subscription.operators.coreos.com/local-storage-operator created
5. Verify that local storage operator resources are deployed.
5.1. List all the pods.
[student@workstation storage-local]$ oc get pods
NAME READY STATUS RESTARTS AGE
local-storage-operator-75d7454577-s475q 1/1 Running 0 32s
274 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
5.2. List the operator group resources.
[student@workstation storage-local]$ oc get operatorgroup
NAME AGE
local-operator-group 10m
5.3. List the subscription resources.
[student@workstation storage-local]$ oc get subs
NAME PACKAGE SOURCE CHANNEL
local-storage-operator local-storage-operator redhat-operators 4.4
5.4. List the cluster service version resource for the local storage operator.
[student@workstation storage-local]$ oc get csv
NAME DISPLAY ... PHASE
`local-storage-operator.4.4.0-2020...` Local Storage ... Succeeded
Note
The CSV resource name in your output may differ from the preceding output.
The CSV resource name matches the pattern: local-storageoperator-
4.4.z-<DATE STRING>.
5.5. For easy access to the CSV name in later steps, create a CSV_NAME variable with the
value of the local storage operator CSV resource name.
[student@workstation storage-local]$ oc get csv -o name
clusterserviceversion.operators.coreos.com/local-storage-operator.4.4.0-2020...
[student@workstation storage-local]$ CSV_NAME=$(oc get csv -o name)
[student@workstation storage-local]$ echo ${CSV_NAME}
clusterserviceversion.operators.coreos.com/local-storage-operator.4.4.0-2020...
6. Display the list of custom resource definition types that the operator owns.
Verify that no operator-owned custom resources exist in the project.
6.1. A CSV resource specification defines a customresourcedefinitions.owned
attribute. This attribute defines a list of custom resource types that the operator
owns.
Use a jsonpath expression to display the kind attribute for all operator-owned
resource types.
[student@workstation storage-local]$ oc get $CSV_NAME -o \
> jsonpath='{.spec.customresourcedefinitions.owned[*].kind}{"\n"}'
LocalVolume
[student@workstation storage-local]$
6.2. Verify that no LocalVolume custom resources exist.
DO380-OCP4.4-en-2-20200803 275
Chapter 8 | Configuring Persistent Storage
[student@workstation storage-local]$ oc get LocalVolume
No resources found in storage-local namespace.
[student@workstation storage-local]$ oc get localvolume
No resources found in storage-local namespace.
Note
The oc get command is case-insenstive.
7. Create a LocalVolume custom resource for the operator.
Ensure the operator creates a persistent volume for the /dev/vdb block device on each
node. Ensure that each persistent volume uses a local-blk storage class name.
7.1. Retrieve an example LocalVolume custom resource definition from the operator
CSV resource.
The alm-examples annotation contains a JSON-encoded string that defines a list
of example custom resource definitions.
[student@workstation storage-local]$ oc get $CSV_NAME \
> -o jsonpath='{.metadata.annotations.alm-examples}{"\n"}'
[
{
"apiVersion": "local.storage.openshift.io/v1",
"kind": "LocalVolume",
"metadata": {
"name": "example"
},
"spec": {
"storageClassDevices": [
{
"devicePaths": [
"/dev/vde",
"/dev/vdf"
],
"fsType": "ext4",
"storageClassName": "foobar",
"volumeMode": "Filesystem"
}
]
}
}
]
[student@workstation storage-local]$
Note
The alm-examples string does not end with a new line character.
The end of the preceding jsonpath expression contains {"\n"} to ensure the
prompt displays on a separate line.
276 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
7.2. Use the provided get_localvolume_template.sh script to create a
LocalVolume resource definition in YAML format. The script converts the
previous example from JSON to YAML and stores the example in the resources/
localvolume.yml file.
[student@workstation storage-local]$ ./get_localvolume_template.sh
Created /home/student/DO380/labs/storage-local/resources/localvolume.yml
[student@workstation storage-local]$
7.3. Review the resources/localvolume.yml template file.
[student@workstation storage-local]$ cat resources/localvolume.yml
apiVersion: local.storage.openshift.io/v1
kind: LocalVolume
metadata:
name: example
spec:
storageClassDevices:
- devicePaths:
- /dev/vde
- /dev/vdf
fsType: ext4
storageClassName: foobar
volumeMode: Filesystem
In the next step, change the name to local-storage.
In the next step, change the devicePaths list to only contain the /dev/vdb
device.
In the next step, change the storage class name to local-blk.
7.4. Use your preferred editor to update values in the resources/localvolume.yml
file. Ensure the file matches the following content, and save the file.
[student@workstation storage-local]$ cat resources/localvolume.yml
apiVersion: local.storage.openshift.io/v1
kind: LocalVolume
metadata:
name: local-storage
spec:
storageClassDevices:
- devicePaths:
- /dev/vdb
fsType: ext4
storageClassName: local-blk
volumeMode: Filesystem
7.5. Use the localvolume.yml file to create the LocalVolume resource.
[student@workstation storage-local]$ oc create -f resources/localvolume.yml
localvolume.local.storage.openshift.io/local-storage created
7.6. Verify that the local-storage resource exists.
DO380-OCP4.4-en-2-20200803 277
Chapter 8 | Configuring Persistent Storage
[student@workstation storage-local]$ oc get localvolume
NAME AGE
local-storage 71s
8. Verify that the operator uses the custom resource to create persistent volumes.
8.1. After you create the LocalVolume resource, verify that the operator deploys pods to
each worker node to control local storage provisioning:
[student@workstation storage-local]$ oc get pods
NAME READY STATUS RESTARTS AGE
local-storage-local-diskmaker-6rqbv 1/1 Running 0 29s
...output omitted...
local-storage-local-diskmaker-zsnqg 1/1 Running 0 30s
local-storage-local-provisioner-4cqnr 1/1 Running 0 30s
...output omitted...
local-storage-local-provisioner-zkfn8 1/1 Running 0 29s
local-storage-operator-75d7454577-kzr9b 1/1 Running 0 11m
There are nine local-diskmaker pods, and nine local-provisioner pods. Each
of the nine nodes contains a local-diskmaker and local-provisioner pod.
8.2. Verify that the local storage operator creates three persistent volumes.
[student@workstation storage-local]$ oc get pv
NAME CAPACITY ... STORAGECLASS REASON AGE
local-pv-21231ed8 20Gi ... local-blk 27s
local-pv-6013b152 20Gi ... local-blk 27s
local-pv-80eb4d7f 20Gi ... local-blk 28s
...output omitted...
Because the additional /dev/vdb block device exists only on the worker04,
worker05, and worker06 nodes, only three persistent volumes exist.
Note
If the command output is difficult to read, try a different command to filter specific
columns.
To only display the NAME, CAPACITY, and STORAGECLASS columns, use the
following command:
[student@workstation local]$ NAME="NAME:.metadata.name"
[student@workstation local]$ CAPACITY="CAPACITY:.spec.capacity.storage"
[student@workstation local]$ CLASS="STORAGECLASS:.spec.storageClassName"
[student@workstation local]$ oc get pv \
> -o custom-columns="$NAME,$CAPACITY,$CLASS"
See oc get --help for more information about custom columns.
8.3. Display the list of storage classes.
278 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
[student@workstation storage-local]$ oc get storageclass
NAME PROVISIONER ...
nfs-storage (default) nfs-storage-provisioner ...
local-blk kubernetes.io/no-provisioner ...
The operator uses information in the LocalVolume custom resource to create the
local-blk storage class.
9. Change to the /home/student/ directory.
[student@workstation storage-local]$ cd
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab storage-local finish
This concludes the section.
DO380-OCP4.4-en-2-20200803 279
Chapter 8 | Configuring Persistent Storage
Lab
Configuring Persistent Storage
In this lab, you will create local block storage resources for the cluster and deploy a database
that uses local block storage.
Outcomes
You should be able to:
• Use a storage class in a persistent volume claim to request block storage.
• Create a storage quota that restricts storage resources by storage class.
• Install the Local Storage Operator
Before You Begin
To perform this exercise, ensure you have access to:
• A running OpenShift cluster.
• The additional worker nodes (worker04, worker05, and worker06), which are active.
On the workstation machine, use the lab command to prepare your system for this
exercise.
The command ensures that the cluster API is reachable.
[student@workstation ~]$ lab storage-review start
Instructions
1. Create an operator group, to start the installation of the Local Storage Operator.
As the admin user, create a local-storage operator group resource in the storagelocal
project. Ensure that the operator group targets the storage-local namespace.
2. To finish operator installation, create a subscription resource for the Local Storage Operator
in the storage-local project.
Verify that a cluster service version resource exists after you create the subscription.
3. The Local Storage Operator uses configuration from LocalVolume resources to create and
manage local persistent volumes.
Create a LocalVolume resource. Ensure that the operator creates a persistent volume for
each /dev/vdb device that is attached to the cluster nodes.
Further ensure that each persistent volume:
• corresponds to the local-blk storage class.
• uses a Filesystem volume mode.
4. As the admin user, create a storage-review project. Add the developer user as an
administrator for the project.
280 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
Create a storage quota that restricts the project to:
• 1 persistent volume claim for storage from the local-blk storage class.
• 10 GB of requested storage, regardless of storage class.
5. As the developer user, deploy a postgres database in the storage-review project.
The ~/DO380/labs/storage-review directory contains a postgres subdirectory. The
subdirectory contains resources to deploy a postgres database.
Modify the postgres/statefulset.yml file to ensure that the database uses a localblk
persistent volume.
Evaluation
As the student user on the workstation machine, use the lab command to grade your work.
Correct any reported failures and rerun the command until successful.
[student@workstation ~]$ lab storage-review grade
Finish
As the student user on the workstation machine, use the lab command to complete this
exercise. This is important to ensure that resources from previous exercises do not impact
upcoming exercises.
[student@workstation ~]$ lab storage-review finish
This concludes the section.
DO380-OCP4.4-en-2-20200803 281
Chapter 8 | Configuring Persistent Storage
Solution
Configuring Persistent Storage
In this lab, you will create local block storage resources for the cluster and deploy a database
that uses local block storage.
Outcomes
You should be able to:
• Use a storage class in a persistent volume claim to request block storage.
• Create a storage quota that restricts storage resources by storage class.
• Install the Local Storage Operator
Before You Begin
To perform this exercise, ensure you have access to:
• A running OpenShift cluster.
• The additional worker nodes (worker04, worker05, and worker06), which are active.
On the workstation machine, use the lab command to prepare your system for this
exercise.
The command ensures that the cluster API is reachable.
[student@workstation ~]$ lab storage-review start
Instructions
1. Create an operator group, to start the installation of the Local Storage Operator.
As the admin user, create a local-storage operator group resource in the storagelocal
project. Ensure that the operator group targets the storage-local namespace.
1.1. Log into your OpenShift cluster as the admin user.
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
1.2. Create the storage-local project.
[student@workstation ~]$ oc new-project storage-local
Now using project "storage-local" on server "https://api.ocp4.example.com:6443".
...output omitted...
282 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
1.3. Create a resource file for the local-storage operator group, which contains the
following:
apiVersion: operators.coreos.com/v1alpha2
kind: OperatorGroup
metadata:
name: local-storage
namespace: storage-local
spec:
targetNamespaces:
- storage-local
1.4. Create the operator group resource from the resource definition file you created:
[student@workstation ~]$ cat operatorgroup.yml
apiVersion: operators.coreos.com/v1alpha2
kind: OperatorGroup
metadata:
name: local-storage
namespace: storage-local
spec:
targetNamespaces:
- storage-local
[student@workstation ~]$ oc apply -f operatorgroup.yml
operatorgroup.operators.coreos.com/local-storage created
2. To finish operator installation, create a subscription resource for the Local Storage Operator
in the storage-local project.
Verify that a cluster service version resource exists after you create the subscription.
2.1. Create a subscription resource file for the local storage operator, which contains the
following:
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
name: local-storage-operator
namespace: storage-local
spec:
channel: "4.4"
installApproval: automatic
name: local-storage-operator
source: redhat-operators
sourceNamespace: openshift-marketplace
2.2. Create the subscription resource from the resource definition file you created:
[student@workstation ~]$ cat subscription.yml
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
name: local-storage-operator
DO380-OCP4.4-en-2-20200803 283
Chapter 8 | Configuring Persistent Storage
namespace: storage-local
spec:
channel: "4.4"
installApproval: automatic
name: local-storage-operator
source: redhat-operators
sourceNamespace: openshift-marketplace
[student@workstation ~]$ oc apply -f subscription.yml
subscription.operators.coreos.com/local-storage-operator created
2.3. Verify the cluster service version resource exists for the local storage operator:
[student@workstation ~]$ oc get csv
NAME DISPLAY ... PHASE
local-storage-operator.4.4.0-2020... Local Storage ... Succeeded
3. The Local Storage Operator uses configuration from LocalVolume resources to create and
manage local persistent volumes.
Create a LocalVolume resource. Ensure that the operator creates a persistent volume for
each /dev/vdb device that is attached to the cluster nodes.
Further ensure that each persistent volume:
• corresponds to the local-blk storage class.
• uses a Filesystem volume mode.
3.1. Create a LocalVolume resource definition file, which contains the following:
apiVersion: local.storage.openshift.io/v1
kind: LocalVolume
metadata:
name: local-storage
spec:
storageClassDevices:
- devicePaths:
- /dev/vdb
fsType: ext4
storageClassName: local-blk
volumeMode: Filesystem
3.2. Create the LocalVolume resource from the resource definition file you created:
[student@workstation ~]$ cat localvolume.yml
apiVersion: local.storage.openshift.io/v1
kind: LocalVolume
metadata:
name: local-storage
spec:
storageClassDevices:
- devicePaths:
- /dev/vdb
fsType: ext4
storageClassName: local-blk
284 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
volumeMode: Filesystem
[student@workstation ~]$ oc apply -f localvolume.yml
localvolume.local.storage.openshift.io/local-storage created
4. As the admin user, create a storage-review project. Add the developer user as an
administrator for the project.
Create a storage quota that restricts the project to:
• 1 persistent volume claim for storage from the local-blk storage class.
• 10 GB of requested storage, regardless of storage class.
4.1. Create the storage-review project.
[student@workstation ~]$ oc new-project storage-review
Now using project "storage-review" on server ...
...output omitted...
4.2. Add the developer user as a project administrator.
[student@workstation ~]$ oc policy add-role-to-user admin developer
clusterrole.rbac.authorization.k8s.io/admin added: "developer"
4.3. Create a storage quota for the storage-review project:
[student@workstation ~]$ sclass=local-blk.storageclass.storage.k8s.io
[student@workstation ~]$ limit1=${sclass}/persistentvolumeclaims=1
[student@workstation ~]$ limit2=requests.storage=10G
[student@workstation ~]$ oc create quota storage \
> --hard=${limit1},${limit2}
resourcequota/storage created
5. As the developer user, deploy a postgres database in the storage-review project.
The ~/DO380/labs/storage-review directory contains a postgres subdirectory. The
subdirectory contains resources to deploy a postgres database.
Modify the postgres/statefulset.yml file to ensure that the database uses a localblk
persistent volume.
5.1. Log in as the developer user.
[student@workstation ~]$ oc login -u developer \
> -p developer https://api.ocp4.example.com:6443
...output omitted...
5.2. Ensure the storage-review project is the active project.
[student@workstation ~]$ oc project storage-review
...output omitted...
5.3. Modify the volumeClaimTemplates section of the ~/DO380/labs/storagereview/
postgres/statefulset.yml file to match the following:
DO380-OCP4.4-en-2-20200803 285
Chapter 8 | Configuring Persistent Storage
...output omitted...
volumeClaimTemplates:
- metadata:
name: data
spec:
storageClassName: local-blk
accessModes: [ "ReadWriteOnce" ]
resources:
requests:
storage: 2Gi
5.4. Deploy the database.
[student@workstation ~]$ oc apply -k ~/DO380/labs/storage-review/postgres/
secret/postgresql created
service/postgresql created
statefuleset.apps/postgresql created
Evaluation
As the student user on the workstation machine, use the lab command to grade your work.
Correct any reported failures and rerun the command until successful.
[student@workstation ~]$ lab storage-review grade
Finish
As the student user on the workstation machine, use the lab command to complete this
exercise. This is important to ensure that resources from previous exercises do not impact
upcoming exercises.
[student@workstation ~]$ lab storage-review finish
This concludes the section.
286 DO380-OCP4.4-en-2-20200803
Chapter 8 | Configuring Persistent Storage
Summary
In this chapter, you learned:
• OpenShift provides many volume plug-ins that enable using a wide variety of storage
technologies in an OpenShift cluster.
• OpenShift uses dynamic provisioning to provide on-demand storage resources for some
storage technologies.
• Operators or playbooks can help manage statically-provisioned storage resources.
• Users specify a storage class in a persistent volume claim to request storage with particular
characteristics. When a storage class is not specified, OpenShift uses the default storage class.
DO380-OCP4.4-en-2-20200803 287
288 DO380-OCP4.4-en-2-20200803
Chapter 9
Managing Cluster Monitoring
and Metrics
Goal Configure and manage the OpenShift monitoring
stack.
Objectives • List cluster monitoring features.
• Send alerts to external locations.
• Use Prometheus and Grafana to assist in
diagnosing cluster problems.
• Configure Prometheus and Alertmanager to
use persistent storage.
Sections • Introducing the Cluster Monitoring Stack (and
Quiz)
• Managing OpenShift Alerts (and Guided
Exercise)
• Troubleshooting Using the Cluster Monitoring
Stack (and Guided Exercise)
• Configuring Storage for the Cluster Monitoring
Stack (and Guided Exercise)
DO380-OCP4.4-en-2-20200803 289
Chapter 9 | Managing Cluster Monitoring and Metrics
Introducing the Cluster Monitoring Stack
Objectives
After completing this section, you should be able to:
• List cluster monitoring features.
• Describe alert types and identify when to silence an alert.
Listing the Cluster Monitoring Stack Components
The monitoring stack deploys the following components in the environment to provide a complete
solution for monitoring the infrastructure, receiving alerts, and consulting performance graphs.
Cluster Monitoring Operator
The cluster monitoring operator (CMO) is the central component of the monitoring stack. It
controls the deployed monitoring components and ensures that they are always in sync with
the latest version of the CMO.
Prometheus Operator
The Prometheus operator deploys and configures both Prometheus and Alertmanager. The
operator also manages the generation and configuration of configuration targets (service
monitors and pod monitors).
Prometheus
Prometheus is the monitoring server.
Prometheus Adapter
The Prometheus adapter exposes cluster resources that are used for Horizontal Pod
Autoscaling (HPA)
Alertmanager
Alertmanager handles alerts sent by the Prometheus server.
kube-state-metrics
kube-state-metrics is a converter agent that exports Kubernetes objects to metrics that
Prometheus can parse.
openshift-state-metrics
openshift-state-metrics is based on the kube-state-metrics and adds monitoring
for OpenShift-specific resources (such as image registry metrics).
node-exporter
node-exporter exports low-level metrics for worker nodes.
Thanos Querier
Thanos Querier is a single, multi-tenant interface that enables aggregating and deduplicating
cluster and user workload metrics.
Grafana
Grafana is a platform for analyzing and visualizing metrics. The Grafana dashboards provided
with the monitoring stack are read-only.
290 DO380-OCP4.4-en-2-20200803
Chapter 9 | Managing Cluster Monitoring and Metrics
Defining the Cluster Monitoring Stack Custom
Resource Definitions
The cluster monitoring operator deploys and manages the life cycle of a Prometheus-based stack
for monitoring the cluster. The operator creates the following five custom resource definitions
(CRDs):
• Prometheus for deploying Prometheus instances. Display all resource instances with:
[user@demo ~]$ oc get prometheuses -A
• ServiceMonitor for managing configuration files that describe how and where to scrape the
data from application services. Display all resource instances with:
[user@demo ~]$ oc get servicemonitors -A
• PodMonitor for managing configuration files for scraping data from pods. Display all resource
instances with:
[user@demo ~]$ oc get podmonitors -A
• Alertmanager for deploying and managing alert manager instances. Display all resource
instances with:
[user@demo ~]$ oc get alertmanagers -A
• PrometheusRule for managing rules. Rules correspond to a query set, a filter, and any other
field. Display all resource instances with:
[user@demo ~]$ oc get prometheusrules -A
The prometheus-k8s-rulefiles-0 configuration map in the openshift-monitoring
namespace provides a combined view of all of the Prometheus rules.
Assigning Cluster Monitoring Roles
Assign the cluster-monitoring-view cluster role to a user to give permission to see cluster
alerts and metrics. This allows a user to see the Monitoring section of the OpenShift web
console.
[user@demo ~]$ oc adm policy add-cluster-role-to-user USER \
> cluster-monitoring-view
Describing Alertmanager Features
An alert is a rule that evaluates to true or false. The rule is often based on cluster observations,
such as cluster CPU utilization.
An alert is also associated with a duration. For an alert to trigger, the alert rule must continue to
evaluate to true for the defined duration.
DO380-OCP4.4-en-2-20200803 291
Chapter 9 | Managing Cluster Monitoring and Metrics
You access cluster alerts from the OpenShift web console at Monitoring ® Alerting. A brief
description of the alert, the alert state, and the alert severity are displayed. View alert details by
clicking the name of the alert.
Figure 9.1: Alert List
An alert has four states:
Firing
The alert rule evaluates to true, and has evaluated to true for longer than the defined alert
duration.
Pending
The alert rule evaluates to true, but has not evaluated to true for longer than the defined alert
duration.
Silenced
The alert is Firing, but is actively being silenced. Administrators can silence an alert to
temporary deactivate it.
Not Firing
Any alert that is not Firing, Pending, or Silenced is labeled as Not Firing.
Although the Firing, Silenced, and Pending alert states are displayed by default from the web
console, the display of each state can be toggled on or off.
Understanding Alert Rules
To stop an alert, you must fix or otherwise address any cluster conditions that cause the alert to
fire.
Click the name of any alert in the OpenShift web console to display alert details. You can check
alert details regardless of the state.
292 DO380-OCP4.4-en-2-20200803
Chapter 9 | Managing Cluster Monitoring and Metrics
Figure 9.2: Alert Details
In this example, the KubeCronJobRunning alert is defined in the KubeCronJobRunning
alerting rule. This alert has a severity of Warning and is currently in a Pending state because it
has not been firing for more than 60 minutes. The Message section describes the alert.
Although hidden in this graphic, the alert detail page typically displays the evaluated metric in a
graph. The graph contains a View in Metrics link that populates Monitoring ® Metrics with
the evaluation query.
Silencing an Alert
Silence an alert while you actively work to resolve the issue. No additional notifications about a
silenced alert are sent until the silence expires.
Silence an alert using the vertical ellipsis menu at the far right of the alert row.
Figure 9.3: Silence Alert
DO380-OCP4.4-en-2-20200803 293
Chapter 9 | Managing Cluster Monitoring and Metrics
An alert is silenced for two hours by default, but it can be silenced for a longer or shorter period.
Complete the Creator and Comment fields to provide insight into who silenced the alert and why
the alert was silenced.
The Alerting page at Monitoring ® Alerting displays silenced alerts by default including when
each silence will end.
Figure 9.4: Alert List with Silence
In this example, the KubeCPUOvercommit alert has been silenced. The silenced alert is scheduled
to end at 12:48am on April 29. The alert will start firing again if the condition causing the alert has
not been resolved by that time.
References
• For more information, refer to the Cluster Monitoring chapter in the OpenShift
Monitoring Guide at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html/monitoring/cluster-monitoring
294 DO380-OCP4.4-en-2-20200803
Chapter 9 | Managing Cluster Monitoring and Metrics
Quiz
Introducing the Cluster Monitoring Stack
Choose the correct answers to the following questions:
1. The cluster monitoring stack (including Prometheus and Grafana) is a separate
component that needs to be added after installing your OpenShift cluster. (True or
False)
a. True
b. False
2. The prometheusrule custom resource definition only has instances in the
openshift-monitoring namespace. (True or False)
a. True
b. False
3. Which two statements about pending alerts are true? (Choose two.)
a. The alert expression for the pending alert is evaluating as true, but the expression has not
evaluated as true for a specified duration of time.
b. The alert expression for the pending alert is not currently evaluating as true even though it
may evaluate as true at some point in the future.
c. Pending alerts rarely apply to critical alerts.
d. Pending alerts are common and should not necessarily cause concern until they move to a
firing state.
4. Which two statements about silenced alerts are true? (Choose two.)
a. When silencing an alert, the default setting is to silence the alert indefinitely.
b. When silencing an alert, the default setting is to silence the alert for two hours.
c. Silence an alert during troubleshooting so that Alertmanager does not send additional
notifications about the alert.
d. Silence an alert during troubleshooting to tell Prometheus to stop checking the evaluation
expression for the alert.
This concludes the section.
DO380-OCP4.4-en-2-20200803 295
Chapter 9 | Managing Cluster Monitoring and Metrics
Solution
Introducing the Cluster Monitoring Stack
Choose the correct answers to the following questions:
1. The cluster monitoring stack (including Prometheus and Grafana) is a separate
component that needs to be added after installing your OpenShift cluster. (True or
False)
a. True
b. False
2. The prometheusrule custom resource definition only has instances in the
openshift-monitoring namespace. (True or False)
a. True
b. False
3. Which two statements about pending alerts are true? (Choose two.)
a. The alert expression for the pending alert is evaluating as true, but the expression has not
evaluated as true for a specified duration of time.
b. The alert expression for the pending alert is not currently evaluating as true even though it
may evaluate as true at some point in the future.
c. Pending alerts rarely apply to critical alerts.
d. Pending alerts are common and should not necessarily cause concern until they move to a
firing state.
4. Which two statements about silenced alerts are true? (Choose two.)
a. When silencing an alert, the default setting is to silence the alert indefinitely.
b. When silencing an alert, the default setting is to silence the alert for two hours.
c. Silence an alert during troubleshooting so that Alertmanager does not send additional
notifications about the alert.
d. Silence an alert during troubleshooting to tell Prometheus to stop checking the evaluation
expression for the alert.
This concludes the section.
296 DO380-OCP4.4-en-2-20200803
Chapter 9 | Managing Cluster Monitoring and Metrics
Managing OpenShift Alerts
Objectives
After completing this section, you should be able to send alerts to external locations.
Describing Alertmanager Default Receivers
By default, alerts are not sent to external locations. The OpenShift web console displays alerts at
Monitoring ® Alerting and alerts can be accessed from the Alertmanager API.
[user@demo ~]$ ALERTMANAGER="$(oc get route/alertmanager-main \
> -n openshift-monitoring -o jsonpath='{.spec.host}')"
[user@demo ~]$ curl -s -k -H "Authorization: Bearer $(oc sa get-token \
> prometheus-k8s -n openshift-monitoring)" \
> https://${ALERTMANAGER}/api/v1/alerts | jq
Configure Alertmanager to send alerts to external locations such as email, PagerDuty, and
HipChat so that you are promptly notified about cluster problems.
Alertmanager sends alerts to the locations configured in the alertmanager-main secret in the
openshift-monitoring namespace.
This is the default configuration of the alertmanager-main secret. The double quotes can be
removed to improve readability.
"global":
"resolve_timeout": "5m"
"receivers":
- "name": "null"
"route":
"group_by":
- "namespace"
"group_interval": "5m"
"group_wait": "30s"
"receiver": "null"
"repeat_interval": "12h"
"routes":
- "match":
"alertname": "Watchdog"
"receiver": "null"
Value used by Alertmanager if the alert does not include a EndsAt field. After this time
passes it can declare the alert as resolved if it has not been updated.
How long to wait before sending a notification about new alerts that are added to a group of
alerts for which an initial notification has already been sent.
How long to wait to send an initial notification for a group of alerts. Allows for waiting for an
inhibiting alert to arrive or for collecting more initial alerts for the same group.
DO380-OCP4.4-en-2-20200803 297
Chapter 9 | Managing Cluster Monitoring and Metrics
This is the default receiver for notifications. A value of null indicates that it does nothing. All
instances of null are typically changed to something like default.
How long to wait before sending a notification again if it has already been sent successfully
for an alert.
A route block defines a node in a routing tree and its children. Its optional configuration
parameters are inherited from its parent node if not set.
Define a filter of type alertname.
Sending Alerts to PagerDuty
If your company already uses PagerDuty, you will likely want to send alerts about your OpenShift
cluster to PagerDuty. Alerts can be routed to cellphones, pagers, and email addresses. The
Prometheus Integration Guide on the PagerDuty website walks through how to configure
PagerDuty for Prometheus notifications. The following configuration will send critical alert
messages to PagerDuty.
"global":
"resolve_timeout": "5m"
"receivers":
- "name": "pagerduty-notification"
"pagerduty_configs":
"service_key": "e679d31da9b34aa4b8bb4f412186546d"
- "name": "default"
"route":
"group_by":
- "job"
"group_interval": "5m"
"group_wait": "30s"
"receiver": "default"
"repeat_interval": "12h"
"routes":
- "match":
"alertname": "Watchdog"
"receiver": "default"
- "match":
"severity": "critical"
"receiver": "pagerduty-notification"
An arbitrary name that is used by an associated route.
The PagerDuty integration key for Prometheus. The service_key property corresponds
to the PagerDuty Events API v1. The routing_key property can be used for the PagerDuty
Events API v2.
The matching criteria for the alert.
Multiple PagerDuty receivers can be configured so that different types of alerts are sent to the
appropriate teams. Consult the References section for additional details.
Sending Alerts to Email
Alerts can be sent by email through an SMTP server. The following example sends critical
alerts to the ocp-admins@example.com email alias.
298 DO380-OCP4.4-en-2-20200803
Chapter 9 | Managing Cluster Monitoring and Metrics
"global":
"resolve_timeout": "5m"
"smtp_smarthost": "utility.lab.example.com:25"
"smtp_from": "alerts@ocp4.example.com"
"smtp_auth_username": "smtp_training"
"smtp_auth_password": "Red_H4T@!"
"smtp_require_tls": false
"receivers":
- "name": "email-notification"
"email_configs":
- "to": "ocp-admins@example.com"
- "name": "default"
"route":
"group_by":
- "job"
"group_interval": "5m"
"group_wait": "30s"
"receiver": "default"
"repeat_interval": "12h"
"routes":
- "match":
"alertname": "Watchdog"
"receiver": "default"
- "match":
"severity": "critical"
"receiver": "email-notification"
The global SMTP host. This host is used if smarthost is not defined in the email_configs
for a receiver.
The global email sender address. This address is used if from is not defined in the
email_configs for a receiver.
The global SMTP user name for optional authentication. This user name is used if
auth_username is not defined in the email_configs for a receiver.
The global SMTP password for optional authentication. This password is used if
auth_password is not defined in the email_configs for a receiver.
A global setting specifying if TLS is required for SMTP. This setting can be overridden using
require_tls in the email_configs for a receiver.
An arbitrary name for the receiver. A route specifies this receiver name for a match.
This setting indicates that the receiver will send alerts by email.
The to setting must be specified under email_conifgs and does not have an equivalent
global SMTP setting.
The receiver to use if the match evaluates as true for the alert.
Applying a New Alertmanager Configuration
Apply a new Alertmanager configuration by updating the alertmanager-main secret in the
openshift-monitoring namespace.
[user@demo ~]$ oc set data secret/alertmanager-main \
> -n openshift-monitoring --from-file=/tmp/alertmanager.yaml
secret/alertmanager-main data updated
DO380-OCP4.4-en-2-20200803 299
Chapter 9 | Managing Cluster Monitoring and Metrics
A successful update generates the log message: "Completed loading of configuration
file" file=/etc/alertmanager/config/alertmanager.yaml Check for this message
in the alertmanager container within the alertmanager-main-0 pod in the openshiftmonitoring
namespace. An incorrect configuration generates a failed log message.
[user@demo ~]$ oc logs -f -c alertmanager alertmanager-main-0 \
> -n openshift-monitoring
...output omitted...
level=info ts=2020-05-15T18:53:09.052Z caller=coordinator.go:119
component=configuration msg="Loading configuration file" file=/etc/alertmanager/
config/alertmanager.yaml
level=info ts=2020-05-15T18:53:09.052Z caller=coordinator.go:131
component=configuration msg="Completed loading of configuration file" file=/etc/
alertmanager/config/alertmanager.yaml
References
• Alertmanager Configuration
https://prometheus.io/docs/alerting/configuration/
• Prometheus Integration Guide
https://www.pagerduty.com/docs/guides/prometheus-integration-guide/
300 DO380-OCP4.4-en-2-20200803
Chapter 9 | Managing Cluster Monitoring and Metrics
Guided Exercise
Managing OpenShift Alerts
In this exercise you will send alerts to the ocp-admins@example.com email address and
resolve the AlertmanagerReceiversNotConfigured alert.
Outcomes
You should be able to:
• Configure Alertmanager to send email alerts.
• Review default alerts.
• Silence a firing alert.
Before You Begin
To perform this exercise, ensure you have access to:
• A running OpenShift cluster.
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise.
This command ensures that the utility.lab.example.com host is configured to receive
email and configures the ocp-admins@example.com email alias to deliver messages to
the lab user on utility.lab.example.com.
[student@workstation ~]$ lab monitor-alerts start
Instructions
1. Configure Alertmanager to send warning alerts to the ocp-admins@example.com email
address.
1.1. Log in to your OpenShift cluster as the admin user.
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
1.2. Extract the existing alertmanager-main secret from the openshiftmonitoring
namespace to the /tmp/ directory.
[student@workstation ~]$ oc extract secret/alertmanager-main --to /tmp/ \
> -n openshift-monitoring --confirm
/tmp/alertmanager.yaml
DO380-OCP4.4-en-2-20200803 301
Chapter 9 | Managing Cluster Monitoring and Metrics
1.3. The default alertmanager-main secret contains many unnecessary quotes.
Remove the quotes using the sed command to improve readability.
[student@workstation ~]$ sed -i 's/"//g' /tmp/alertmanager.yaml
1.4. Modify the /tmp/alertmanager.yaml file.
[student@workstation ~]$ vim /tmp/alertmanager.yaml
1.5. Add the lines in bold to the file. Ensure that you correctly indent each line. Replace
instances of null with default.
global:
resolve_timeout: 5m
smtp_smarthost: 192.168.50.254:25
smtp_from: alerts@ocp4.example.com
smtp_require_tls: false
receivers:
- name: default
- name: email-notification
email_configs:
- to: ocp-admins@example.com
route:
group_by:
- namespace
group_interval: 5m
group_wait: 30s
receiver: default
repeat_interval: 12h
routes:
- match:
alertname: Watchdog
receiver: default
- match:
severity: warning
receiver: email-notification
Note
Setting a severity of warning sends many email messages. You change the severity
to critical later in the exercise.
The /home/student/DO380/solutions/monitor-alerts/
alertmanager.yaml file contains the correct configuration.
1.6. Update the existing alertmanager-main secret in the openshift-monitoring
namespace with the content of the /tmp/alertmanager.yaml file.
[student@workstation ~]$ oc set data secret/alertmanager-main \
> -n openshift-monitoring --from-file /tmp/alertmanager.yaml
secret/alertmanager-main data updated
302 DO380-OCP4.4-en-2-20200803
Chapter 9 | Managing Cluster Monitoring and Metrics
1.7. Follow the alertmanager container logs and look for the message: "Completed
loading of configuration file" file=/etc/alertmanager/config/
alertmanager.yaml
It can take up to 30 seconds to display new log messages. Press Ctrl+C to exit the
oc logs command.
[student@workstation ~]$ oc logs -f -c alertmanager alertmanager-main-0 \
> -n openshift-monitoring
...output omitted...
level=info ts=2020-05-15T18:53:09.052Z caller=coordinator.go:119
component=configuration msg="Loading configuration file" file=/etc/alertmanager/
config/alertmanager.yaml
level=info ts=2020-05-15T18:53:09.052Z caller=coordinator.go:131
component=configuration msg="Completed loading of configuration file" file=/etc/
alertmanager/config/alertmanager.yaml
Note
If you see configuration errors in the logs, modify the /tmp/alertmanager.yaml
file and reapply your changes.
2. The lab user on the utility.lab.example.com host receives emails sent to the ocpadmins@
example.com email address. Verify that Alertmanager sent an email for each
firing alert.
2.1. Use SSH to connect to the utility.lab.example.com host as the lab user.
[student@workstation ~]$ ssh lab@utility.lab.example.com
...output omitted...
2.2. Run the mutt command to access mail.
[lab@utility ~]$ mutt
Allow mutt to create the /home/lab/Mail directory.
/home/lab/Mail does not exist. Create it? ([yes]/no): y
2.3. The existing emails sent from alerts@ocp4.example.com demonstrate that
Alertmanager sent email notifications for firing alerts with a severity of warning or
greater. Press q to quit.
q:Quit d:Del u:Undel s:Save m:Mail r:Reply g:Group ?:Help
1 N Jun 25 alerts@ocp4.exa ( 192) [FIRING:1] openshift-image-registry (I...
2 N Jun 25 alerts@ocp4.exa ( 171) [FIRING:1] (AlertmanagerReceiversNotC...
Note
Alertmanager sends alerts as email attachments in HTML format. Although not
necessary for this exercise, you can save the attachments and transfer them to a
machine with graphical capabilities.
DO380-OCP4.4-en-2-20200803 303
Chapter 9 | Managing Cluster Monitoring and Metrics
2.4. Disconnect from the SSH session.
[lab@utility ~]$ exit
3. View alerts from the OpenShift web console.
3.1. Use Firefox to navigate to https://console-openshiftconsole.
apps.ocp4.example.com.
Note
If Firefox displays a warning message, then adds exceptions to allow using the
default OpenShift certificates.
3.2. Log in with htpasswd_provider using admin as the user name and redhat as the
password.
3.3. From the OpenShift web console, navigate to Monitoring ® Alerting.
3.4. Because there may be additional alerts, firing alerts are expected to include the
ImagePruningDisabled and Watchdog alerts.
4. Examine the ImagePruningDisabled alert and then silence the alert for troubleshooting.
4.1. Click the ImagePruningDisabled link to get details.
This alert has a severity of warning and it fires when automatic image pruning is not
enabled. The alert message indicates that the alert can be resolved by ensuring the
suspend field is set to false in the imagepruner/cluster resource.
4.2. Silence the alert so that Alertmanager does not send additional notifications while
you resolve the problem. Select Actions ® Silence Alert to silence the alert.
By default, the alert will be silenced for two hours. You do not need to change the
Start or End fields.
In the Creator field, type your name.
In the Comment field, type Silenced during troubleshooting.
Click Create to silence the alert.
5. Resolve the ImagePruningDisabled alert by setting the suspend field to false in the
imagepruner/cluster resource.
5.1. The image-pruner cron job in the openshift-image-registry namespace is
disabled by default. Use the oc edit command to enable the cron job by setting the
suspend field to false in the imagepruner/cluster resource.
For demonstration purposes, configure the cron job to run every 5 minutes by setting
the schedule field to */5 * * * *. If you do not modify the schedule field, then
the image-pruner cron job defaults to running once per day at midnight.
[student@workstation ~]$ oc edit imagepruner/cluster
...output omitted...
spec:
failedJobsHistoryLimit: 3
keepTagRevisions: 3
304 DO380-OCP4.4-en-2-20200803
Chapter 9 | Managing Cluster Monitoring and Metrics
schedule: "*/5 * * * *"
successfulJobsHistoryLimit: 3
suspend: false
...output omitted...
5.2. Navigate to Monitoring ® Alerting in the web console to verify that the
ImagePruningDisabled alert is no longer firing.
Note
The ImagePruningDisabled alert might display a status of Silenced for a
minute before it disappears.
Additionally, a new pending KubeCronJobRunning alert might appear until the
image-pruner cron job runs and completes.
6. Although you previously configured Alertmanager to send email alerts with a severity of
warning or greater to the ocp-admins@example.com email address, this configuration
will likely send more messages than desired. Modify the Alertmanager configuration to send
critical alerts to the ocp-admins@example.com email address.
6.1. Use the sed command to replace warning with critical in the /tmp/
alertmanager.yaml file.
[student@workstation ~]$ sed -i 's/warning/critical/' /tmp/alertmanager.yaml
6.2. Update the existing alertmanager-main secret in the openshift-monitoring
namespace with the content of the /tmp/alertmanager.yaml file.
[student@workstation ~]$ oc set data secret/alertmanager-main \
> -n openshift-monitoring --from-file /tmp/alertmanager.yaml
secret/alertmanager-main data updated
Alertmanager now sends an email for any alert with a severity of critical to the
ocp-admins@example.com email address.
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab monitor-alerts finish
This concludes the section.
DO380-OCP4.4-en-2-20200803 305
Chapter 9 | Managing Cluster Monitoring and Metrics
Troubleshooting Using the Cluster
Monitoring Stack
Objectives
After completing this section, you should be able to use Prometheus and Grafana to assist in
diagnosing cluster problems.
Introducing Prometheus
Prometheus is an open source project for system monitoring and alerting.
Both Red Hat OpenShift Container Platform and Kubernetes integrate Prometheus to enable
cluster metrics, monitoring, and alerting capabilities.
Prometheus gathers and stores streams of data from the cluster as time series data. Time series
data consists of a sequence of samples, with each sample containing:
• A timestamp
• A numeric value (such as an integer, float, or Boolean)
• A set of labels in the form of key/value pairs
The key/value pairs are used to isolate groups of related values for filtering.
As an example, the machine_cpu_cores metric in Prometheus contains a sequence of
measurement samples of the number of CPU cores for each machine.
OpenShift integrates Prometheus metrics at Monitoring ® Metrics. A separate Prometheus
user interface is accessible using the URL of the prometheus-k8s route in the openshiftmonitoring
namespace.
From the Metrics page, enter an expression, such as a metric name, and then click Run Queries
to retrieve the most recent sample for the metric.
The following example displays the instance:node_cpu_utilisation:rate1m metric over
time.
306 DO380-OCP4.4-en-2-20200803
Chapter 9 | Managing Cluster Monitoring and Metrics
Figure 9.5: Prometheus Metrics
The metric contains data for each node instance in the cluster.
Using Prometheus Query Language
Prometheus provides a query language, PromQL, that allows you to select and aggregate timeseries
data.
You can filter a metric to include only certain key/value pairs. For example, modify the
previous query to only show metrics for the worker02 node by using the expression
instance:node_cpu_utilisation:rate1m{instance="worker02"}.
Prometheus Query Language provides several operators to compute new time-series metrics.
PromQL contains arithmetic operators, including addition, subtraction, multiplication, and division
operators. PromQL contains comparison operators, including equality, greater than, and less than
operators.
PromQL contains a number of built-in functions that you can include in PromQL expressions
including:
• sum() - which totals the value of all sample entries at a given time.
• rate() - computes the per-second average of a time series for a given time range.
• count() - counts the number of sample entries at a given time.
Every Prometheus alert contains a Prometheus Query Language expression. Consider the
KubeCPUOvercommit alert.
DO380-OCP4.4-en-2-20200803 307
Chapter 9 | Managing Cluster Monitoring and Metrics
Figure 9.6: KubeCPUOvercommit Alert Rule
The KubeCPUOvercommit alert expression compares two ratios using the sum function, the
division operator, and the greater-than operator:
• The left ratio is the total number of CPU requests divided by the total number of CPU cores.
• The right ratio is a count of entries minus 1 for the CPU cores metric, divided by a count of
entries for the CPU metric.
Each entry in the CPU cores metric corresponds to a node. Therefore, the right side ratio
corresponds to a percentage of cluster CPU capacity if a single node fails. If the expression
evaluates as true for five minutes, then the alert starts firing with a warning severity.
Firing alerts can notify administrators of potential cluster problems that might require further
investigation.
Introducing Grafana
Grafana is an open source analytics and visualization project and aims to improve the observability
of system metrics. OpenShift integrates Grafana dashboards at Monitoring ® Dashboards.
A separate Grafana user interface is accessible using the URL of the grafana route in the
openshift-monitoring namespace.
With Grafana, you can create customized dashboards to visualize key cluster metrics. Grafana
dashboards frequently refresh to display current summary metrics and graphs.
Grafana graphs are interactive. With Grafana, you can further explore interesting data features and
characteristics you observe in a graph.
OpenShift Container Platform provides several dashboards in Grafana. These dashboards serve as
a good starting point for near real-time observability of cluster metrics and health.
After being notified of an alert, an administrator might use Grafana dashboards to investigate
the problem. This could include checking if a specific node or project has a problem. Additionally,
Grafana dashboards can help identify if a problem was temporary or if it appears to be persistent.
308 DO380-OCP4.4-en-2-20200803
Chapter 9 | Managing Cluster Monitoring and Metrics
Grafana includes the following default dashboards:
etcd
This dashboard provides information on etcd instances running in the cluster.
Kubernetes/Compute Resources/Cluster
This dashboard provides a high level view of cluster resources.
Kubernetes/Compute Resources/Namespace (Pods)
This dashboard displays resource usage for pods within a namespace.
Kubernetes/Compute Resources/Namespace (Workloads)
This dashboard filters resource usage first by namespace and then by workload type, such as
deployment, daemonset, and statefulset. Grafana displays all workloads of the specified type
within the namespace.
Kubernetes/Compute Resources/Node (Pods)
This dashboard show pod resource usage filtered by node.
Kubernetes/Compute Resources/Pod
This dashboard displays the resource usage for individual pods. Select a namespace and a pod
within the namespace.
Kubernetes/Compute Resources/Workload
This dashboard provides resources usage filtered by namespace, workload, and workload type.
Kubernetes/Networking/Cluster
This dashboard displays network usage for the cluster. Grafana sorts many items to show
namespaces with the highest usage.
Prometheus
This dashboard provides detailed information about the prometheus-k8s pods running in
the openshift-monitoring namespace.
USE Method/Cluster
USE is an acronym for Utilization Saturation and Errors. This dashboard displays several
graphics that can identify if the cluster is over utilized, over saturated, or experiencing a
high number of errors. Because Grafana displays all nodes in the cluster, you may be able to
identity a node that is not behaving the same as the other nodes in the cluster.
The graphic that follows indicates that worker03 is experiencing a higher level of memory
saturation compared to the other nodes in the cluster.
DO380-OCP4.4-en-2-20200803 309
Chapter 9 | Managing Cluster Monitoring and Metrics
Figure 9.7: USE Method/Cluster - Memory Saturation
Additionally, worker03 is experiencing higher disk IO saturation.
Figure 9.8: USE Method/Cluster - Disk IO Saturation
USE Method/Node
This dashboard filters by node to display potential problems using the USE (Utilization
Saturation and Errors) method.
310 DO380-OCP4.4-en-2-20200803
Chapter 9 | Managing Cluster Monitoring and Metrics
References
For more information about Prometheus, refer to the Prometheus documentation at
https://prometheus.io/docs/introduction/overview/
For more information about Prometheus metrics, refer to the Prometheus Data
Model documentation at
https://prometheus.io/docs/concepts/data_model/
For more information on Prometheus Query Language, refer to the Prometheus
documentation at
https://prometheus.io/docs/prometheus/2.15/querying/basics/
For more information about Grafana, refer to the Getting Started guide at
https://grafana.com/docs/grafana/v6.2/guides/getting_started/
For more information about the USE Method, refer to
http://www.brendangregg.com/usemethod.html
DO380-OCP4.4-en-2-20200803 311
Chapter 9 | Managing Cluster Monitoring and Metrics
Guided Exercise
Troubleshooting Using the Cluster
Monitoring Stack
In this exercise you will identify a potential application problem using the cluster monitoring
stack.
Outcomes
You should be able to use Grafana to identify a deployment that consumes excessive
amounts of CPU and memory.
Before You Begin
To perform this exercise, ensure you have access to:
• A running OpenShift cluster.
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise.
This command ensures that the cluster API is reachable and deploys an application with a
potential problem.
[student@workstation ~]$ lab monitor-troubleshoot start
Instructions
1. Use Grafana to display cluster resource usage information within the OpenShift web
console.
1.1. Access the web console URL at https://console-openshiftconsole.
apps.ocp4.example.com using Firefox. Accept the TLS certificates,
if prompted, and then log in with htpasswd_provider using admin as the
username and redhat as the password.
1.2. Navigate to Monitoring ® Dashboards.
1.3. Select the Kubernetes/Compute Resources/Cluster dashboard.
1.4. Scroll to the CPU Quota section, and then click the CPU Usage column header. You
might need to click the column header more than once so that it is blue with an arrow
pointing down to indicate that rows are sorted showing namespaces using the highest
to lowest amount of CPU.
312 DO380-OCP4.4-en-2-20200803
Chapter 9 | Managing Cluster Monitoring and Metrics
Figure 9.9: Namespaces with the Highest CPU Usage
It is expected that the monitor-troubleshoot namespace is listed as one of
the top five namespaces using the most amount of CPU resources. Although the
monitor-troubleshoot namespace only requests 0.5 CPU resources, it uses
considerably more. The result is that the CPU Requests % column reports between
100% to 500%.
1.5. Scroll to the Requests by Namespace section and then click the Memory Usage
column header. You might need to click the column header more than once so
that it is blue with an arrow pointing down to indicate that rows are sorted showing
namespaces using the highest to lowest amount of memory.
Figure 9.10: Namespaces with the Highest Memory Usage
It is expected that the monitor-troubleshoot namespace is listed as one of the
top five namespaces using the most amount of memory resources. As with CPU
usage, the monitor-troubleshoot namespace uses considerably more memory
than the 200 MiB requested by the pods in the namespace.
2. Use Grafana to identify the workload and pod that uses the most cluster resources in the
monitor-troubleshoot namespace.
2.1. Select the Kubernetes/Compute Resources/Namespace (Workloads)
dashboard. Select the monitor-troubleshoot namespace and leave
deployment as the workload type.
2.2. The CPU Quota section shows that the hello deployment requests a total of
0.5 CPU resources and currently uses almost no CPU resources. The pythonload
deployment does not request or limit CPU resources, but the one pod in the
deployment consumes more than three CPU resources.
DO380-OCP4.4-en-2-20200803 313
Chapter 9 | Managing Cluster Monitoring and Metrics
Figure 9.11: Deployment CPU Usage
Although the actual CPU usage in your cluster might be different, it is clear that the
python-load deployment uses the most CPU resources.
2.3. The Memory Usage graph shows that the python-load deployment has dramatic
increases and decreases in memory usage. On the other hand, the ten pods in the
hello deployment consistently use a total of about 122 MiB of memory.
Figure 9.12: Deployment Memory Usage
3. Now that you have identified a deployment that appears to be behaving erratically and
might be using more resources than intended. The next step might be to identify the owner
of the python-load deployment so that the owner can verify the application.
Minimally, the owner should add either resource requests or limits for both CPU and
memory to the python-load deployment. Although a cluster administrator can do this,
setting incorrect limits might result in the pod entering a CrashLoopBackOff state.
Similarly, a cluster administrator can implement a quota for the monitor-troubleshoot
namespace to limit the total amount of CPU and memory resources used by the project.
Because the python-load deployment does not currently specify requests or limits for
either CPU or memory, restricting CPU and memory resources with a quota would prevent
the python-load pod from running.
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab monitor-troubleshoot finish
This concludes the section.
314 DO380-OCP4.4-en-2-20200803
Chapter 9 | Managing Cluster Monitoring and Metrics
Configuring Storage for the Cluster
Monitoring Stack
Objectives
After completing this section, you should be able to configure Prometheus and Alertmanager to
use persistent storage.
Configuring the Cluster Monitoring Operator
You can configure the cluster monitoring operator by creating a configuration map named
cluster-monitoring-config in the openshift-monitoring namespace. This
configuration map does not exist by default. Within the config.yaml entry of the data section,
specify components and their configurations.
The following cluster monitoring operator components are configurable:
• alertmanagerMain
• auth
• ingress
• kubeStateMetrics
• nodeExporter
• prometheusK8s
• prometheusOperator
For example, use the prometheusK8s component to configure persistent storage for
Prometheus, and the alertmanagerMain component to configure persistent storage for
Alertmanager.
The basic skeleton for the cluster-monitoring-config configuration map follows:
apiVersion: 1
kind: ConfigMap
metadata:
name: cluster-monitoring-config
namespace: openshift-monitoring
data:
config.yaml: |
<component>:
<component-configuration-options>
Configuring Prometheus Persistent Storage
Add persistent storage for Prometheus if you want stored metrics to survive recreation or
redeployment of the Prometheus pods. By default, Prometheus stores 15 days worth of metrics
using ephemeral storage.
DO380-OCP4.4-en-2-20200803 315
Chapter 9 | Managing Cluster Monitoring and Metrics
Prometheus Database storage requirements based on number of nodes/pods in the cluster
Number of
Nodes
Number of
Pods
Prometheus
storage
growth per
day
Prometheus
storage
growth per
15 days
RAM Space
(per scale
size)
Network
(per tsdb
chunk)
50 1800 6.3 GB 94 GB 6 GB 16 MB
100 3600 13 GB 195 GB 10 GB 26 MB
150 5400 19 GB 283 GB 12 GB 36 MB
200 7200 25 GB 375 GB 14 GB 46 MB
The following configuration map attaches 40 GB of persistent storage to Prometheus using the
existing gp2 storage class. The configuration specifies that data is retained for 15 days.
apiVersion: v1
kind: ConfigMap
metadata:
name: cluster-monitoring-config
namespace: openshift-monitoring
data:
config.yaml: |
prometheusK8s:
retention: 15d
volumeClaimTemplate:
metadata:
name: prometheus-local-pv
spec:
storageClassName: gp2
volumeMode: Filesystem
resources:
requests:
storage: 40Gi
Assuming a file name of persistent-storage-prometheus.yaml, use the oc apply
command to either create or update the configuration map:
[user@demo ~]$ oc apply -f persistent-storage-prometheus.yaml
configmap/cluster-monitoring-config created
After applying the configuration map, OpenShift redeploys the Prometheus pods as a stateful set
and reloads the configuration for Prometheus.
Run the oc get pvc command to retrieve the list of persistent volumes claims in the
openshift-monitoring namespace.
[user@demo ~]$ oc get pvc
NAME ... CAPACITY ACCESS MODES STORAGECLASS
prometheus-k8s-db-prometheus-k8s-0 ... 40Gi RWO gp2
prometheus-k8s-db-prometheus-k8s-1 ... 40Gi RWO gp2
316 DO380-OCP4.4-en-2-20200803
Chapter 9 | Managing Cluster Monitoring and Metrics
Configuring Alert Manager Persistent Storage
By default, alertmanager-main pods running in the openshift-monitoring namespace use
ephemeral storage. Configure Alertmanager to use persistent storage so that alerts are not lost if
the alertmanager-main pods are recreated or redeployed.
The following example configures Alertmanager to attach a 20 GB persistent volume using the
existing gp2 storage class for the cluster.
apiVersion: v1
kind: ConfigMap
metadata:
name: cluster-monitoring-config
namespace: openshift-monitoring
data:
config.yaml: |
alertmanagerMain:
volumeClaimTemplate:
metadata:
name: alertmanager-local-pv
spec:
storageClassName: gp2
volumeMode: Filesystem
resources:
requests:
storage: 20Gi
Assuming a file name of persistent-storage-alertmanager.yaml, apply the configuration
with:
[user@demo]$ oc apply -f persistent-storage-alertmanager.yaml
configmap/cluster-monitoring-config configured
References
• For more information, refer to the Scaling the Cluster Monitoring Operator chapter
in the OpenShift Scalability and Performance Guide at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/scalability_and_performance/
index#prometheus-database-storage-requirements_cluster-monitoring-operator
• Prometheus Storage
https://prometheus.io/docs/prometheus/latest/storage/
DO380-OCP4.4-en-2-20200803 317
Chapter 9 | Managing Cluster Monitoring and Metrics
Guided Exercise
Configuring Storage for the Cluster
Monitoring Stack
In this exercise you will configure Prometheus and Alertmanager to use persistent storage.
Outcomes
You should be able to:
• Create a new configuration map for the cluster monitoring operator.
• Define persistent storage for Prometheus and Alertmanager using an existing storage
class.
Before You Begin
To perform this exercise, ensure you have access to:
• A running OpenShift cluster.
• An existing storage class capable of dynamically provisioning persistent volumes.
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise.
This command ensures that the cluster API is reachable and that Prometheus and
Alertmanager use ephemeral storage.
[student@workstation ~]$ lab monitor-storage start
Instructions
1. Identify if your cluster already contains a configuration map for the cluster monitoring
operator.
1.1. Log in to your OpenShift cluster as the admin user.
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
1.2. Look for the cluster-monitoring-config configuration map in the
openshift-monitoring namespace. This command returns an error because the
cluster-monitoring-config configuration map does not exist by default.
[student@workstation ~]$ oc get configmap cluster-monitoring-config \
> -n openshift-monitoring
Error from server (NotFound): configmaps "cluster-monitoring-config" not found
318 DO380-OCP4.4-en-2-20200803
Chapter 9 | Managing Cluster Monitoring and Metrics
2. Identify storage files used by Prometheus.
2.1. Connect to the prometheus container in one of the prometheus-k8s pods in the
openshift-monitoring namespace.
[student@workstation ~]$ oc rsh -n openshift-monitoring -c prometheus \
> prometheus-k8s-0
sh-4.2$
2.2. List files in the /prometheus/ directory. Although the Prometheus pods use
ephemeral storage, the pods are stopped and restarted in the classroom environment
rather than being recreated. The result is that you might see directories created on a
previous day.
sh-4.2$ ls -l /prometheus/
total 20
drwxr-sr-x. 3 1000360000 1000360000 68 Jun 18 17:05 01EB469DE6GQC85YE72ZGXRZ6F
-rw-r--r--. 1 1000360000 1000360000 20001 Jun 18 18:15 queries.active
drwxr-sr-x. 4 1000360000 1000360000 156 Jun 18 18:15 wal
Note
Although your output displays the queries.active file and the wal directory, all
other output will vary.
2.3. Check to see if the /prometheus/ directory is a mount point.
sh-4.2$ df -h /prometheus/
Filesystem Size Used Avail Use% Mounted on
/dev/mapper/coreos-luks-root-nocrypt 40G 5.9G 34G 15% /prometheus
Although the /prometheus/ directory is a mount point, the volume definition in the
prometheus-k8s stateful set uses emptyDir to indicate that it is a placeholder.
The /dev/mapper/coreos-luks-root-nocrypt devices indicates ephemeral
storage on the node.
Note
The values of your output for the Used, Avail, and Use% columns might be
different.
2.4. Exit from the pod.
sh-4.2$ exit
3. Delete the Prometheus pods and notice how the new pods do not contain the files
previously identified.
3.1. Delete the prometheus-k8s pods in the openshift-monitoring namespace.
DO380-OCP4.4-en-2-20200803 319
Chapter 9 | Managing Cluster Monitoring and Metrics
[student@workstation ~]$ oc delete pods -l app=prometheus \
> -n openshift-monitoring
pod "prometheus-k8s-0" deleted
pod "prometheus-k8s-1" deleted
3.2. After the new prometheus-k8s-0 pod transitions to a running state, list the
contents of the /prometheus/ directory in the prometheus-k8s-0 pod. Aside
from the /prometheus/wal/ directory, the previously listed directories no longer
exist.
[student@workstation ~]$ oc rsh -n openshift-monitoring -c prometheus \
> prometheus-k8s-0 ls -l /prometheus/
total 20
-rw-r--r--. 1 1000030000 1000030000 20001 May 21 14:50 queries.active
drwxr-sr-x. 2 1000030000 1000030000 38 May 21 14:49 wal
4. Configure the cluster monitoring operator so that both Prometheus and Alertmanager use
persistent storage.
4.1. Identify the existing storage class for the cluster. You will use this storage class name
for Prometheus and Alertmanager.
[student@workstation ~]$ oc get storageclass
NAME PROVISIONER RECLAIMPOLICY ...
nfs-storage (default) nfs-storage-provisioner Delete ...
Warning
Using NFS to back persistent volumes used by core services, such as Prometheus, is
not recommended. Block storage is recommended for Prometheus.
The classroom environment uses NFS storage for simplicity.
4.2. Change to the ~/DO380/labs/monitor-storage/ directory.
[student@workstation ~]$ cd ~/DO380/labs/monitor-storage/
4.3. Modify the persistent-storage.yml file.
[student@workstation monitor-storage]$ vim persistent-storage.yml
4.4. Ensure that the file matches the following bold lines and then save the file.
prometheusK8s:
retention: 15d
volumeClaimTemplate:
spec:
storageClassName: nfs-storage
resources:
requests:
320 DO380-OCP4.4-en-2-20200803
Chapter 9 | Managing Cluster Monitoring and Metrics
storage: 40Gi
alertmanagerMain:
volumeClaimTemplate:
spec:
storageClassName: nfs-storage
resources:
requests:
storage: 20Gi
Important
This configuration indicates that each Prometheus pod requests a 40GB persistent
volume claim, and each Alertmanager pod requests a 20GB persistent volume
claim.
Although the storage class creates each persistent volume claim with the desired
size, the classroom environment only allocates 40GB in total to the NFS server
(utility.lab.example.com).
4.5. Apply the configuration using the oc create command.
[student@workstation monitor-storage]$ oc create configmap \
> cluster-monitoring-config --from-file config.yaml=persistent-storage.yml \
> -n openshift-monitoring
configmap/cluster-monitoring-config created
5. Verify that Prometheus uses persistent storage.
5.1. List the persistent volume claims using the app=prometheus label in the
openshift-monitoring namespace. The nfs-storage storage class creates the
persistent volume claims dynamically, and this process might take up to a minute to
complete. After the persistent volume claims appear, press Ctrl+C to exit the watch
command.
[student@workstation monitor-storage]$ watch oc get pvc -n openshift-monitoring \
> -l app=prometheus
NAME STATUS VOLUME ...
prometheus-k8s-db-prometheus-k8s-0 Bound pvc-d029d039-d26c-4e68-a743-3b3...
prometheus-k8s-db-prometheus-k8s-1 Bound pvc-e804510d-7e38-4687-b3ba-4c5...
5.2. Verify that the prometheus-k8s-db-prometheus-k8s-0 persistent volume claim
is mounted to the /prometheus/ directory in the prometheus-k8s-0 pod. The /
exports/ directory on the NFS server (listed under Filesystem) includes the name
of the namespace, the name of the persistent volume claim, and the name of the
associated volume.
DO380-OCP4.4-en-2-20200803 321
Chapter 9 | Managing Cluster Monitoring and Metrics
[student@workstation monitor-storage]$ oc rsh -n openshift-monitoring \
> -c prometheus prometheus-k8s-0 df -h /prometheus/
Filesystem Size Used Avail Use%
Mounted on
192.168.50.254:/exports/openshift-monitoring-prometheus-k8s-db-prometheus-k8s-0
-pvc-d029d039-d26c-4e68-a743-3b3586dd00e9/prometheus-db 40G 1.1G 39G 3%
/prometheus
6. Verify that Alertmanager uses persistent storage.
6.1. List the persistent volume claims using the app=alertmanager label in the
openshift-monitoring namespace.
[student@workstation monitor-storage]$ oc get pvc -n openshift-monitoring \
> -l app=alertmanager
NAME STATUS VOLUME ...
alertmanager-main-db-alertmanager-main-0 Bound pvc-4f168ac5-2161-4daa-a8...
alertmanager-main-db-alertmanager-main-1 Bound pvc-51d64a64-4abc-43c8-af...
alertmanager-main-db-alertmanager-main-2 Bound pvc-404420c4-a7e3-4ff4-b1...
6.2. Verify that the alertmanager-main-db-alertmanager-main-0 persistent
volume claim is mounted to the /alertmanager/ directory in the alertmanagermain-
0 pod. The /exports/ directory on the NFS server (listed under Filesystem)
includes the name of the namespace, the name of the persistent volume claim, and
the name of the associated volume.
[student@workstation monitor-storage]$ oc rsh -n openshift-monitoring \
> -c alertmanager alertmanager-main-0 df -h /alertmanager/
Filesystem Size Used Avail
Use% Mounted on
192.168.50.254:/exports/openshift-monitoring-alertmanager-main-db-alertmanagermain-
0-pvc-4f168ac5-2161-4daa-a896-98233b32430e/alertmanager-db 40G 981M 40G
3% /alertmanager
7. Change to the /home/student/ directory.
[student@workstation monitor-storage]$ cd
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab monitor-storage finish
This concludes the section.
322 DO380-OCP4.4-en-2-20200803
Chapter 9 | Managing Cluster Monitoring and Metrics
Summary
In this chapter, you learned:
• OpenShift 4 installs the cluster monitoring stack by default.
• Prometheus rules generate alerts for potential cluster problems.
• Alertmanager can send alerts to external locations in addition to displaying alerts in the
OpenShift web console.
• Grafana graphically displays metrics collected by Prometheus and can be used to identify trends
and problems for the cluster, nodes, and namespaces.
• Configure Prometheus and Alertmanager to use persistent storage to prevent data loss.
DO380-OCP4.4-en-2-20200803 323
324 DO380-OCP4.4-en-2-20200803
Chapter 10
Provisioning and Inspecting
Cluster Logging
Goal Deploy and query cluster-wide logging, and
diagnose common issues using tools.
Objectives • Deploy and configure cluster logging.
• Inspect Openshift and application logs using
Lucene queries in the Kibana UI.
• Create charts to visualize logs with Kibana.
• Diagnose cluster logging problems with
debugging and monitoring tools.
Sections • Deploying Cluster Logging (and Guided
Exercise)
• Querying Cluster Logs with Kibana (and Guided
Exercise)
• Visualizing Cluster Logs with Kibana (and
Guided Exercise)
• Diagnosing Cluster Logging Problems (and
Guided Exercise)
DO380-OCP4.4-en-2-20200803 325
Chapter 10 | Provisioning and Inspecting Cluster Logging
Deploying Cluster Logging
Objectives
After completing this section, you should be able to deploy and configure cluster logging.
Discussing Cluster Logging
The main benefit of cluster logging is the aggregation of all the logs from the pods and node of
an OpenShift cluster to a centralized location. Centralized logging allows for easier searching,
visualizing, and reporting of the data.
Cluster logging components
logStore The logStore is the Elasticsearch cluster that
• Stores the logs into indices
• Provides RBAC access of the logs
• Provides data redundancy.
collection Implemented with Fluentd, the collector collects node and application
logs, adds pod and namespace metadata, and stores them in the
logStore. The collector is a DaemonSet, so there will be a Fluentd pod
on each node.
visualization The centralized, web UI from Kibana displays the logs and provides a
way to query and chart the aggregated data.
curation The Curator removes old logs from the Elasticsearch indices. The
Curator deploys as a CronJob.
event routing The Event Router monitors the OpenShift events API and sends the
events to STDOUT so the collector can forward them to the logStore.
The events from OpenShift are stored in the .operations index in
Elasticsearch.
Installing the Elasticsearch Operator
The Elasticsearch Operator handles the creation of and updates to the Elasticsearch cluster
defined in the Cluster Logging Custom Resource. Each node in the Elasticsearch cluster is
deployed with a PVC named and managed by the Elasticsearch Operator. A unique Deployment is
created per Elasticsearch node to ensure that each Elasticsearch node has a storage volume of its
own.
The Elasticsearch Operator needs to be installed in a namespace other than openshiftoperators
to avoid possible conflicts with metrics from other Community Operators. Create and
use the openshift-operators-redhat namespace for the Elasticsearch Operator.
326 DO380-OCP4.4-en-2-20200803
Chapter 10 | Provisioning and Inspecting Cluster Logging
apiVersion: v1
kind: Namespace
metadata:
name: openshift-operators-redhat
annotations:
openshift.io/node-selector: ""
labels:
openshift.io/cluster-monitoring: "true"
Create an OperatorGroup to install the operator in all namespaces and a Subscription to have the
openshift-operators-redhat namespace subscribed to the operator.
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
name: openshift-operators-redhat
namespace: openshift-operators-redhat
spec: {}
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
name: "elasticsearch-operator"
namespace: "openshift-operators-redhat"
spec:
channel: "4.4"
installPlanApproval: "Automatic"
source: "redhat-operators"
sourceNamespace: "openshift-marketplace"
name: "elasticsearch-operator"
Create the RBAC objects to grant Prometheus permission to access the openshiftoperators-
redhat namespace.
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
name: prometheus-k8s
namespace: openshift-operators-redhat
rules:
- apiGroups:
- ""
resources:
- services
- endpoints
- pods
verbs:
- get
- list
- watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
DO380-OCP4.4-en-2-20200803 327
Chapter 10 | Provisioning and Inspecting Cluster Logging
metadata:
name: prometheus-k8s
namespace: openshift-operators-redhat
roleRef:
apiGroup: rbac.authorization.k8s.io
kind: Role
name: prometheus-k8s
subjects:
- kind: ServiceAccount
name: prometheus-k8s
namespace: openshift-operators-redhat
---
Verify that operator is available in each namespace.
[user@host ~]$ oc get csv -A
NAMESPACE NAME
DISPLAY VERSION REPLACES PHASE
default elasticsearchoperator.
4.4.0-202004261927 Elasticsearch Operator 4.4.0-202004261927
Succeeded
kube-node-lease elasticsearchoperator.
4.4.0-202004261927 Elasticsearch Operator 4.4.0-202004261927
Succeeded
kube-public elasticsearchoperator.
4.4.0-202004261927 Elasticsearch Operator 4.4.0-202004261927
Succeeded
kube-system
...output omitted...
Installing the Cluster Logging Operator
The Cluster Logging Operator creates components detailed in the Cluster Logging Custom
Resource and updated the deployment upon any changes to the CR.
Similar to the Elasticsearch Operator, the Cluster Logging Operator requires a Namespace,
OperatorGroup, and a Subscription.
apiVersion: v1
kind: Namespace
metadata:
name: openshift-logging
annotations:
openshift.io/node-selector: ""
labels:
openshift.io/cluster-logging: "true"
openshift.io/cluster-monitoring: "true"
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
name: cluster-logging
namespace: openshift-logging
spec:
328 DO380-OCP4.4-en-2-20200803
Chapter 10 | Provisioning and Inspecting Cluster Logging
targetNamespaces:
- openshift-logging
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
name: cluster-logging
namespace: openshift-logging
spec:
channel: "4.4"
name: cluster-logging
source: redhat-operators
sourceNamespace: openshift-marketplace
---
Confirm the installation.
[user@host ~]$ oc get csv -n openshift-logging
NAME DISPLAY VERSION
REPLACES PHASE
clusterlogging.4.4.0-202004261927 Cluster Logging
4.4.0-202004261927 Succeeded
elasticsearch-operator.4.4.0-202004261927 Elasticsearch Operator
4.4.0-202004261927 Succeeded
Deploying a Cluster Logging Instance
The Cluster Logging Custom Resource defines the configuration for an instance of the cluster
logging components and provides the configuration for the components.
Configuring Elasticsearch
There are important things to consider when configuring the Elasticsearch component.
• For availability and scalability, a minimum of three nodes are recommended. The first three
nodes of the Elasticsearch cluster are created with master, client, and data roles. Additional
nodes are provisioned as data nodes, with client and data roles.
• Elasticsearch can use persistent or ephemeral storage, but persistent storage is recommended
to not lose data. Fast storage, like SSDs, is preferred over spinning disks. Use dedicated local
storage instead of NFS, SMB, or Amazon EBS. Consider the logging requirements of the
applications in the OpenShift cluster as well as the OpenShift nodes when sizing the storage for
Elasticsearch.
• The redundancy policy determines how Elasticsearch shards are replicated across data nodes in
the cluster.
FullRedundancy Elasticsearch copies the shards for each index to every data
node in the cluster. Fully replicated shards provide the most
redundancy to protect from accidental data loss.
MultipleRedundancy The primary shards for each index are replicated to half
of the data nodes. This provides a good balance between
performance and redundancy.
DO380-OCP4.4-en-2-20200803 329
Chapter 10 | Provisioning and Inspecting Cluster Logging
SingleRedundancy Each primary shard is copied once to another node. If at
least two data nodes remain available, the logs will be
recoverable. If the Elasticsearch cluster has 5 or more
nodes, SingleRedundancy is more performant than
MultipleRedundancy.
ZeroRedundancy Shards are not replicated. Data loss could happen if a node
fails.
Configuring the Curator
The curator is a CronJob and can be scheduled in cron format. Determine how frequently the
logs in your cluster need trimming and update the schedule in the Cluster Logging Custom
Resource. The job that runs will remove the logs that are old enough to be removed. For example,
the following schedule will run the curator CronJob everyday at 03:30.
curation:
type: "curator"
curator:
schedule: "30 3 * * *"
Configure the default or per-project retention period by editing the Curator ConfigMap. The
settings in the config.yaml section of the ConfigMap determine how long to hold on to logs.
Use the examples to customize the retention periods.
[user@host ~]$ oc edit configmaps curator -n openshift-logging
...output omitted...
config.yaml: |
# Logging example curator config file
# uncomment and use this to override the defaults from env vars
#.defaults:
# delete:
# days: 30
# to keep ops logs for a different duration:
#.operations:
# delete:
# weeks: 8
# example for a normal project
#myapp:
# delete:
# weeks: 1
...output omitted...
Example Cluster Logging Instance
apiVersion: "logging.openshift.io/v1"
kind: "ClusterLogging"
metadata:
name: "instance"
330 DO380-OCP4.4-en-2-20200803
Chapter 10 | Provisioning and Inspecting Cluster Logging
namespace: "openshift-logging"
spec:
managementState: "Managed"
logStore:
type: "elasticsearch"
elasticsearch:
nodeCount: 3
nodeSelector:
node-role.kubernetes.io/infra: ''
storage:
storageClassName: "local-ssd-fs"
size: 50G
redundancyPolicy: "MultipleRedundancy"
visualization:
type: "kibana"
kibana:
nodeSelector:
node-role.kubernetes.io/infra: ''
replicas: 1
curation:
type: "curator"
curator:
nodeSelector:
node-role.kubernetes.io/infra: ''
schedule: "30 3 * * *"
collection:
logs:
type: "fluentd"
fluentd: {}
Note
All of the components are targeting dedicated infrastructure nodes.
Confirm the cluster logging components statuses with oc get clusterlogging instance -
o yaml.
[user@host ~]$ oc get clusterlogging instance -o yaml
apiVersion: logging.openshift.io/v1
kind: ClusterLogging
...output omitted...
status:
collection:
logs:
fluentdStatus:
daemonSet: fluentd
...output omitted...
pods:
failed: []
notReady: []
ready:
- fluentd-bfzjr
- fluentd-hjstw
DO380-OCP4.4-en-2-20200803 331
Chapter 10 | Provisioning and Inspecting Cluster Logging
- fluentd-hk9fn
- fluentd-kkpx6
- fluentd-vp5zv
- fluentd-xrjgg
- fluentd-zzkds
curation:
curatorStatus:
- clusterCondition:
curator-1588974900-8rhs7:
- lastTransitionTime: "2020-05-08T21:56:07Z"
reason: Completed
status: "True"
type: ContainerTerminated
cronJobs: curator
schedules: '*/5 * * * *'
suspended: false
logStore:
elasticsearchStatus:
- cluster:
activePrimaryShards: 15
activeShards: 15
initializingShards: 0
numDataNodes: 3
numNodes: 3
pendingTasks: 0
relocatingShards: 0
status: green
unassignedShards: 0
...output omitted...
pods:
client:
failed: []
notReady: []
ready:
- elasticsearch-cdm-qvqu0hka-1-84ff8467b9-5tprt
- elasticsearch-cdm-qvqu0hka-2-8db6c9bcc-w6gd5
- elasticsearch-cdm-qvqu0hka-3-7b795ddd5-xx7lm
data:
failed: []
notReady: []
ready:
- elasticsearch-cdm-qvqu0hka-1-84ff8467b9-5tprt
- elasticsearch-cdm-qvqu0hka-2-8db6c9bcc-w6gd5
- elasticsearch-cdm-qvqu0hka-3-7b795ddd5-xx7lm
master:
failed: []
notReady: []
ready:
- elasticsearch-cdm-qvqu0hka-1-84ff8467b9-5tprt
- elasticsearch-cdm-qvqu0hka-2-8db6c9bcc-w6gd5
- elasticsearch-cdm-qvqu0hka-3-7b795ddd5-xx7lm
shardAllocationEnabled: all
visualization:
kibanaStatus:
- deployment: kibana
332 DO380-OCP4.4-en-2-20200803
Chapter 10 | Provisioning and Inspecting Cluster Logging
pods:
failed: []
notReady: []
ready:
- kibana-8458b84b7c-258q7
replicaSets:
- kibana-8458b84b7c
replicas: 1
Installing the Event Router
By default, the Kubernetes events are stored in etcd. The Event Router monitors the events API
and prints them to STDOUT so Fluentd can collect them and send them to Elasticsearch. The
events are stored in the .operations index in Elasticsearch.
kind: Template
apiVersion: v1
metadata:
name: eventrouter-template
annotations:
description: "A pod forwarding kubernetes events to cluster logging stack."
tags: "events,EFK,logging,cluster-logging"
objects:
- kind: ServiceAccount
apiVersion: v1
metadata:
name: eventrouter
namespace: ${NAMESPACE}
- kind: ClusterRole
apiVersion: v1
metadata:
name: event-reader
rules:
- apiGroups: [""]
resources: ["events"]
verbs: ["get", "watch", "list"]
- kind: ClusterRoleBinding
apiVersion: v1
metadata:
name: event-reader-binding
subjects:
- kind: ServiceAccount
name: eventrouter
namespace: ${NAMESPACE}
roleRef:
kind: ClusterRole
name: event-reader
- kind: ConfigMap
apiVersion: v1
metadata:
name: eventrouter
namespace: ${NAMESPACE}
data:
config.json: |-
DO380-OCP4.4-en-2-20200803 333
Chapter 10 | Provisioning and Inspecting Cluster Logging
{
"sink": "stdout"
}
- kind: Deployment
apiVersion: apps/v1
metadata:
name: eventrouter
namespace: ${NAMESPACE}
labels:
component: eventrouter
logging-infra: eventrouter
provider: openshift
spec:
selector:
matchLabels:
component: eventrouter
logging-infra: eventrouter
provider: openshift
replicas: 1
template:
metadata:
labels:
component: eventrouter
logging-infra: eventrouter
provider: openshift
name: eventrouter
spec:
serviceAccount: eventrouter
containers:
- name: kube-eventrouter
image: ${IMAGE}
imagePullPolicy: IfNotPresent
resources:
limits:
memory: ${MEMORY}
requests:
cpu: ${CPU}
memory: ${MEMORY}
volumeMounts:
- name: config-volume
mountPath: /etc/eventrouter
volumes:
- name: config-volume
configMap:
name: eventrouter
parameters:
- name: IMAGE
displayName: Image
value: "registry.redhat.io/openshift4/ose-logging-eventrouter:latest"
- name: MEMORY
displayName: Memory
value: "128Mi"
- name: CPU
displayName: CPU
value: "100m"
334 DO380-OCP4.4-en-2-20200803
Chapter 10 | Provisioning and Inspecting Cluster Logging
- name: NAMESPACE
displayName: Namespace
value: "openshift-logging"
References
For more information, refer to the Understanding cluster logging and OpenShift
Container Platform, About deploying cluster logging, Deploying cluster logging, and
Configuring your cluster logging deployment chapters in the Red Hat OpenShift
Container Platform 4.4 Logging documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/logging/index
Tune for indexing speed
https://www.elastic.co/guide/en/elasticsearch/reference/current/tune-forindexing-
speed.html#_use_faster_hardware
DO380-OCP4.4-en-2-20200803 335
Chapter 10 | Provisioning and Inspecting Cluster Logging
Guided Exercise
Deploying Cluster Logging
In this exercise you will deploy and configure cluster logging.
Outcomes
You should be able to:
• Install the Elasticsearch and Cluster Logging operators.
• Install the Event Router.
• Install and configure Cluster Logging.
Before You Begin
This exercise depends upon extra workers being available. Guided Exercise: Creating Custom
Machine Config Pools describes how to add the extra infra nodes.
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise. The command ensures that extra OpenShift nodes are available to
use.
[student@workstation ~]$ lab logging-deploy start
Instructions
1. Review the provided Elasticsearch Operator, Cluster Logging Operator, Cluster Logging
instance, and Event Router Operator manifests.
1.1. Change to the ~/DO380/labs/logging-deploy/ directory.
[student@workstation ~]$ cd ~/DO380/labs/logging-deploy/
Note
The Elasticsearch cluster is being provisioned in a minimal setup to accommodate
the classroom. A standard deployment would have at least three Elasticsearch
nodes, each with at least 16Gb memory.
2. Update the Cluster Logging instance manifest cl-minimal.yml.
2.1. Set the redundancyPolicy to MultipleRedundancy
2.2. Set the Curator 'schedule' to run everyday at 02:20.
2.3. The updated manifest should look like the following.
336 DO380-OCP4.4-en-2-20200803
Chapter 10 | Provisioning and Inspecting Cluster Logging
apiVersion: "logging.openshift.io/v1"
kind: "ClusterLogging"
metadata:
name: "instance"
namespace: "openshift-logging"
spec:
managementState: "Managed"
logStore:
type: "elasticsearch"
elasticsearch:
nodeCount: 2
nodeSelector:
node-role.kubernetes.io/infra: ''
storage:
storageClassName: "local-ssd-fs"
size: 20G
redundancyPolicy: "MultipleRedundancy"
resources:
limits:
memory: "8Gi"
requests:
cpu: "1"
memory: "8Gi"
visualization:
type: "kibana"
kibana:
nodeSelector:
node-role.kubernetes.io/infra: ''
replicas: 1
curation:
type: "curator"
curator:
nodeSelector:
node-role.kubernetes.io/infra: ''
schedule: "20 2 * * *"
collection:
logs:
type: "fluentd"
fluentd: {}
3. Run the provided Playbook to install Cluster Logging.
3.1. Log into your OpenShift cluster as the admin user.
[student@workstation logging-deploy]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
3.2. Execute the Playbook.
DO380-OCP4.4-en-2-20200803 337
Chapter 10 | Provisioning and Inspecting Cluster Logging
[student@workstation logging-deploy]$ ansible-playbook cluster-logging.yml -v
Using /etc/ansible/ansible.cfg as config file
PLAY [Install Cluster Logging] *************************************************
TASK [Create objects from the manifests] ***************************************
changed: [localhost] => (item=logging-operator.yml) => changed=true
ansible_loop_var: item
item: logging-operator.yml
result:
...output omitted...
PLAY RECAP *********************************************************************
localhost : ok=1 changed=1 unreachable=0 failed=0
skipped=0 rescued=0 ignored=0
4. Confirm the installation.
After a couple minutes the Elasticsearch Operator will be installed in all namespaces.
[student@workstation logging-deploy]$ oc get csv -A
NAMESPACE NAME
DISPLAY VERSION REPLACES PHASE
default elasticsearchoperator.
4.4.0-202006160135 Elasticsearch Operator 4.4.0-202006160135
Succeeded
kube-node-lease elasticsearchoperator.
4.4.0-202006160135 Elasticsearch Operator 4.4.0-202006160135
Succeeded
kube-public elasticsearchoperator.
4.4.0-202006160135 Elasticsearch Operator 4.4.0-202006160135
Succeeded
...output omitted...
Verify the Cluster Logging Operator succeeded in the openshift-logging Namespace.
[student@workstation logging-deploy]$ oc get csv -n openshift-logging
NAME DISPLAY VERSION
REPLACES PHASE
clusterlogging.4.4.0-202006160135 Cluster Logging
4.4.0-202006160135 Succeeded
elasticsearch-operator.4.4.0-202006160135 Elasticsearch Operator
4.4.0-202006160135 Succeeded
The Cluster Logging components and the Event Router will take a few minutes to deploy.
Watch the components to validate everything deploys.
[student@workstation logging-deploy]$ watch oc get deployment,pod,pvc,svc,route \
> -n openshift-logging
338 DO380-OCP4.4-en-2-20200803
Chapter 10 | Provisioning and Inspecting Cluster Logging
5. Log into the Kibana UI.
• Use oc to discover the route oc get routes -n openshift-logging or click
Monitoring ® Logging in the OpenShift web console.
• On the Authorize Access page click Allow selected permissions.
6. Change to the /home/student/ directory.
[student@workstation logging-deploy]$ cd
Note
If the Elasticsearch Cluster does not come up correctly or the Kibana UI shows
errors with Elasticsearch, run the same playbook with k8s_state set to absent.
ansible-playbook cluster-logging.yml -v -e k8s_state=absent
The removal of the logging components takes a couple minutes. Run the watch
command from step 4 to validate that the components are removed before
rerunning the playbook from step 3.
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab logging-deploy finish
This concludes the section.
DO380-OCP4.4-en-2-20200803 339
Chapter 10 | Provisioning and Inspecting Cluster Logging
Querying Cluster Logs with Kibana
Objectives
After completing this section, you should be able to inspect OpenShift and application logs using
Lucene queries in the Kibana UI.
Introducing the Kibana UI
There are two ways to navigate to Kibana:
• Use oc to discover the route oc get routes -n openshift-logging.
• Click Monitoring ® Logging in the OpenShift web console. After logging in, the Discover
page displays. This page is the primary screen for querying Elasticsearch for logs.
Search the logs using a Lucene query.
Add a filter to narrow the search based on the existence or value of a field.
Select the index pattern to filter the query by project.
Set the time range to a relative range, such as Last 15 minutes, or an absolute time
range.
A time series chart of the count of log records that match the search query.
Table of log records that match the search query.
Selecting Indexes
• Elasticsearch organizes logs in indices. These indices are organized by project and date.
• Index names follow the format .operations.YEAR.MONTH.DAY and
.project.PROJECT.NAMESPACE-ID.YEAR.MONTH.DAY
340 DO380-OCP4.4-en-2-20200803
Chapter 10 | Provisioning and Inspecting Cluster Logging
• Logs from projects that begin with openshift- or kube- automatically sort into the
.operations.* indices. Other logs, such as from application workloads, sort into the
.project.* indices.
• When querying for logs in Kibana, first select the correct index pattern for your query.
• Elasticsearch applies role-based access control (RBAC) rules to the the indices based on
OpenShift project permissions.
Selecting Table Fields
• Elasticsearch stores OpenShift logs as JSON documents. A JSON document is similar to a table
row in a relational database.
• The default Kibana Discover view displays a table of logs with Time and _source fields.
• Other fields can be added by either:
– Clicking add next to an entry in the Available Fields list.
– Expanding a log record by clicking the arrow next to it, and then clicking the table icon next to
the field to add.
Querying Logs
Introducing Lucene Queries
• Lucene is a search engine.
• Type any term or terms to search the logs within the selected index pattern. For example, type
dns to find all records that mention the text string dns in any of the indexed text fields.
• Wrap terms that include symbols such as - in double-quotes so that Lucene parses them as a
single term. For example, "openshift-apiserver-operator".
• Type multiple terms separated by a space to search for multiple terms. However, remember that
this is a search and all results may not contain all terms.
• Use the + and - symbols to identify that fields must or must not match, respectively. For
example, +dns -api searches for all records that container the term dns but not api.
• Log data is structured using a data model. Lucene queries specify fields with the colon
character in the following format: FIELD:_TERM_. For example, level:info message:dns.
Filtering Queries
Fluentd adds OpenShift metadata to pod logs to provide fields for querying and filtering, for
example:
• hostname
• kubernetes.labels.app
• kubernetes.container_name
• kubernetes.namespace_name
• kubernetes.pod_name
• level
DO380-OCP4.4-en-2-20200803 341
Chapter 10 | Provisioning and Inspecting Cluster Logging
The value of the originally logged message itself is in the message field.
For example, enter the following query to search for all messages that contain the term elected
in the openshift-etcd namespace.
+kubernetes.namespace_name:openshift-etcd +message:elected
Optionally, use the Add a filter button to filter the results of a query. This displays a form to filter
based on values or the existence of fields.
Finding OpenShift Event Logs
The event router pod watches OpenShift and sends events to the cluster logging system.
Find these events with the following query under the .operations index:
kubernetes.event:*.
Event logs contain useful fields such as:
• kubernetes.event.involvedObject.name
• kubernetes.event.involvedObject.namespace
• kubernetes.event.reason
• kubernetes.event.verb
References
Query string syntax
https://www.elastic.co/guide/en/elasticsearch/reference/5.6/query-dsl-querystring-
query.html#query-string-syntax
342 DO380-OCP4.4-en-2-20200803
Chapter 10 | Provisioning and Inspecting Cluster Logging
Guided Exercise
Querying Cluster Logs with Kibana
In this exercise you will query logs from the Kibana graphical user interface (GUI).
Outcomes
You should be able to use Lucene queries and filters to find logs.
Before You Begin
Before starting this exercise, you must set up cluster logging by completing the Guided
Exercise: Deploying Cluster Logging.
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise. The command deploys OpenShift pods that frequently generate
logs.
[student@workstation ~]$ lab logging-query start
Instructions
1. Navigate to the Kibana GUI and log in as the admin user.
1.1. Navigate to https://console-openshiftconsole.
apps.ocp4.example.com in Firefox.
1.2. From the OpenShift web console, navigate to Monitoring ® Logging, and then
click Log in with OpenShift.
2. Query the logs for messages from the machine-api-operator container. The lab
start script restarted this container.
2.1. Click New, and then ensure that the operations.* index pattern is selected.
2.2. Type kubernetes.container_name:"machine-api-operator" in the query
input field, and then press +. If no results are found, click the time range selector and
pick a larger range such as Last 1 hour.
2.3. In the Available Fields list, click add next to the message field. The machine API
operator logs typically output messages about syncing status.
2.4. Locate the log messages from the container startup, such as successfully
acquired lease openshift-machine-api/machine-api-operator and
Starting Machine API Operator.
3. View the event log from the event router to see events related to the restarted pod.
3.1. Click New, and then ensure that the operations.* index pattern is selected.
DO380-OCP4.4-en-2-20200803 343
Chapter 10 | Provisioning and Inspecting Cluster Logging
3.2. Type kubernetes.event:* in the query input field and press +. If no results are
returned, then click the time range selector and pick a larger range, such as Last 1
hour.
3.3. Click the arrow next to a log result to display the table of fields. Locate a log
record where the kubernetes.event.involvedObject.namespace field is
openshift-machine-api, and then click the Filter for value magnifying
glass icon for that field.
3.4. Click the Toggle column in table icon next to the message field.
3.5. Click the arrow next to the log time to collapse result listing.
3.6. Locate the Created container and Started container log messages. If they
are not visible in the listing, then select a larger time range.
4. In the logging-query project, locate server error logs marked with a 500 status code.
4.1. Click New, and then select the project.* index pattern.
4.2. Click Add a filter. Create a filter in the kubernetes.namespace_name field.
Select the is operator and a value of logging-query. Click Save to view the
filtered results.
4.3. In the Lucene query text field, type message:500 to list all messages with the term
"500".
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab logging-query finish
This concludes the section.
344 DO380-OCP4.4-en-2-20200803
Chapter 10 | Provisioning and Inspecting Cluster Logging
Visualizing Cluster Logs with Kibana
Objectives
After completing this section, you should be able to create charts to visualize logs with Kibana.
Creating Visualizations
The categories for visualizations are Basic Charts, Data, Maps, Time Series, and Other. Hover over
a visualization type to view a brief description of it. For most visualizations, you must provide the
search source by selecting an index or a saved query.
Creating a Pie Chart
• From the Kibana GUI, click Visualize, and then click + to create a new visualization.
• Click the pie chart under Basic Charts.
• Choose the project.* index.
• Optionally, add a query to limit the available data for the chart.
• Click the Split Slices bucket type.
• Choose Filters for the Aggregation.
• For Filter 1, input 200, then click Add Filter.
• Add 2 more filters for 404 and 500.
• Click the Apply changes arrow icon to update the visualization.
DO380-OCP4.4-en-2-20200803 345
Chapter 10 | Provisioning and Inspecting Cluster Logging
Figure 10.1: The Kibana Visualize screen with a Pie Chart
• In the upper right, click Save, type in a name, and then click Save under the input field.
Creating Time Series with Timelion
Timelion produces a time series to visualize your data over time. Timelion provides functions to
pull data from multiple datasources and to transform the data. Click Docs in the upper right for a
list of functions and how to use them.
Creating a Sample Time Series
Important
Timelion queries should be typed in a single line. They are presented here as
multiline statements for readability.
• From the Kibana GUI, click Visualize and then click New to create a new time series.
– The default expression is .es(*), which uses the Elasticsearch function to pass Lucene
queries to Elasticsearch.
This gathers all the data from the default index in Elasticsearch.
• Add a basic expression
– Edit the expression to be .es('dns').
346 DO380-OCP4.4-en-2-20200803
Chapter 10 | Provisioning and Inspecting Cluster Logging
This expression shows the count of logs with dns in them over the currently selected time
range.
• Click Add to add another time series to the current window.
• Add more detailed expression
– Edit the expression to be:
.es(index=project.*, q='+kubernetes.namespace_name:logging-query +message:200')
This expression sets the index to project.*, and then queries it for messages that contain
200 in the logging-query namespace.
• Add another time series and overlay multiple expressions
– Edit the expression to be:
.es('+kubernetes.namespace_name:logging-query +message:200'),
.es('+kubernetes.namespace_name:logging-query +message:404'),
.es('+kubernetes.namespace_name:logging-query +message:500')
This expression shows the count of messages from the logging-query namespace that
have either 200, 404, or 500 in them.
• Adjust the previous expression with more functions.
– Get the percentage of each message compared with the rest with by adding division and
multiplication to the previous expression. For example:
.es('+kubernetes.namespace_name:logging-query +message:200')
.divide(.es('+kubernetes.namespace_name:logging-query +message:*')).multiply(100),
.es('+kubernetes.namespace_name:logging-query +message:500')
.divide(.es('+kubernetes.namespace_name:logging-query +message:*')).multiply(100),
.es('+kubernetes.namespace_name:logging-query +message:404')
.divide(.es('+kubernetes.namespace_name:logging-query +message:*')).multiply(100)
This expression divides each Elasticsearch query by the total number of logs for that same
namespace and multiplies by 100 to get the percentage.
• Add another time series and change the display of the chart.
– Edit the expression to be:
.es(*).points(), .es(200).bars().color(#f33)
This expression shows the count of all logs as points and the logs that contain 200 as bright
red bars.
• Add another time series to demonstrate labels and offsetting data.
– Edit the expression to be:
DO380-OCP4.4-en-2-20200803 347
Chapter 10 | Provisioning and Inspecting Cluster Logging
.es(index=project.*, q='admin').label(current),
.es(index=project.*, q=cart, offset=-10m).label('previous')
This expression labels the query for admin as current and the query for cart as previous.
The cart expression is offset by -10 minutes compared to the admin expression.
Figure 10.2: The Kibana Timelion screen
• Finally, save the charts as either a Timelion sheet or as a Kibana dashboard panel.
– Click Save in the Kibana toolbar, choose a save option, provide a name, and then click Save.
References
Timelion Tutorial - From Zero to Hero
https://www.elastic.co/blog/timelion-tutorial-from-zero-to-hero
348 DO380-OCP4.4-en-2-20200803
Chapter 10 | Provisioning and Inspecting Cluster Logging
Guided Exercise
Visualizing Cluster Logs with Kibana
In this exercise you will create visualizations in Kibana.
Outcomes
You should be able to:
• Create a simple Kibana chart.
• Create a time series chart using Timelion.
Before You Begin
Before starting this exercise, you must set up cluster logging by completing the Guided
Exercise: Deploying Cluster Logging.
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise. The command ensures that an application that creates access logs
containing HTTP status codes is deployed.
[student@workstation ~]$ lab logging-visualize start
Instructions
1. Navigate to the Kibana GUI and log in as the admin user.
1.1. Navigate to https://console-openshiftconsole.
apps.ocp4.example.com in Firefox.
1.2. From the OpenShift web console, navigate to Monitoring ® Logging, and then
click Log in with OpenShift.
2. Create a bar chart displaying buckets of the status codes.
2.1. On the Visualize screen, click Create a visualization.
2.2. Click the Horizontal Bar basic chart.
2.3. Select the project.* index pattern.
2.4. Click Add a filter. Create a filter on the kubernetes.container_name field.
Select the is operator and a value of logger. Click Save.
2.5. Click the Split Series bucket type, and then select Filters as the
Aggregation.
2.6. Click Add Filter twice. Enter the following values in the Filter 1, Filter 2, and Filter
3 inputs respectively: message:200, message:40?, message:50?. The ? question
mark symbol matches any character in that position. For example, message:40? will
match 400 and 403.
DO380-OCP4.4-en-2-20200803 349
Chapter 10 | Provisioning and Inspecting Cluster Logging
2.7. Click the Apply changes arrow icon to view the chart.
3. Create a Timelion chart that displays the count of 500 Internal Server Error status
codes per minute.
3.1. Click Timelion to navigate to the timeline chart tool.
3.2. Enter the following Timelion expression to create a graph of logs with a 500 status
code compared with the total count of logs.
.es('+kubernetes.container_name:logger +message:500'),
.es('+kubernetes.container_name:logger +message:*')
Important
Timelion queries should be typed in a single line. They are presented here as
multiline statements for readability.
There will be an error stating "Timelion: Error: in cell #1: input must be a seriesList" if
there are spaces between function calls.
Press + or click the arrow button to render the graph.
3.3. Select an interval of 1m from the interval selection list to the right of the query input
field.
3.4. Click the arrow button to re-render the graph with the updated time interval.
4. Update the Timelion chart to display the percent of 500 status code messages using the
divide() and multiply() functions.
4.1. Modify the Timelion expression to the following text.
.es('+kubernetes.container_name:logger +message:500')
.divide(.es('+kubernetes.container_name:logger +message:*'))
.multiply(100)
4.2. Press + or click the arrow button to render the updated graph.
5. Create a Timelion chart that compares now versus five minutes ago.
5.1. Modify the Timelion expression to match the following.
.es('+kubernetes.container_name:logger +message:500').label(current)
5.2. Click the time range selector, click Relative, update the first set of text fields to read
5 Minutes ago, and then click Go.
5.3. Add a second Elasticsearch es query with a -5m offset labeled previous.
.es('+kubernetes.container_name:logger +message:500').label(current),
.es(q='+kubernetes.container_name:logger +message:500',
offset=-5m).label(previous)
350 DO380-OCP4.4-en-2-20200803
Chapter 10 | Provisioning and Inspecting Cluster Logging
5.4. Press + or click the arrow button to render the updated graph.
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab logging-visualize finish
This concludes the section.
DO380-OCP4.4-en-2-20200803 351
Chapter 10 | Provisioning and Inspecting Cluster Logging
Diagnosing Cluster Logging Problems
Objectives
After completing this section, you should be able to diagnose cluster logging problems with
debugging and monitoring tools.
Discussing Cluster Logging Alerts
• OpenShift has preconfigured cluster logging system alerts. These alerts focus on the Fluentd
log collectors and Elasticsearch log store.
Common Cluster Logging Alerts
Alert Description
FluentdErrorsHigh Fluentd is reporting more than one error per
minute.
FluentdQueueLengthIncreasing The log buffer is growing faster than Fluentd
can process.
ElasticsearchNodeDiskWatermarkReached Available disk space is low.
ElasticsearchProcessCPUHigh The CPU usage is above 90%.
• Examine the alerting rule to see the PromQL expression. This expression can then be executed
on the Monitoring ® Metrics screen.
– From the web console, click the alert and then click the Alerting Rule.
– From the command line, use oc to get the prometheus-k8s-rulefiles-0 ConfigMap in
the openshift-monitoring namespace and output it as YAML. The relevant alerts and
rules are prefixed with logging_.
• Remove the conditional expression to see the underlying metrics.
– For example, the FluentdQueueLengthIncreasing alerting rule expression is
delta(fluentd_output_status_buffer_queue_length[1m]) > 1. Remove the > 1
and click Run Queries to graph the change between the first and last value in a one minute
range vector.
Observing Cluster Logging Metrics
• View key health metrics on the Monitoring ® Metrics dashboards in the web console.
• Select the openshift-logging namespace from the Kubernetes / Compute
Resources / Namespace (Pods).
– The CPU Usage time series graph displays CPU usage by pod. Fluentd CPU will rise with
increased log frequency, size, and complexity.
352 DO380-OCP4.4-en-2-20200803
Chapter 10 | Provisioning and Inspecting Cluster Logging
– The Memory Usage time series graph displays memory usage by pod.
• Select an infra node from the USE Method / Node dashboard to view CPU, memory,
network, and disk utilization and saturation time series graphs for a specific host.
• If Elasticsearch consumes too much disk space, consider:
– Configuring the Curator to delete older logs more aggressively.
– Adding an Elasticsearch node.
• Query the available disk space from the es_fs_path_available_bytes Prometheus metric.
Querying Kibana for Cluster Logging Health
• Use the Kibana UI to view cluster logging component logs.
• Narrow the scope of the log search by creating a filter that only matches logs in the
openshift-logging namespace: kubernetes.namespace_name:"openshiftlogging".
• Search for specific components using the container_name. For example:
kubernetes.container_name:curator.
• Use the visualize feature to count the number of logs per kubernetes.namespace_name field
value.
Discussing Elasticsearch Troubleshooting Tools
• The Elasticsearch image contains tools that can be useful for debugging purposes.
– es_util: Query the Elasticsearch REST endpoint.
– es_cluster_health: Returns a JSON summary of the Elasticsearch cluster health.
[user@host ~]$ POD=elasticsearch-cdm-ovwceoym-1-5599cd9444-mh67p
[user@host ~]$ oc exec -c elasticsearch $POD -- es_util \
> --query=_cat/allocation?pretty
8 30gb 30.1gb 15.8gb 46gb 65 ... elasticsearch-cdm-ovwceoym-1
7 31.6gb 31.7gb 14.2gb 46gb 69 ... elasticsearch-cdm-ovwceoym-2
[user@host ~]$ oc exec -c elasticsearch $POD -- es_util \
> --query=_cat/indices?pretty
green open .searchguard ... 1 0 5 0 83.8kb 83.8kb
green open .kibana ... 1 0 5 3 133.6kb 133.6kb
green open .kibana.647a750f1787408bf50088... ... 1 0 2 1 53.7kb 53.7kb
green open .operations.2020.05.13 ... 2 0 8824610 0 7.7gb 7.7gb
green open project.logging-query.ade96cd0... ... 2 0 6460 0 6mb 6mb
green open .operations.2020.05.14 ... 2 0 24336636 0 20.6gb 20.6gb
green open .operations.2020.05.18 ... 2 0 8893693 0 8.3gb 8.3gb
[user@host ~]$ oc exec -c elasticsearch $POD -- es_cluster_health | jq
{
"cluster_name" : "elasticsearch",
"status" : "green",
DO380-OCP4.4-en-2-20200803 353
Chapter 10 | Provisioning and Inspecting Cluster Logging
"timed_out" : false,
"number_of_nodes" : 2,
"number_of_data_nodes" : 2,
"active_primary_shards" : 15,
"active_shards" : 15,
"relocating_shards" : 0,
"initializing_shards" : 0,
"unassigned_shards" : 0,
"delayed_unassigned_shards" : 0,
"number_of_pending_tasks" : 0,
"number_of_in_flight_fetch" : 0,
"task_max_waiting_in_queue_millis" : 0,
"active_shards_percent_as_number" : 100.0
}
• The Elasticsearch curator defaults to an INFO debug level.
– Set curator to a more verbose log level, such as DEBUG, by specifying the
CURATOR_LOG_LEVEL env on the curator CronJob.
– Manually run the curator by creating a Job from the CronJob. For example: oc create job
--from=cronjob/curator curator-job.
Note
See the references for OpenShift documentation discussing troubleshooting
specific errors related to accessing and viewing the Kibana console.
References
For more information, refer to the Troubleshooting Curator section in the Curation
of Elasticsearch data chapter, and the Viewing Cluster Logging Status and
Troubleshooting Kibana chapters in the Red Hat OpenShift Container Platform 4.4
Logging documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/logging/index
354 DO380-OCP4.4-en-2-20200803
Chapter 10 | Provisioning and Inspecting Cluster Logging
Guided Exercise
Diagnosing Cluster Logging Problems
In this exercise you will monitor the health of the cluster logging system and identify a
problematic application.
Outcomes
You should be able to:
• Verify the health of the cluster logging system using Prometheus.
• Identify and stop an application that is overwhelming the log collector.
Before You Begin
Before starting this exercise, you must set up cluster logging by completing the Guided
Exercise: Deploying Cluster Logging.
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise. The command creates a logging-diagnose project and a noisy
deployment that outputs multiple log messages per second.
[student@workstation ~]$ lab logging-diagnose start
Instructions
1. First, observe the steady state logging metrics collected by Prometheus.
1.1. Navigate to https://console-openshiftconsole.
apps.ocp4.example.com in Firefox.
1.2. Click Monitoring ® Dashboards, and then select the Kubernetes / Compute
Resources / Namespace (Pods) dashboard and the openshift-logging
namespace. Review the data, paying particular attention to the CPU Usage and
Memory Usage charts.
1.3. Click Monitoring ® Metrics, and then enter the expression
max_over_time(fluentd_output_status_buffer_queue_length[1m]) and
click Run Queries. Review the chart that displays the maximum number of logs in the
collector’s buffer per minute.
2. The current deployment has only one replica and generates logs at a reasonable rate (every
100ms). Increase it to add load.
2.1. From the workstation machine, log in to OpenShift as the admin user.
DO380-OCP4.4-en-2-20200803 355
Chapter 10 | Provisioning and Inspecting Cluster Logging
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
You have access to 58 projects, the list has been suppressed. You can list all
projects with 'oc projects'
Using project "default".
2.2. Edit the deployment to specify ten replicas and a container --delay argument of
1ms.
[student@workstation ~]$ oc project logging-diagnose
Now using project "logging-diagnose" on server "https://
api.ocp4.example.com:6443".
[student@workstation ~]$ oc edit deployment/noisy
apiVersion: apps/v1
kind: Deployment
metadata:
...output omitted...
spec:
progressDeadlineSeconds: 600
replicas: 10
revisionHistoryLimit: 10
...output omitted...
spec:
containers:
- args:
- --delay
- 1ms
image: quay.io/redhattraining/logger:v0.5
...output omitted...
3. Review logging metrics in the monitoring dashboard for changes in the metrics. The log
load is still small and may not be immediately noticeable.
3.1. Click Monitoring ® Dashboards, and then review the Kubernetes / Compute
Resources / Namespace (Pods) dashboard for the openshift-logging
namespace.
3.2. Click Monitoring ® Metrics, enter the expression
max_over_time(fluentd_output_status_buffer_queue_length[1m]),
and then click Run Queries. Review the chart that displays the maximum number of
logs in the collector’s buffer per minute.
4. Scale noisy deployment to 50 replicas.
[student@workstation ~]$ oc scale deployment/noisy --replicas 50
deployment.apps/noisy scaled
356 DO380-OCP4.4-en-2-20200803
Chapter 10 | Provisioning and Inspecting Cluster Logging
5. Review the logging metrics in the monitoring dashboard.
5.1. Click Monitoring ® Dashboards, and then review the Kubernetes / Compute
Resources / Namespace (Pods) dashboard for the openshift-logging
namespace. Notice the increase in CPU Usage.
5.2. Click Monitoring ® Metrics, enter the expression
max_over_time(fluentd_output_status_buffer_queue_length[1m]),
and then click Run Queries. Review the chart that displays the maximum number of
logs in the collector’s buffer per minute. Notice the increasing queue length.
6. Review the log collector alerts.
6.1. Click Monitoring ® Alerting, and then review the alerts in Pending and Firing
states.
6.2. Click Select all filters, and then type fluentd in the Filter Alerts by
name… text field. Review the list of preconfigured alerts related to Fluentd.
6.3. Type elastic in the Filter Alerts by name… text field, and then review the list
of alerts related to Elasticsearch.
7. Query Elasticsearch to discover the namespace containing the application responsible for
the problem.
7.1. Click Monitoring ® Logging to navigate to the Kibana graphical user interface
(GUI). If prompted to log in, click Log in with OpenShift.
7.2. Select the .all index pattern. Click the arrow preceding a log result to display the
table of fields and values, and then click the Toggle column in table icon for
the kubernetes.namespace_name field.
7.3. Click the kubernetes.namespace_name in the Selected Fields list, and then
click Visualize. Review the chart that displays the number of logs per namespace
generated in the last 15 minutes.
7.4. Click Discover to return to the log query screen, and then click New. Select the
project.* index pattern.
7.5. Create a filter on the kubernetes.namespace_name field. Select the is operator
and a value of logging-diagnose. Click Save to view the filtered results.
7.6. Click add next to the message and kubernetes.container_name fields to list
the log messages and container name in the table.
8. Scale the noisy deployment to one replica.
[student@workstation ~]$ oc scale deployment/noisy --replicas 1
deployment.apps/noisy scaled
The log collector will continue to process the log buffer. After a few minutes, the CPU
Usage will return to baseline levels and logs will be fully available in the Kibana GUI.
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
DO380-OCP4.4-en-2-20200803 357
Chapter 10 | Provisioning and Inspecting Cluster Logging
[student@workstation ~]$ lab logging-diagnose finish
This concludes the section.
358 DO380-OCP4.4-en-2-20200803
Chapter 10 | Provisioning and Inspecting Cluster Logging
Summary
In this chapter, you learned:
• About the cluster logging components for OpenShift and how they interact with each other.
• How to install and configure the cluster logging components.
• How to use the Kibana UI to query Elasticsearch for logs.
• How to use the Kibana UI to create charts and visualizations of the data that resides in
Elasticsearch.
• Diagnostic steps to find problematic deployments using the tools provided by cluster logging.
DO380-OCP4.4-en-2-20200803 359
360 DO380-OCP4.4-en-2-20200803
Chapter 11
Recovering Failed Worker
Nodes
Goal Inspect, troubleshoot, and remediate worker nodes
in a variety of failure scenarios.
Objectives • Profile worker nodes to examine healthy nodes,
ensure the node services function properly, and
identify adverse performance or degradation.
• Diagnose and troubleshoot capacity issues that
occur on worker nodes.
Sections • Profiling Degraded Worker Nodes (and Guided
Exercise)
• Troubleshooting Worker Node Capacity Issues
(and Guided Exercise)
DO380-OCP4.4-en-2-20200803 361
Chapter 11 | Recovering Failed Worker Nodes
Profiling Degraded Worker Nodes
Objectives
After completing this section, you should be able to profile worker nodes to examine healthy
nodes, ensure the node services function correctly, and identify adverse performance or
degradation.
Examining Healthy Worker Node Statuses
To properly troubleshoot issues with a worker node, you must understand the typical status and
behaviors of healthy worker nodes. Familiarity with the output of commands that report worker
node status, running processes, and other critical information about the cluster and worker
node health are essential to identifying a degraded environment. This allows you to identify and
troubleshoot issues using valuable details that aid mitigation, such as the taints Openshift applies
to degraded worker nodes that describe the symptom.
Retrieve node status:
oc get nodes <NODE>
Get information for a node through top output for currently running processes:
oc adm top node <NODE>
Check for any applied taints for a node:
oc describe node <NODE> | grep -i taint
Additionally, inspect the worker node through the web console and CLI commands to familiarize
yourself with healthy operational statuses and where to find valuable information, such as on
the Monitoring page. You can find a link to the OpenShift web console for your cluster using the
command:
oc whoami --show-console
Ensuring Worker Node Service Functionality
Each worker node has two main services, kubelet and cri-o, that allow for cluster operations.
On the monitoring page, various probes are configured to monitor worker node performance and
provide necessary details during service issues. More severe service issues might require direct
access using CLI commands on the worker node, such as systemctl, to troubleshoot further and
restore node health.
• Determine the appropriate method to manage the services on worker nodes:
– The web console monitoring provides insight into unhealthy worker nodes, but may not give
specific details for every scenario.
362 DO380-OCP4.4-en-2-20200803
Chapter 11 | Recovering Failed Worker Nodes
– Use the oc client commands to gather as much information as available for the given
symptoms.
– If you cannot correct the issue using other means, then connect directly to the worker node
using SSH to mitigate the issue.
• Navigate the web console information alerting administrators to node issues.
Identifying Adverse Worker Node Performance
When OpenShift detects an adverse node condition, OpenShift applies a corresponding taint to
the node.
A taint that represents an adverse node condition consists of a key value pair directing cluster
interactions with the worker node. The taint key helps administrators identify the cause of the
node degradation. For example, the node.kubernetes.io/disk-pressure taint key indicates
that a node is low on available disk space.
OpenShift uses the taint effect value to direct pod scheduling preference on the tainted node.
The table that follows describes the three taint effects and the corresponding impact on pod
scheduling.
OpenShift Taint Effects
Effect value Description
NoSchedule • New pods that do not match the taint are not scheduled on
the node.
• Pods that are running on the node continue running.
PreferNoSchedule • New pods that do not match the taint are only scheduled on
the node when no other suitable node can be located.
• Pods that are running on the node continue running.
NoExecute • New pods that do not match the taint are not scheduled on
the node.
• Pods that are running on the node are removed and
deployed on another suitable node.
DO380-OCP4.4-en-2-20200803 363
Chapter 11 | Recovering Failed Worker Nodes
References
For more information about using taints in scheduling, refer to the Controlling pod
placement using node taints section in the Chapter 2. Controlling pod placement
onto nodes (scheduling) chapter in the Red Hat OpenShift Container Platform 4.4
Nodes documentation at
https://access.redhat.com/documentation/en-us/
openshift_container_platform/4.4/html-single/nodes/index#nodes-schedulertaints-
tolerations
Taints: Expectations vs. Reality
https://www.openshift.com/blog/taints-expectations-vs.-reality
364 DO380-OCP4.4-en-2-20200803
Chapter 11 | Recovering Failed Worker Nodes
Guided Exercise
Profiling Degraded Worker Nodes
In this exercise, you will profile a degraded worker node and utilize proper corrective
measures to restore node functionality.
Outcomes
You should be able to:
• Identify cluster issues resulting from an unhealthy worker node.
• Inspect and isolate the specific cause of a worker node degradation.
• Repair the issue and restore proper worker node functionality.
Before You Begin
To perform this exercise, ensure you have access to:
• A running OpenShift cluster.
• Access to the workstation virtual machine and cluster administrator credentials.
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise.
This command ensures that the cluster API is reachable and downloads sources of the
sample application.
[student@workstation ~]$ lab workers-degrade start
Instructions
1. Log in to the OpenShift cluster and create a workers-degrade project.
1.1. Log into your OpenShift cluster as the admin user.
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
1.2. Create a new project workers-degrade.
[student@workstation ~]$ oc new-project workers-degrade
Now using project "workers-degrade" on server "https://api.ocp4.example.com:6443".
...output omitted...
2. Deploy and scale a test application. Inspect pod distribution between three worker nodes.
DO380-OCP4.4-en-2-20200803 365
Chapter 11 | Recovering Failed Worker Nodes
2.1. Create a new application named hello using the container located at quay.io/
redhattraining/hello-world-nginx:v1.0.
[student@workstation ~]$ oc create deployment hello
> --image quay.io/redhattraining/hello-world-nginx:v1.0
deployment.apps/hello created
2.2. Manually scale the application to include fifteen running pods.
[student@workstation ~]$ oc scale --replicas 15 deployment/hello
deployment.apps/hello scaled
2.3. Verify that the running pods are distributed between all worker nodes. Notice that
the worker01 node is missing from the list and is not running a replica. This indicates
that the worker01 node is in a degraded state.
[student@workstation ~]$ oc get pods -o wide
NAME ... NODE
hello-54d5d9f48-9pvn4 ... worker05
hello-54d5d9f48-bxj6j ... worker03
hello-54d5d9f48-adnz0 ... worker06
hello-54d5d9f48-f8cgc ... worker02
hello-54d5d9f48-nv5dw ... worker04
hello-54d5d9f48-3pznn ... worker02
hello-54d5d9f48-6cnn6 ... worker05
hello-54d5d9f48-mp284 ... worker03
hello-54d5d9f48-qdnn8 ... worker06
hello-54d5d9f48-wmcgc ... worker04
hello-54d5d9f48-8xw0e ... worker03
hello-54d5d9f48-8kpnn ... worker06
hello-54d5d9f48-2knnn ... worker05
hello-54d5d9f48-7cmn6 ... worker02
hello-54d5d9f48-cxj86 ... worker04
3. From the workstation machine, inspect the OpenShift web console monitoring page for
alerts that provide information regarding cluster performance degradation.
3.1. Display the address for the web console by running the command:
[student@workstation ~]$ oc whoami --show-console
https://console-openshift-console.apps.ocp4.example.com
3.2. Open the link provided in the command output in a browser, and then log in by
providing valid cluster credentials.
4. Navigate to the Monitoring section of the web console and inspect the Firing tab
found on the Alerting page. After a few minutes, several alerts with a severity of
Warning are displayed with additional AlertManager notifications. The alerts provide
details about the cluster degradation, cluster member absence, and make explicit mention
of worker01 being unready.
366 DO380-OCP4.4-en-2-20200803
Chapter 11 | Recovering Failed Worker Nodes
5. Utilize CLI commands to profile the status of the worker nodes to determine the
appropriate remediation.
5.1. Check the status of all nodes. Notice that one worker node reports a Not Ready
status.
[student@workstation ~]$ oc get nodes
NAME STATUS ROLES AGE VERSION
master01 Ready master,worker 14d v1.17.1
master02 Ready master,worker 14d v1.17.1
master03 Ready master,worker 14d v1.17.1
worker01 Not Ready worker 14d v1.17.1
worker02 Ready worker 14d v1.17.1
worker03 Ready worker 14d v1.17.1
worker04 Ready infra,worker 14d v1.17.1
worker05 Ready infra,worker 14d v1.17.1
worker01 Ready infra,worker 14d v1.17.1
5.2. Inspect the worker01 node showing a Not Ready status using the oc describe
command.
[student@workstation ~]$ oc describe node/worker01
...
Taints: node.kubernetes.io/not-ready:NoExecute
node.kubernetes.io/not-ready:NoSchedule
...
...
Conditions:
Type Status ... Reason Message
...
Ready False ... KubeletNotReady container runtime is down
...
...
OpenShift is unable to schedule pods for this worker node because the worker01
node has NoSchedule and NoExecute taints. The Conditions section shows
that the container runtime (cri-o) is down, which is the reason for the taints and Not
Ready status.
5.3. Use the oc debug command to log in to the cluster and attempt remediation.
[student@workstation ~]$ oc debug node/worker01
Starting pod/worker01-debug ...
To use host binaries, run `chroot /host`
Removing debug pod ...
error: watch closed before UntilWithoutRetry timeout
5.4. The oc debug command fails to start the debug pod because the worker node is
in a Not Ready state. It takes some time to display the timeout message from this
failure. The output of this failure is shown in the preceding example.
6. Inspect the OpenShift services and correct the outage for the crio service on the worker
node.
DO380-OCP4.4-en-2-20200803 367
Chapter 11 | Recovering Failed Worker Nodes
6.1. Check the status of crio service for worker01 to reveal that the service is not
running.
[student@workstation ~]$ ssh core@worker01 "sudo systemctl is-active crio"
inactive
6.2. Start crio service for worker01.
[student@workstation ~]$ ssh core@worker01 "sudo systemctl start crio"
6.3. Check status of crio service for worker01 to confirm that the service is running.
[student@workstation ~]$ ssh core@worker01 "sudo systemctl is-active crio"
active
7. Verify the functionality of deployments on the worker node, and also that all monitoring
alerts are resolved.
7.1. Check the status of all Nodes and verify the node has been properly restored. It may
take several minutes for the worker01 node to return to the Ready status.
[student@workstation ~]$ oc get nodes
NAME STATUS ROLES AGE VERSION
master01 Ready master,worker 14d v1.17.1
master02 Ready master,worker 14d v1.17.1
master03 Ready master,worker 14d v1.17.1
worker01 Ready worker 14d v1.17.1
worker02 Ready worker 14d v1.17.1
worker03 Ready worker 14d v1.17.1
worker04 Ready infra,worker 14d v1.17.1
worker05 Ready infra,worker 14d v1.17.1
worker01 Ready infra,worker 14d v1.17.1
7.2. Inspect the worker01 node for taints using the oc describe command.
[student@workstation ~]$ oc describe node/worker01 | grep -i taints
Taints: <none>
Worker node worker01 is healthy and no longer displays any taints.
8. Remove the project to clean up the exercise.
[student@workstation ~]$ oc delete project workers-degrade
project.project.openshift.io "workers-degrade" deleted
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab workers-degrade finish
368 DO380-OCP4.4-en-2-20200803
Chapter 11 | Recovering Failed Worker Nodes
This concludes the section.
DO380-OCP4.4-en-2-20200803 369
Chapter 11 | Recovering Failed Worker Nodes
Troubleshooting Worker Node Capacity
Issues
Objectives
After completing this section, you should be able to diagnose and troubleshoot capacity issues
that occur on worker nodes.
Causes for Worker Node Storage Exhaustion
The OpenShift environment, and applications running within that environment, can cause storage
issues on one or more worker nodes. As a cluster operator or administrator, you have an array
of tools, such as configured monitoring probes and scripted cluster health reports, that aid in
assessing and remediating worker node storage constraints. Typical cluster operations, as well
as the various deployments running within the cluster, are often the main causes of storage
exhaustion, which can be avoided through honed operational practices or mitigated through
focused troubleshooting techniques. Continually improving the operational practices, such as
removing stale logs or container images, instill behaviors that result in good capacity management.
Additionally, simulating disaster recovery scenarios and practicing troubleshooting skills directly
correlate to overall better cluster health.
When left unmanaged, services that log output to the local disk of a worker node result in
depleted disk space that can degrade overall cluster health and performance. Large or numerous
container images consume space and require proper management of their footprint to avoid
worker node storage depletion. Additionally, applications that are configured to utilize ephemeral
storage allocated directly from the local disk of a worker node deplete available capacity. One
clue that OpenShift provides when worker node storage issues arise is the application of a diskpressure:
NoSchedule and/or disk-pressure:NoExecute taint. These taints are displayed
in the output of the oc describe node <NODE_NAME> command.
Other Worker Node Capacity Concerns
Because worker node storage issues commonly arise in active clusters, these are not the main
resources that require proper management by cluster administrators. System memory used by
running applications is finite, and a large or poorly authored deployment is a candidate for worker
node RAM depletion. In much the same manner, CPU processing power is a limited resource
and requires consideration when deploying applications with high computational requirements.
Lastly, network bandwidth is shared between all applications that run on a worker node, and an
application that utilizes heavy traffic throughput results in degradation of all pods running on the
worker node.
Best Practices for Managing Worker Node Resources
A best practice for maintaining worker node performance, as well as overall cluster health, is
properly utilizing the default configured monitoring. It is also prudent to configure additional
monitoring probes for cluster operations, as needed, for both custom operators and deployed
applications. Traditional system administration tools, such as logrotate, are invaluable for
maintaining resource allocations. Employing quotas to manage application consumption of
resources is also prudent in large scale environments. Curating container images and using good
management practices to prune old, stale, or large images reduces storage constraints.
370 DO380-OCP4.4-en-2-20200803
Chapter 11 | Recovering Failed Worker Nodes
Guided Exercise
Troubleshooting Worker Node Capacity
Issues
In this exercise, you will identify a capacity issue on a worker node and remediate the root
cause.
Outcomes
You should be able to:
• Identify cluster capacity issues through web console and CLI interactions.
• Inspect and isolate the specific cause of a capacity constraint.
• Repair the issue and restore proper worker node functionality.
Before You Begin
To perform this exercise, ensure you have access to:
• A running OpenShift cluster.
• Access to the workstation VM and cluster administrator credentials.
As the student user on the workstation machine, use the lab command to prepare your
system for this exercise.
This command ensures that the cluster API is reachable and downloads the sample
application.
[student@workstation ~]$ lab workers-capacity start
Instructions
1. Log in to OpenShift cluster and create a workers-capacity project.
1.1. Log in in as the admin user.
[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
1.2. Create a new project workers-capacity.
[student@workstation ~]$ oc new-project workers-capacity
Now using project "workers-capacity" on server "https://
api.ocp4.example.com:6443".
...output omitted...
DO380-OCP4.4-en-2-20200803 371
Chapter 11 | Recovering Failed Worker Nodes
2. Deploy a custom application and inspect the worker nodes to determine the pod placement
within the cluster.
2.1. Deploy a custom application named capacity using the container image located at
quay.io/ redhattraining/workers-capacity.
[student@workstation ~]$ oc create deployment capacity
> --image quay.io/redhattraining/workers-capacity
deployment.apps/capacity created
2.2. Verify that the pod for the deployment is in a Running status. Note that your specific
pod hash will vary.
[student@workstation ~]$ oc get pods
NAME READY STATUS RESTARTS AGE
capacity-89f594b99-l4m7f 1/1 Running 0 8s
2.3. Identify the worker node on which the pod for the application is running. The output
below shows this pod running on worker03 located at IP address 192.168.50.15
within the cluster. Note that your specific worker node may vary.
[student@workstation ~]$ oc describe pod capacity-89f594b99-l4m7f | grep Node
Node: worker03/192.168.50.15
...output omitted...
3. Access the pod and simulate an application issue or rogue execution resulting in a malicious
action and filling the pod storage.
3.1. In a separate terminal window, log in to the pod for the application. A separate
window is necessary so that you can quickly toggle between prompts for the
workstation machine and the pod to properly view and troubleshoot the
degradation.
[student@workstation ~]$ oc rsh capacity-89f594b99-l4m7f
sh-4.2$
3.2. View the current available storage for the overlay storage partition.
sh-4.2$ df -h | grep overlay
Filesystem Size Used Avail Use% Mounted on
overlay 40G 9.8G 30G 25% /
3.3. Execute a dd command to simulate an issue that fills the local storage of the pod.
Note that this command may take several minutes to complete and you may be
logged out of the pod during the wait.
sh-4.2$ dd if=/dev/zero of=/tmp/large-file.img bs=1024 count=30000000
4. In the workstation terminal, inspect the worker node on which the pod is deployed for
taints to identify the issue. It may take several minutes for the execution to complete on the
pod.
372 DO380-OCP4.4-en-2-20200803
Chapter 11 | Recovering Failed Worker Nodes
4.1. From the workstation terminal window, simultaneously check for taints on the
specific worker node on which the pod is running, as identified in prior steps.
[student@workstation ~]$ oc describe node worker03 | grep -i taint
Taints: node.kybernetes.io/disk-pressure:NoSchedule
The taint shows a disk-pressure due to the depleted worker node storage and
results in the effect of NoSchedule for the worker node. This means that no pods
may be scheduled on this worker node until the disk-pressue issue is resolved.
5. Quickly return to the pod terminal window and remove the problematic file that you
created. If you do not remediate the issue quickly enough, OpenShift will evacuate the pod
and try to redeploy it. Should this occur, you must restart the exercise.
5.1. Remove /tmp/large-file.img to alleviate the disk-pressure taint on the
worker node.
sh-4.2$ rm /tmp/large-file.img
5.2. Verify that the storage constraint is corrected by checking the overlay partition
storage.
sh-4.2$ df -h | grep overlay
Filesystem Size Used Avail Use% Mounted on
overlay 40G 9.8G 30G 25% /
6. Verify the worker node is fully operational and can resume scheduling deployed pods.
[student@workstation ~]$ oc describe node worker03 | grep -i taint
Taints: <none>
This command shows that there is no longer a taint on the worker node and that cluster
health has been restored.
7. Log out of the pod and close the additional terminal.
sh-4.2$ exit
exit
[student@workstation ~]$
8. Remove the project to clean up the exercise.
[student@workstation ~]$ oc delete project workers-capacity
project.project.openshift.io "workers-capacity" deleted
Finish
On the workstation machine, use the lab command to complete this exercise. This is important
to ensure that resources from previous exercises do not impact upcoming exercises.
[student@workstation ~]$ lab workers-capacity finish
DO380-OCP4.4-en-2-20200803 373
Chapter 11 | Recovering Failed Worker Nodes
This concludes the section.
374 DO380-OCP4.4-en-2-20200803
Chapter 11 | Recovering Failed Worker Nodes
Summary
In this chapter, you learned:
• Worker nodes can experience a degradation of performance and functionality that can be
identified and mitigated through the use of monitoring, CLI commands, and status information
provided by Openshift.
• Proper administration of an Openshift cluster requires awareness and mitigation of various
capacity issues that arise during operation.
DO380-OCP4.4-en-2-20200803 375
376 DO380-OCP4.4-en-2-20200803
Appendix A
Creating a Quay Account
Goal Describe how to create a Quay account for labs in
the course.
DO380-OCP4.4-en-2-20200803 377
Appendix A | Creating a Quay Account
Creating a Quay Account
Objectives
After completing this section, you should be able to create a Quay account and create public
container image repositories for the labs in the course.
Creating a Quay Account
You need a Quay account to create one or more public container image repositories for the labs
in this course. If you already have a Quay account, you can skip the steps to create a new account
listed in this appendix.
Important
If you already have a Quay account, ensure that you only create public container
image repositories for the labs in this course. The lab grading scripts and
instructions require unauthenticated access to pull container images from the
repository.
To create a new Quay account, perform the following steps:
1. Navigate to https://quay.io using a web browser.
2. Click Sign in in the upper-right corner (next to the search bar).
3. On the Sign in page, you can log in using your Google or GitHub credentials. Alternatively,
click Create Account to create a new account.
378 DO380-OCP4.4-en-2-20200803
Appendix A | Creating a Quay Account
Figure A.1: Sign in using Google or GitHub credentials or create a new account.
4. If you chose to skip the Google or GitHub log-in method and instead opted to create a new
account, you will receive an email with instructions on how to activate your Quay account.
Verify your email address and then sign in to the Quay website with the username and
password you provided during account creation.
DO380-OCP4.4-en-2-20200803 379
Appendix A | Creating a Quay Account
References
Getting Started with Quay.io
https://docs.quay.io/solution/getting-started.html
380 DO380-OCP4.4-en-2-20200803
Appendix A | Creating a Quay Account
Repositories Visibility
Objectives
After completing this section, you should be able to control repository visibility on Quay.io.
Quay.io Repositories Visibility
Quay.io offers the possibility of creating public and private repositories. Public repositories can be
read by anyone without restrictions, despite write permissions must be explicitly granted. Private
repositories have both read and write permissions restricted. Nevertheless, the number of private
repositories in quay.io is limited depending on the namespace's plan.
Default Repository Visibility
Repositories created by pushing images to quay.io are private by default. In order OpenShift (or
any other tool) to fetch those images you can either configure a private key in both OpenShift and
Quay, or make the repository public, so no authentication is required. Setting up private keys is out
of scope of this document.
DO380-OCP4.4-en-2-20200803 381
Appendix A | Creating a Quay Account
Updating Repository Visibility
In order to set repository visibility to public select the appropriate repository in https://
quay.io/repository/ (log in to your account if needed) and open the Settings page by
clicking on the gear on the left-bottom edge. Scroll down to the Repository Visibility
section and click on the Make Public button.
Get back the the list of repositories. The lock icon besides the repository name have disappeared,
indicating the repository became public.
382 DO380-OCP4.4-en-2-20200803
